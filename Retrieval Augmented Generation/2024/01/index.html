
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../02/">
      
      
        <link rel="next" href="../../2023/12/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.31">
    
    
      
        <title>2024-01(50) - ArXiv Tracker</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="ArXiv Tracker" class="md-header__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ArXiv Tracker
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2024-01(50)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Emission%20Trading%20System/2024/07/" class="md-tabs__link">
          
  
  Emission Trading System

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../08/" class="md-tabs__link">
          
  
  Retrieval Augmented Generation

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="ArXiv Tracker" class="md-nav__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ArXiv Tracker
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Emission Trading System
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Emission Trading System
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2016/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2011/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2011-10(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Retrieval Augmented Generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Retrieval Augmented Generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-08(16)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(125)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(152)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-05(121)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-04(86)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-03(70)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(87)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2024-01(50)
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(33)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-11(38)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(43)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(20)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-08(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-07(11)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(33)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-04(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-02(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(15)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-09(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-07(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-05(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-04(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-12(9)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-09(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-08(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-06(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-06(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-04(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-12(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2015/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2012/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2012-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2009/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2009-03(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="retrieval-augmented-generation-202401">Retrieval Augmented Generation - 2024年01月</h1>
<h2 id="rag-fusion-a-new-take-on-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2402.03367v2">RAG-Fusion: a New Take on Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-01-31</p>
<p>作者：Zackary Rackauckas</p>
<h4 id="_1">中文摘要：</h4>
<p>英飞凌公司发现工程师、客户经理和客户需要快速获取产品信息。传统上，这个问题通过检索增强生成（RAG）聊天机器人来解决，但在这项研究中，我评估了新兴的RAG-Fusion方法的使用。RAG-Fusion通过生成多个查询，使用互反评分重新排序它们，并融合文档和评分，结合了RAG和互反排名融合（RRF）。通过手动评估答案的准确性、相关性和全面性，我发现RAG-Fusion能够提供准确和全面的答案，因为生成的查询从不同角度对原始查询进行了上下文化。然而，当生成的查询与原始查询的相关性不足时，一些答案会偏离主题。这项研究在人工智能（AI）和自然语言处理（NLP）应用方面取得了重大进展，并在全球和多个行业背景下展示了变革。</p>
<h4 id="_2">一句话总结：</h4>
<p>本研究评估了RAG-Fusion方法在快速获取产品信息方面的有效性，并展示了其在人工智能和自然语言处理领域的应用潜力。</p>
<hr />
<h2 id="resllm-large-language-models-are-strong-resource-selectors-for-federated-search"><a href="http://arxiv.org/abs/2401.17645v1">ReSLLM: Large Language Models are Strong Resource Selectors for Federated Search</a></h2>
<p>发布时间：2024-01-31</p>
<p>作者：Shuai Wang, Shengyao Zhuang, Bevan Koopman, Guido Zuccon</p>
<h4 id="_3">中文摘要：</h4>
<p>联邦搜索，涉及整合多个独立搜索引擎的结果，将在检索增强生成（Retrieval-Augmented Generation）管道中发挥越来越关键的作用，尤其是在支持基于大型语言模型（LLM）的应用，如聊天机器人。这些系统通常根据用户话语的性质，将查询分配到各种搜索引擎，从专业搜索引擎（例如PubMed）到通用搜索引擎（例如Google）。联邦搜索的一个关键方面是资源选择——在发出查询之前选择适当的资源，以确保高质量和快速的响应，并包含调用外部搜索引擎的成本。然而，当前最先进的资源选择方法主要依赖于基于特征的学习方法。这些方法通常涉及为每个资源创建劳动密集型和昂贵的训练标签。相比之下，LLM在自然语言处理（NLP）和信息检索（IR）任务中表现出作为零样本方法的强大有效性。我们假设在联邦搜索的背景下，LLM可以在无需大量预定义标签或特征的情况下评估资源的相关性。在本文中，我们提出了ReSLLM方法。我们的ReSLLM方法利用LLM在零样本设置中驱动联邦搜索中的资源选择。此外，我们设计了一种无监督的微调协议，称为合成标签增强微调（Synthetic Label Augmentation Tuning，SLAT），其中使用现成的LLM预测先前记录的查询和资源片段的相关性，然后将其用于针对资源选择微调ReSLLM。我们的实证评估和分析详细说明了影响LLM在此背景下有效性的因素。结果表明，ReSLLM在资源选择方面具有优势：不仅在零样本设置中具有竞争力的有效性，而且在使用SLAT协议微调时效果显著。</p>
<h4 id="_4">一句话总结：</h4>
<p>本文提出了一种基于LLM的联邦搜索资源选择方法ReSLLM，通过无监督微调协议SLAT实现高效且成本低的资源选择。</p>
<hr />
<h2 id="weaver-foundation-models-for-creative-writing"><a href="http://arxiv.org/abs/2401.17268v1">Weaver: Foundation Models for Creative Writing</a></h2>
<p>发布时间：2024-01-30</p>
<p>作者：Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, Wangchunshu Zhou</p>
<h4 id="_5">中文摘要：</h4>
<p>这项工作介绍了Weaver，这是我们首个致力于内容创作的大型语言模型（LLM）系列。Weaver在精心挑选的语料库上进行预训练，旨在提升大型语言模型的写作能力。随后，我们对Weaver进行微调和针对创意和专业写作目的进行优化，并利用一系列新颖的指令数据合成和LLM对齐方法，使其能够生成更接近人类文本并遵循更多样化的内容创作指令。Weaver家族包括Weaver Mini（1.8B）、Weaver Base（6B）、Weaver Pro（14B）和Weaver Ultra（34B）等不同规模的模型，适用于不同的应用场景，并可由路由代理根据查询复杂度动态调度，以平衡响应质量和计算成本。在用于评估LLM写作能力的精心设计的基准测试中，所有规模的Weaver模型都优于其数倍大小的通用LLM。值得注意的是，我们最强大的Weaver Ultra模型在各种写作场景中超越了GPT-4这一最先进的通用LLM，证明了针对写作目的训练专业LLM的优势。此外，Weaver原生支持检索增强生成（RAG）和功能调用（工具使用）。我们展示了这些能力在提升AI辅助写作系统中的应用案例，包括集成外部知识库、工具或API，以及提供个性化写作辅助。此外，我们还讨论并总结了针对特定领域LLM的预训练和微调的指南和最佳实践。</p>
<h4 id="_6">一句话总结：</h4>
<p>Weaver系列大型语言模型通过针对写作目的的专门训练，显著提升了内容创作的质量和效率。</p>
<hr />
<h2 id="llamp-large-language-model-made-powerful-for-high-fidelity-materials-knowledge-retrieval-and-distillation"><a href="http://arxiv.org/abs/2401.17244v2">LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation</a></h2>
<p>发布时间：2024-01-30</p>
<p>作者：Yuan Chiang, Elvis Hsieh, Chia-Hong Chou, Janosh Riebesell</p>
<h4 id="_7">中文摘要：</h4>
<p>减少大型语言模型（LLMs）的幻觉对于其在科学领域的应用至关重要，因为可靠性和可重复性是关键。然而，LLMs本身缺乏长期记忆，这使得在特定领域的文献和数据上微调它们成为一个非平凡、临时且不可避免带有偏差的任务。在这里，我们介绍了LLaMP，这是一个多模态检索增强生成（RAG）框架，包含分层推理和行动（ReAct）代理，能够动态和递归地与材料项目（MP）上的计算和实验数据进行交互，并通过高通量工作流程界面运行原子模拟。无需微调，LLaMP展示了强大的工具使用能力，能够理解和整合材料科学概念的多种模态，实时获取相关数据存储，处理高级数据（如晶体结构和弹性张量），并简化计算材料和化学中的复杂任务。我们提出了一种简单的指标，结合不确定性和置信度估计来评估LLaMP和普通LLMs响应的自我一致性。我们的基准测试表明，LLaMP有效地缓解了LLMs的内在偏差，抵消了来自混合数据源的大块模量、电子能带间隙和形成能的错误。我们还展示了LLaMP使用预训练的机器学习力场编辑晶体结构和运行退火分子动力学模拟的能力。该框架提供了一种直观且几乎无幻觉的方法来探索和扩展材料信息学，并为知识蒸馏和微调其他语言模型开辟了途径。代码和实时演示可在https://github.com/chiang-yuan/llamp获取。</p>
<h4 id="_8">一句话总结：</h4>
<p>LLaMP是一种减少大型语言模型幻觉的新框架，通过多模态检索增强生成和分层推理行动代理，提高了材料科学领域的数据处理和模拟能力。</p>
<hr />
<h2 id="crud-rag-a-comprehensive-chinese-benchmark-for-retrieval-augmented-generation-of-large-language-models"><a href="http://arxiv.org/abs/2401.17043v3">CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models</a></h2>
<p>发布时间：2024-01-30</p>
<p>作者：Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen</p>
<h4 id="_9">中文摘要：</h4>
<p>检索增强生成（RAG）是一种通过整合外部知识源来提升大型语言模型（LLMs）能力的技术。这种方法解决了LLMs的常见局限性，包括信息过时和产生不准确“幻觉”内容的问题。然而，RAG系统的评估具有挑战性，因为现有的基准测试在范围和多样性方面有限。大多数当前基准测试主要评估问答应用，而忽略了RAG可能具有优势的更广泛的应用场景。此外，它们只评估了RAG管道中LLM组件的性能，而忽略了检索组件和外部知识数据库的影响。为了解决这些问题，本文构建了一个大规模且更全面的基准测试，并在各种RAG应用场景中评估了RAG系统的所有组件。具体来说，我们将RAG应用范围分为四种不同的类型——创建（Create）、读取（Read）、更新（Update）和删除（Delete）（CRUD），每种类型代表一个独特的用例。“创建”指的是需要生成原创、多样化内容的情况。“读取”涉及在知识密集型环境中回答复杂问题。“更新”侧重于修正和纠正现有文本中的不准确或不一致之处。“删除”涉及将大量文本总结成更简洁的形式。对于这些CRUD类别中的每一个，我们都开发了全面的数据库来评估RAG系统的性能。我们还分析了RAG系统中各种组件的影响，如检索器、上下文长度、知识库构建和LLM。最后，我们提供了针对不同场景优化RAG技术的有用见解。</p>
<h4 id="_10">一句话总结：</h4>
<p>本文构建了一个全面的RAG基准测试，评估了RAG系统在多种应用场景中的性能，并提供了优化RAG技术的见解。</p>
<hr />
<h2 id="large-multi-modal-models-lmms-as-universal-foundation-models-for-ai-native-wireless-systems"><a href="http://arxiv.org/abs/2402.01748v2">Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems</a></h2>
<p>发布时间：2024-01-30</p>
<p>作者：Shengzhe Xu, Christo Kurisummoottil Thomas, Omar Hashash, Nikhil Muralidhar, Walid Saad, Naren Ramakrishnan</p>
<h4 id="_11">中文摘要：</h4>
<p>大型语言模型（LLMs）和基础模型最近被誉为6G系统的变革者。然而，近期针对无线网络的LLMs研究仅限于直接应用为自然语言处理（NLP）应用设计的现有语言模型。为了应对这一挑战并创建以无线为中心的基础模型，本文提出了一种全面的设计愿景，旨在设计适用于人工智能（AI）原生网络部署的通用基础模型。与基于NLP的基础模型不同，所提出的框架促进了大多模态模型（LMMs）的设计，这些模型由三个关键能力推动：1）处理多模态感知数据，2）利用因果推理和检索增强生成（RAG）将物理符号表示在现实世界无线系统中进行归一化，3）通过神经符号AI的逻辑和数学推理，使无线环境反馈可指令化，从而促进动态网络适应。本质上，这些特性使得所提出的LMM框架能够构建适应各种跨层网络任务和不同领域意图对齐的通用能力。初步的实验评估结果表明，在LMMs中使用RAG进行归一化的有效性，并展示了LMMs与无线系统设计的契合度。此外，与传统的LLMs相比，LMMs在回答数学问题时所展现的增强推理能力，证明了LMMs内在的逻辑和数学推理能力。基于这些结果，我们提出了一系列针对LMMs的开放问题和挑战。最后，我们提出了一些建议，以点燃通往LMM赋能的AI原生系统的道路。</p>
<h4 id="_12">一句话总结：</h4>
<p>本文提出了一种基于大多模态模型（LMMs）的通用基础模型设计框架，旨在推动无线网络中人工智能原生网络的部署。</p>
<hr />
<h2 id="development-and-testing-of-a-novel-large-language-model-based-clinical-decision-support-systems-for-medication-safety-in-12-clinical-specialties"><a href="http://arxiv.org/abs/2402.01741v2">Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties</a></h2>
<p>发布时间：2024-01-29</p>
<p>作者：Jasmine Chiat Ling Ong, Liyuan Jin, Kabilan Elangovan, Gilbert Yong San Lim, Daniel Yan Zheng Lim, Gerald Gui Ren Sng, Yuhe Ke, Joshua Yi Min Tung, Ryan Jian Zhong, Christopher Ming Yao Koh, Keane Zhi Hao Lee, Xiang Chen, Jack Kian Chng, Aung Than, Ken Junyang Goh, Daniel Shu Wei Ting</p>
<h4 id="_13">中文摘要：</h4>
<p>本研究引入了一种新颖的检索增强生成（RAG）-大型语言模型（LLM）框架作为临床决策支持系统（CDSS），以支持安全用药处方。研究旨在评估基于LLM的CDSS在不同患者案例摘要中正确识别药物错误的效果，与由人类专家小组得出的真实情况进行比较。研究比较了两种不同的CDSS实际医疗整合模式：基于LLM的CDSS独立使用（完全自主模式）与初级药剂师+基于LLM的CDSS（协同驾驶，辅助模式）。研究利用了具有最先进的医学相关LLM（GPT-4、Gemini Pro 1.0和Med-PaLM 2）的RAG模型，在12个不同医学和外科专科的23个复杂临床案例中嵌入了61个处方错误场景。多学科专家小组使用PCNE分类评估这些案例的药物相关问题（DRPs），并使用修订的NCC MERP药物错误指数对严重程度/潜在危害进行分级。结果显示，与单独使用LLM相比，RAG-LLM表现更佳。在协同驾驶模式下，准确率、召回率和F1分数得到优化，表明其在识别中到重度DRPs方面的有效性。RAG-LLM在DRP检测的准确性在几个类别中得到提高，但以降低精确度为代价。</p>
<h4 id="_14">一句话总结：</h4>
<p>本研究证实，基于RAG-LLM的CDSS与初级药剂师协同使用时，显著提高了药物错误识别的准确性，特别是在检测严重药物相关问题时，同时揭示了当前最先进的LLM在RAG-LLM CDSS系统中的比较性能。</p>
<hr />
<h2 id="realizing-disentanglement-in-lm-latent-space-via-vocabulary-defined-semantics"><a href="http://arxiv.org/abs/2401.16184v5">Realizing Disentanglement in LM Latent Space via Vocabulary-Defined Semantics</a></h2>
<p>发布时间：2024-01-29</p>
<p>作者：Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang</p>
<h4 id="_15">中文摘要：</h4>
<p>理解语言模型（LMs）的潜在空间对于提高LMs的性能和可解释性至关重要。现有的分析往往无法充分利用语言模型的语义属性，并且常常忽视了语言模型适应的关键方面。为此，我们提出了一种开创性的方法，称为词汇定义语义，该方法在LM潜在空间内建立了一个以LM词汇为基础的参考框架。我们提出了一种新颖的技术来计算潜在空间中的解耦logits和梯度，而不是词汇上的纠缠。此外，我们通过对数据表示进行语义聚类作为LM适应的一种新方法。通过在多种文本理解数据集上进行的大量实验，我们的方法优于检索增强生成和参数高效微调的现有最先进方法，展示了其有效性和效率。</p>
<h4 id="_16">一句话总结：</h4>
<p>本研究提出了一种基于词汇定义语义的新方法，通过在语言模型潜在空间中进行解耦logits和梯度计算以及语义聚类，显著提升了语言模型性能和可解释性。</p>
<hr />
<h2 id="kaucus-knowledge-augmented-user-simulators-for-training-language-model-assistants"><a href="http://arxiv.org/abs/2401.16454v1">KAUCUS: Knowledge Augmented User Simulators for Training Language Model Assistants</a></h2>
<p>发布时间：2024-01-29</p>
<p>作者：Kaustubh D. Dhole</p>
<h4 id="_17">中文摘要：</h4>
<p>通过创建一个能够生成有用交互数据的模拟器，可以开发出一个有效的多轮指令跟随助手。除了依赖其内在权重外，一个理想的用户模拟器还应该能够快速以原始形式启动外部知识，以模拟互联网上可用的各种文本多样性。以往的用户模拟器通常缺乏多样性，大多是封闭领域，并且需要严格的方案制定，这使得它们难以快速扩展以纳入外部知识。在这方面，我们引入了Kaucus，一个知识增强用户模拟器框架，以概述创建多样化用户模拟器的过程，这些模拟器可以无缝利用外部知识，并有助于下游助手模型训练。通过两个基于GPT-J的模拟器，即检索增强模拟器和摘要控制模拟器，我们生成了多样化的模拟器-助手交互。通过基于奖励和偏好模型的评估，我们发现这些交互作为有用的训练数据，并创建了更有帮助的下游助手。我们还发现，通过检索增强或摘要控制来融入知识有助于创建更好的助手。</p>
<h4 id="_18">一句话总结：</h4>
<p>本研究提出了一种知识增强用户模拟器框架Kaucus，通过检索增强和摘要控制技术，实现了多样化用户模拟器，有效提升了下游助手模型的训练效果。</p>
<hr />
<h2 id="development-and-testing-of-retrieval-augmented-generation-in-large-language-models-a-case-study-report"><a href="http://arxiv.org/abs/2402.01733v1">Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report</a></h2>
<p>发布时间：2024-01-29</p>
<p>作者：YuHe Ke, Liyuan Jin, Kabilan Elangovan, Hairil Rizal Abdullah, Nan Liu, Alex Tiong Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat Ling Ong, Daniel Shu Wei Ting</p>
<h4 id="_19">中文摘要：</h4>
<p>目的：大型语言模型（LLMs）在医疗应用中具有巨大的潜力。检索增强生成（RAG）作为一种定制LLMs领域知识的有前景的方法而出现。本案例研究介绍了针对医疗保健，特别是围手术期医学，开发并评估了一个LLM-RAG管道。方法：我们使用35个围手术期指南开发了一个LLM-RAG模型，并将其与人类生成的回答进行了测试，共评估了1260个回答。RAG过程涉及使用基于Python的框架如LangChain和Llamaindex将临床文档转换为文本，并将这些文本处理成块以进行嵌入和检索。使用Pinecone进行向量存储（维度为1536）和余弦相似度作为损失度量来优化数据检索。使用初级医生提供的人类生成的答案作为比较。结果：LLM-RAG模型在平均15-20秒内生成答案，比人类通常需要的10分钟快得多。在基本LLMs中，GPT4.0表现出最佳的准确率，为80.1%。当模型通过RAG增强时，准确率进一步提高到91.4%。与人类生成的指令（准确率为86.3%）相比，GPT4.0 RAG模型的性能显示出非劣效性（p=0.610）。结论：在本案例研究中，我们展示了一个适用于医疗保健实施的LLM-RAG模型。该管道显示了基于知识、可升级性和可扩展性作为医疗LLM部署重要方面的优势。</p>
<h4 id="_20">一句话总结：</h4>
<p>本研究开发了一个针对医疗保健的LLM-RAG模型，显著提高了围手术期医学问答的准确性和效率。</p>
<hr />
<h2 id="corrective-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2401.15884v2">Corrective Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-01-29</p>
<p>作者：Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling</p>
<h4 id="_21">中文摘要：</h4>
<p>大型语言模型（LLMs）由于无法仅通过封装的参数知识保证生成文本的准确性，不可避免地会表现出幻觉。尽管检索增强生成（RAG）是LLMs的一个可行的补充，但它高度依赖于检索文档的相关性，这引发了关于如果检索出错模型将如何表现的问题。为此，我们提出了纠正检索增强生成（CRAG）来提高生成的鲁棒性。具体来说，设计了一个轻量级的检索评估器来评估查询检索文档的整体质量，并基于此返回一个置信度，根据这个置信度可以触发不同的知识检索动作。由于从静态和有限的语料库中检索只能返回次优文档，因此利用大规模网络搜索作为检索结果的增强。此外，为检索文档设计了一个分解后重组算法，以选择性地关注关键信息并过滤掉其中的无关信息。CRAG是即插即用的，可以无缝地与各种基于RAG的方法相结合。在涵盖短文本和长文本生成任务的四个数据集上的实验表明，CRAG可以显著提高基于RAG方法的性能。</p>
<h4 id="_22">一句话总结：</h4>
<p>CRAG通过引入轻量级检索评估器和大规模网络搜索，增强了RAG方法的鲁棒性，显著提升了生成文本的质量。</p>
<hr />
<h2 id="multihop-rag-benchmarking-retrieval-augmented-generation-for-multi-hop-queries"><a href="http://arxiv.org/abs/2401.15391v1">MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries</a></h2>
<p>发布时间：2024-01-27</p>
<p>作者：Yixuan Tang, Yi Yang</p>
<h4 id="_23">中文摘要：</h4>
<p>检索增强生成（RAG）通过检索相关知识来增强大型语言模型（LLM），在减轻LLM的幻觉和提升响应质量方面展现出良好的潜力，从而促进了LLM在实际应用中的广泛采用。然而，我们发现现有的RAG系统在回答多跳查询方面存在不足，这些查询需要检索和推理多份支持证据。此外，据我们所知，目前没有现存的RAG基准数据集专注于多跳查询。在本文中，我们开发了一个新的数据集，MultiHop-RAG，它包含一个知识库、大量多跳查询、它们的真实答案以及相关的支持证据。我们详细描述了构建数据集的过程，利用英文新闻文章数据集作为底层RAG知识库。我们在两个实验中展示了MultiHop-RAG的基准测试效用。第一个实验比较了用于检索多跳查询证据的不同嵌入模型。在第二个实验中，我们检验了包括GPT-4、PaLM和Llama2-70B在内的各种最先进LLM在给定证据的情况下推理和回答多跳查询的能力。两个实验都表明，现有的RAG方法在检索和回答多跳查询方面表现不佳。我们希望MultiHop-RAG将成为社区在开发有效RAG系统中的一个宝贵资源，从而促进LLM在实际应用中的更广泛采用。MultiHop-RAG和实现的RAG系统可在https://github.com/yixuantt/MultiHop-RAG/公开获取。</p>
<h4 id="_24">一句话总结：</h4>
<p>本文提出并开发了MultiHop-RAG数据集，旨在解决现有RAG系统在处理多跳查询时的不足，以促进LLM在实际应用中的更广泛采用。</p>
<hr />
<h2 id="a-rag-based-question-answering-system-proposal-for-understanding-islam-mufassirqas-llm"><a href="http://arxiv.org/abs/2401.15378v4">A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM</a></h2>
<p>发布时间：2024-01-27</p>
<p>作者：Ahmet Yusuf Alan, Enis Karaarslan, Ömer Aydin</p>
<h4 id="_25">中文摘要：</h4>
<p>在学习和理解宗教方面存在挑战，例如宗教教义和教诲的复杂性和深度。作为问答系统的聊天机器人可以帮助解决这些挑战。基于大型语言模型（LLM）的聊天机器人利用自然语言处理（NLP）技术建立主题之间的联系，并准确回答复杂问题。这些能力使其成为作为问答聊天机器人的宗教启蒙的理想选择。然而，LLM也倾向于生成虚假信息，这被称为幻觉。此外，聊天机器人的回答可能包含侮辱个人宗教信仰、宗教间冲突、有争议或敏感话题的内容。必须避免此类情况，同时不宣扬仇恨言论或冒犯某些群体或其信仰。本研究采用基于向量数据库的检索增强生成（RAG）方法来提高LLM的准确性和透明度。我们的问答系统被称为“MufassirQAS”。我们创建了一个包含土耳其语境的开放获取书籍数据库。这些书籍包含伊斯兰教的土耳其语翻译和解释。该数据库用于回答与宗教相关的问题，并确保我们的答案是可信的。同时呈现了数据集中LLM也使用的相关部分以及答案。我们投入了大量精力创建系统提示，以指导防止产生有害、冒犯或不尊重的回应，尊重人们的价值观并提供可靠的结果。系统还会回答并分享额外信息，例如来自相应书籍的页码和获取信息的参考文献。MufassirQAS和ChatGPT也针对敏感问题进行了测试。我们的系统表现更佳。研究和改进仍在进行中，结果和未来工作将给出。</p>
<h4 id="_26">一句话总结：</h4>
<p>本研究提出了一种基于RAG的问答系统MufassirQAS，旨在提高LLM在宗教领域的问答准确性和透明度，同时避免产生有害或冒犯性的回答。</p>
<hr />
<h2 id="improving-medical-reasoning-through-retrieval-and-self-reflection-with-retrieval-augmented-large-language-models"><a href="http://arxiv.org/abs/2401.15269v3">Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models</a></h2>
<p>发布时间：2024-01-27</p>
<p>作者：Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang</p>
<h4 id="_27">中文摘要：</h4>
<p>近年来，私有大型语言模型（LLMs）如GPT-4在解决生物医学领域的多样化挑战方面取得了里程碑式的进展，这些挑战包括从多项选择题到长篇生成。为了解决LLMs编码知识仍无法处理的挑战，已经开发出各种检索增强生成（RAG）方法，通过搜索知识库中的文档，并将它们无条件或选择性添加到LLMs的输入中进行生成。然而，当将现有方法应用于不同领域的特定问题时，泛化能力差的问题变得明显，导致检索到错误的文档或做出不准确的判断。在本文中，我们介绍了Self-BioRAG，这是一个针对生物医学文本的可靠框架，专门用于生成解释、检索特定领域的文档以及自我反思生成的响应。我们利用84k个过滤的生物医学指令集来训练Self-BioRAG，使其能够使用定制的反思标记来评估其生成的解释。我们的工作证明，特定领域的组件，如检索器、领域相关文档语料库和指令集对于遵循领域相关指令是必要的。使用三个主要的医学问答基准数据集，Self-BioRAG的实验结果表明，与参数大小为7B或更少的现有最先进开放基础模型相比，平均绝对性能提升了7.2%。总的来说，我们分析认为，Self-BioRAG在问题中寻找线索，在需要时检索相关文档，并像医学专家一样理解如何使用检索到的文档和编码知识来回答问题。我们发布了我们的数据和代码，用于训练框架组件和模型权重（7B和13B），以增强在生物医学和临床领域的功能。</p>
<h4 id="_28">一句话总结：</h4>
<p>Self-BioRAG是一种针对生物医学文本的可靠框架，通过检索特定领域的文档和自我反思生成的响应，显著提升了生物医学问答的性能。</p>
<hr />
<h2 id="enhancing-large-language-model-performance-to-answer-questions-and-extract-information-more-accurately"><a href="http://arxiv.org/abs/2402.01722v1">Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately</a></h2>
<p>发布时间：2024-01-27</p>
<p>作者：Liang Zhang, Katherine Jijo, Spurthi Setty, Eden Chung, Fatima Javid, Natan Vidra, Tommy Clifford</p>
<h4 id="_29">中文摘要：</h4>
<p>大型语言模型（LLMs）能够生成对问题的回答；然而，它们的有效性常常受到答案质量不佳和偶尔无法准确回答问题的限制。为了解决这些挑战，采用了一种微调过程，涉及反馈和示例来优化模型。目标是通过对模型进行连续的反馈循环来提升人工智能模型，使用余弦相似度、LLM评估和Rouge-L分数等指标来评估模型。利用GPT-3.5、GPT4ALL、LLaMA2和Claude等LLMs，该方法在包括FinanceBench和RAG Instruct Benchmark Tester Dataset在内的金融数据集上进行了基准测试，说明了微调的必要性。结果显示，经过微调的模型能够超越零样本LLMs的准确性，提供更优越的问题和回答能力。值得注意的是，将LLM的微调与一种称为检索增强生成（RAG）的过程相结合，可以生成更准确的回答。</p>
<h4 id="_30">一句话总结：</h4>
<p>通过微调大型语言模型并结合检索增强生成技术，显著提升了模型在金融数据集上的问答准确性和性能。</p>
<hr />
<h2 id="the-power-of-noise-redefining-retrieval-for-rag-systems"><a href="http://arxiv.org/abs/2401.14887v4">The Power of Noise: Redefining Retrieval for RAG Systems</a></h2>
<p>发布时间：2024-01-26</p>
<p>作者：Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, Fabrizio Silvestri</p>
<h4 id="_31">中文摘要：</h4>
<p>检索增强生成（RAG）作为一种方法，通过使用信息检索（IR）系统检索的相关段落或文档来增强原始提示，从而扩展大型语言模型（LLM）的预训练知识。RAG在生成式人工智能解决方案中变得越来越重要，尤其是在企业环境或知识不断更新且无法被LLM记忆的任何领域。我们认为，无论是密集型还是稀疏型的RAG系统的检索组件，都应得到研究社区的更多关注。因此，我们首次对RAG系统的检索策略进行了全面和系统的考察。我们特别关注在RAG解决方案中，信息检索系统应该检索的段落类型。我们的分析考虑了多个因素，如包含在提示上下文中的段落的关联性、位置和数量。这项工作的一个反直觉发现是，与查询不直接相关的检索器得分最高的文档（例如，不包含答案）会负面地影响LLM的有效性。更令人惊讶的是，我们发现向提示中添加随机文档可以将LLM的准确性提高高达35%。这些结果突出了在将检索与LLM集成时需要研究适当的策略，从而为该领域的未来研究奠定了基础。</p>
<h4 id="_32">一句话总结：</h4>
<p>本研究首次全面考察了检索增强生成（RAG）系统的检索策略，发现非直接相关的高分文档和随机文档的添加都能显著提升大型语言模型（LLM）的准确性。</p>
<hr />
<h2 id="from-rag-to-qa-rag-integrating-generative-ai-for-pharmaceutical-regulatory-compliance-process"><a href="http://arxiv.org/abs/2402.01717v1">From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process</a></h2>
<p>发布时间：2024-01-26</p>
<p>作者：Jaewoong Kim, Moohong Min</p>
<h4 id="_33">中文摘要：</h4>
<p>在制药行业中，合规性监管需要通过复杂且庞大的指南，这通常需要大量的人力资源。为了解决这些挑战，我们的研究引入了一种利用生成式人工智能和检索增强生成（RAG）方法的聊天机器人模型。这个聊天机器人被设计用来搜索与用户查询相关的指南文件，并根据检索到的指南提供答案。鉴于该领域对高可靠性的内在需求，我们提出了问答检索增强生成（QA-RAG）模型。在比较实验中，QA-RAG模型在准确性方面取得了显著提升，优于包括传统RAG方法在内的所有基线。本文详细介绍了QA-RAG的结构和性能评估，强调了其在制药行业合规性监管领域以及更广泛的应用潜力。我们的研究成果已公开供进一步的研究和开发使用。</p>
<h4 id="_34">一句话总结：</h4>
<p>本研究提出了一种基于QA-RAG模型的聊天机器人，用于提高制药行业合规性监管中的指南检索和问答准确性。</p>
<hr />
<h2 id="k-qa-a-real-world-medical-qa-benchmark"><a href="http://arxiv.org/abs/2401.14493v1">K-QA: A Real-World Medical Q&amp;A Benchmark</a></h2>
<p>发布时间：2024-01-25</p>
<p>作者：Itay Manes, Naama Ronn, David Cohen, Ran Ilan Ber, Zehavi Horowitz-Kugler, Gabriel Stanovsky</p>
<h4 id="_35">中文摘要：</h4>
<p>确保大型语言模型（LLMs）提供的响应的准确性至关重要，尤其是在临床环境中，错误的信息可能会直接影响到患者的健康。为了应对这一挑战，我们构建了K-QA数据集，该数据集包含1,212个来自K Health（一个由人工智能驱动的临床平台）的实时对话中的患者问题。我们聘请了一组内部医生来回答并手动将K-QA的一部分分解成独立的陈述。此外，我们制定了两个基于NLI的评价指标，近似于召回率和精确率：（1）全面性，衡量生成答案中必要临床信息的百分比；（2）幻觉率，衡量医生编写的响应中与LLM答案相矛盾的说法数量。最后，我们使用K-QA以及这些指标来评估几个最先进的模型，以及作者开发的情境学习效果和医学导向的增强检索方案。我们的发现表明，情境学习提高了模型的全面性，增强检索在减少幻觉方面是有效的。我们将K-QA提供给社区，以促进对医学准确的自然语言处理应用的研究。</p>
<h4 id="_36">一句话总结：</h4>
<p>本研究通过构建K-QA数据集和制定评估指标，验证了情境学习在提高大型语言模型全面性方面的作用，并证明了增强检索在减少模型幻觉方面的有效性。</p>
<hr />
<h2 id="accelerating-retrieval-augmented-language-model-serving-with-speculation"><a href="http://arxiv.org/abs/2401.14021v1">Accelerating Retrieval-Augmented Language Model Serving with Speculation</a></h2>
<p>发布时间：2024-01-25</p>
<p>作者：Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lanting Li, Phitchaya Mangpo Phothilimthana, Zhihao Jia</p>
<h4 id="_37">中文摘要：</h4>
<p>检索增强语言模型（RaLM）通过结合非参数知识库和参数化语言模型，展示了解决知识密集型自然语言处理（NLP）任务的潜力。与微调一个完全参数化模型不同，RaLM擅长以低成本适应最新数据和更好的源归因机制。在众多RaLM方法中，迭代RaLM由于检索器和语言模型之间更频繁的交互，提供了更好的生成质量。尽管如此，迭代RaLM通常由于频繁的检索步骤而遇到高开销。为此，我们提出了RaLMSpec，一个受推测启发框架，它通过推测检索和批量验证提供了对迭代RaLM的通用加速。通过进一步集成预取、最优推测步长调度器和异步验证，RaLMSpec可以自动充分利用加速潜力。对于原始迭代RaLM服务，在三个语言模型和四个下游问答数据集上的广泛评估表明，与基线相比，当检索器分别为精确密集检索器、近似密集检索器和稀疏检索器时，RaLMSpec可以实现1.75-2.39倍、1.04-1.39倍和1.31-1.77倍的加速比。对于KNN-LM服务，当检索器分别为精确密集检索器和近似密集检索器时，RaLMSpec可以分别实现高达7.59倍和2.45倍的加速比。</p>
<h4 id="_38">一句话总结：</h4>
<p>RaLMSpec通过推测检索和批量验证，实现了对迭代RaLM的加速，显著提高了检索增强语言模型在知识密集型NLP任务中的性能。</p>
<hr />
<h2 id="unims-rag-a-unified-multi-source-retrieval-augmented-generation-for-personalized-dialogue-systems"><a href="http://arxiv.org/abs/2401.13256v1">UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems</a></h2>
<p>发布时间：2024-01-24</p>
<p>作者：Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong Wang, Yufei Wang, Fei Mi, Jeff Z. Pan, Kam-Fai Wong</p>
<h4 id="_39">中文摘要：</h4>
<p>大型语言模型（LLMs）在许多自然语言理解和生成任务中展现出了卓越的能力。然而，个性化问题仍然是一个备受瞩目的特性，尤其是在涉及对话系统中的多个来源时。为了更好地规划和整合多个来源在生成个性化响应中的应用，我们首先将其分解为三个子任务：知识源选择、知识检索和响应生成。随后，我们提出了一种新颖的统一多源检索增强生成系统（UniMS-RAG）。具体来说，我们在训练过程中将这三个子任务以不同的公式统一到相同的序列到序列范式，以自适应地检索证据并在需要时使用特殊标记（称为行为标记和评估标记）来评估其相关性。使语言模型生成行为标记有助于与各种知识源进行交互，允许它们根据不同的任务需求调整其行为。同时，评估标记衡量对话上下文与检索到的证据之间的相关性。此外，我们精心设计了一种自我完善机制，通过迭代地考虑以下因素来优化生成的响应：1）生成的响应与检索到的证据之间的一致性得分；2）相关性得分。在两个个性化数据集（DuLeMon和KBP）上的实验表明，UniMS-RAG在知识源选择和响应生成任务上实现了最先进的性能，并以统一的方式将其自身作为检索器。提供了广泛的分析和讨论，以提供对个性化对话系统的一些新视角。</p>
<h4 id="_40">一句话总结：</h4>
<p>UniMS-RAG通过统一多源检索和生成，实现了个性化对话系统的最先进性能。</p>
<hr />
<h2 id="the-neglected-tails-in-vision-language-models"><a href="http://arxiv.org/abs/2401.12425v3">The Neglected Tails in Vision-Language Models</a></h2>
<p>发布时间：2024-01-23</p>
<p>作者：Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, Shu Kong</p>
<h4 id="_41">中文摘要：</h4>
<p>视觉语言模型（VLMs）在零样本识别方面表现出色，但它们在不同视觉概念上的性能差异很大。例如，尽管CLIP在ImageNet上实现了令人印象深刻的准确率（60-80%），但其性能对于像夜蛇这样的超过十个概念低于10%，这可能是由于它们在预训练数据中的存在有限。然而，测量VLMs大规模数据集中概念的频率是一项挑战。我们通过使用大型语言模型（LLMs）来计算包含这些概念同义词的预训练文本数量来解决这一问题。我们的分析证实，如LAION这样的流行数据集表现出长尾概念分布，导致VLMs的性能存在偏差。我们还发现，VLMs的下游应用，包括视觉聊天机器人（例如，GPT-4V）和文本到图像模型（例如，Stable Diffusion），通常无法识别或生成我们方法识别的罕见概念图像。为了减轻零样本VLMs的不平衡性能，我们提出了检索增强学习（REAL）。首先，REAL不是使用原始类名提示VLMs，而是使用在预训练文本中找到的最频繁同义词。这种简单的改变已经在九个基准数据集上超过了昂贵的人工工程和LLM丰富提示。其次，REAL在用概念同义词检索的小而平衡的预训练数据集上训练一个线性分类器。REAL超越了之前的零样本SOTA，使用了400倍更少的存储和10000倍更少的训练时间！</p>
<h4 id="_42">一句话总结：</h4>
<p>本研究提出了一种名为REAL的检索增强学习方法，通过使用概念同义词来改进零样本视觉语言模型（VLMs）的性能，显著提升了罕见概念的识别和生成能力。</p>
<hr />
<h2 id="revolutionizing-retrieval-augmented-generation-with-enhanced-pdf-structure-recognition"><a href="http://arxiv.org/abs/2401.12599v1">Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition</a></h2>
<p>发布时间：2024-01-23</p>
<p>作者：Demiao Lin</p>
<h4 id="_43">中文摘要：</h4>
<p>随着大型语言模型（LLMs）的快速发展，检索增强生成（RAG）已成为专业知识问答领域的主要方法。目前，主要的基础模型公司已经开放了嵌入和聊天API接口，LangChain等框架也已经集成了RAG过程。看起来RAG中的关键模型和步骤已经得到解决，从而引发了一个问题：专业知识问答系统现在是否正在接近完美？本文发现，当前的主要方法依赖于访问高质量文本语料库的前提。然而，由于专业文档主要存储在PDF中，PDF解析的低准确性严重影响了基于专业知识的问答的有效性。我们对来自相应现实世界专业文档的数百个问题进行了实证RAG实验。结果表明，ChatDOC，一个配备全景和精确PDF解析器的RAG系统，检索到更准确和完整的片段，从而提供了更好的答案。实证实验表明，ChatDOC在近47%的问题上优于基线，在38%的情况下与基线持平，只有15%的情况下表现不佳。这表明，我们可以通过增强PDF结构识别来革新RAG。</p>
<h4 id="_44">一句话总结：</h4>
<p>本文通过增强PDF结构识别，展示了如何革新检索增强生成（RAG）系统，使其在专业知识问答中取得显著成效。</p>
<hr />
<h2 id="blinded-by-generated-contexts-how-language-models-merge-generated-and-retrieved-contexts-when-knowledge-conflicts"><a href="http://arxiv.org/abs/2401.11911v6">Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?</a></h2>
<p>发布时间：2024-01-22</p>
<p>作者：Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng</p>
<h4 id="_45">中文摘要：</h4>
<p>虽然辅助信息已成为提升大型语言模型（LLMs）的关键，但对于LLMs如何合并这些上下文，特别是由LLMs生成和从外部来源检索的上下文，了解相对较少。为了研究这一问题，我们制定了一个系统框架来识别LLMs的响应是否归因于生成的上下文或检索的上下文。为了便于追踪响应的来源，我们构建了具有冲突上下文的语料库，即每个问题都配对生成和检索的上下文，但其中只有一个包含正确答案。我们的实验揭示，在多个LLMs（GPT-4/3.5和Llama2）中存在显著的偏差，倾向于选择生成的上下文，即使它们提供错误信息。我们进一步确定了导致这种偏差的两个关键因素：i) LLM生成的上下文通常与问题具有更高的相似性，增加了被选中的可能性；ii) 检索上下文中使用的分割过程破坏了其完整性，从而阻碍了其在LLMs中的充分利用。我们的分析增强了人们对LLMs如何合并不同上下文的理解，为推进当前的LLM增强方法提供了宝贵的见解，并突出了生成错误信息对检索增强LLMs的风险。</p>
<h4 id="_46">一句话总结：</h4>
<p>本研究揭示了LLMs在合并上下文时倾向于选择生成上下文的偏差，并分析了导致此偏差的关键因素。</p>
<hr />
<h2 id="interactive-ai-with-retrieval-augmented-generation-for-next-generation-networking"><a href="http://arxiv.org/abs/2401.11391v1">Interactive AI with Retrieval-Augmented Generation for Next Generation Networking</a></h2>
<p>发布时间：2024-01-21</p>
<p>作者：Ruichen Zhang, Hongyang Du, Yinqiu Liu, Dusit Niyato, Jiawen Kang, Sumei Sun, Xuemin Shen, H. Vincent Poor</p>
<h4 id="_47">中文摘要：</h4>
<p>随着人工智能（AI）的进步，Google Gemini和OpenAI Q*的出现标志着向通用人工智能（AGI）方向的转变。为了实现AGI，引入了交互式人工智能（IAI）的概念，它能够交互式地理解和响应不仅包括人类用户输入，还包括动态系统和网络条件。在本文中，我们探讨了在网络安全中IAI的集成和增强。我们首先全面回顾了AI的最新发展和未来展望，然后介绍了IAI的技术和组件。接着，我们探讨了IAI在下一代网络中的集成，重点关注隐式和显式交互如何增强网络功能、改善用户体验和促进高效网络管理。随后，我们提出了一种IAI驱动的网络管理和优化框架，该框架由环境、感知、行动和大脑单元组成。我们还设计了可插拔的大型语言模型（LLM）模块和检索增强生成（RAG）模块，以构建大脑单元中决策的知识库和上下文记忆。我们通过案例研究证明了该框架的有效性。最后，我们讨论了基于IAI网络的潜在研究方向。</p>
<h4 id="_48">一句话总结：</h4>
<p>本文探讨了交互式人工智能在网络安全中的应用，提出了一种IAI驱动的网络管理和优化框架，以提升网络功能、用户体验和高效管理。</p>
<hr />
<h2 id="prompt-rag-pioneering-vector-embedding-free-retrieval-augmented-generation-in-niche-domains-exemplified-by-korean-medicine"><a href="http://arxiv.org/abs/2401.11246v1">Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine</a></h2>
<p>发布时间：2024-01-20</p>
<p>作者：Bongsu Kang, Jundong Kim, Tae-Rim Yun, Chang-Eop Kim</p>
<h4 id="_49">中文摘要：</h4>
<p>我们提出了一种基于自然语言提示的检索增强生成（Prompt-RAG）方法，这是一种新颖的增强生成大型语言模型（LLMs）在特定领域性能的方法。传统的RAG方法大多需要向量嵌入，然而，基于通用LLM的嵌入表示是否适合特定领域仍然不确定。为了探索和证明这一点，我们比较了韩国医学（KM）和传统医学（CM）文档的向量嵌入，发现KM文档嵌入与标记重叠的相关性更高，而与人类评估的文档相关性较低，这与CM嵌入不同。Prompt-RAG与传统的RAG模型不同，它不需要嵌入向量。其性能通过问答（QA）聊天机器人应用进行评估，其中对响应的相关性、可读性和信息量进行评估。结果表明，Prompt-RAG在相关性和信息量方面优于现有模型，包括ChatGPT和基于传统向量嵌入的RAG。尽管存在内容结构和响应延迟等挑战，但LLM的进步预计将鼓励Prompt-RAG的使用，使其成为其他需要RAG方法的领域的有希望的工具。</p>
<h4 id="_50">一句话总结：</h4>
<p>Prompt-RAG是一种无需向量嵌入的检索增强生成方法，能够有效提升生成大型语言模型在特定领域的性能。</p>
<hr />
<h2 id="evaluating-and-enhancing-large-language-models-performance-in-domain-specific-medicine-osteoarthritis-management-with-docoa"><a href="http://arxiv.org/abs/2401.12998v1">Evaluating and Enhancing Large Language Models Performance in Domain-specific Medicine: Osteoarthritis Management with DocOA</a></h2>
<p>发布时间：2024-01-20</p>
<p>作者：Xi Chen, MingKe You, Li Wang, WeiZhi Liu, Yu Fu, Jie Xu, Shaoting Zhang, Gang Chen, Kang Li, Jian Li</p>
<h4 id="_51">中文摘要：</h4>
<p>本研究主要关注评估和提升大型语言模型（LLMs）在特定领域，尤其是针对如骨关节炎（OA）等复杂疾病的管理方面的临床能力。研究以骨关节炎管理作为案例研究，开发了一个针对特定领域的基准框架，该框架评估LLMs在从特定领域知识到现实临床场景中的临床应用等多个方面的表现。研究开发了一个名为DocOA的专门针对OA管理的LLM，该模型集成了检索增强生成（RAG）和指令提示。研究通过客观和人工评估比较了GPT-3.5、GPT-4和专门助手DocOA的性能。结果显示，像GPT-3.5和GPT-4这样的通用LLM在OA管理的特定领域效果较差，尤其是在提供个性化治疗建议方面。然而，DocOA表现出显著的改进。本研究引入了一个新的基准框架，该框架从多个方面评估LLMs在特定领域的功能，突出了通用LLM在临床环境中的局限性，并展示了定制方法在开发特定领域医疗LLMs方面的潜力。</p>
<h4 id="_52">一句话总结：</h4>
<p>本研究开发了一种针对骨关节炎管理的定制LLM，并证明了其在提供个性化治疗建议方面的有效性，同时揭示了通用LLM在特定医学领域的局限性。</p>
<hr />
<h2 id="chatqa-surpassing-gpt-4-on-conversational-qa-and-rag"><a href="http://arxiv.org/abs/2401.10225v4">ChatQA: Surpassing GPT-4 on Conversational QA and RAG</a></h2>
<p>发布时间：2024-01-18</p>
<p>作者：Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro</p>
<h4 id="_53">中文摘要：</h4>
<p>在这项工作中，我们介绍了ChatQA，一套在检索增强生成（RAG）和对话式问答（QA）方面优于GPT-4的模型。为了提升生成效果，我们提出了一种两阶段指令微调方法，显著提升了RAG的性能。为了有效检索，我们引入了一种针对对话式QA优化的密集检索器，其结果与替代的顶级查询重写模型相当，同时大幅降低了部署成本。我们还提出了ChatRAG Bench，它包含十个数据集，涵盖了RAG、表格相关QA、算术计算以及涉及无法回答问题的场景的全面评估。基于Llama2（一个比GPT-4更弱的基座模型）构建的ChatQA-1.0-70B（得分：54.14），在ChatRAG Bench上可以略微优于GPT-4-0613（得分：53.90）和GPT-4-Turbo-2024-04-09（得分：54.03），而不依赖于任何来自OpenAI GPT模型的合成数据。值得注意的是，Llama3-ChatQA-1.5-70B模型超越了GPT-4-Turbo-2024-04-09的准确性，实现了4.4%的提升。为了推进该领域的研究，我们将模型权重、指令微调数据、ChatRAG Bench和检索器开源供社区使用：https://chatqa-project.github.io/。</p>
<h4 id="_54">一句话总结：</h4>
<p>ChatQA模型通过创新的检索增强生成和指令微调技术，在对话式问答任务上超越了GPT-4，并开源了相关资源以促进研究发展。</p>
<hr />
<h2 id="climategpt-towards-ai-synthesizing-interdisciplinary-research-on-climate-change"><a href="http://arxiv.org/abs/2401.09646v1">ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change</a></h2>
<p>发布时间：2024-01-17</p>
<p>作者：David Thulke, Yingbo Gao, Petrus Pelser, Rein Brune, Rricha Jalota, Floris Fok, Michael Ramos, Ian van Wyk, Abdallah Nasir, Hayden Goldstein, Taylor Tragemann, Katie Nguyen, Ariana Fowler, Andrew Stanco, Jon Gabriel, Jordan Taylor, Dean Moro, Evgenii Tsymbalov, Juliette de Waal, Evgeny Matusov, Mudar Yaghi, Mohammad Shihadah, Hermann Ney, Christian Dugast, Jonathan Dotan, Daniel Erasmus</p>
<h4 id="_55">中文摘要：</h4>
<p>本文介绍了ClimateGPT，这是一个针对气候变化领域的大规模语言模型家族，它综合了跨学科气候变化研究。我们从300B个科学导向的数据集中从头开始训练了两个7B模型。对于第一个模型，在预训练期间包含了4.2B个特定领域的标记，第二个模型在预训练后适应了气候领域。此外，ClimateGPT-7B、13B和70B模型从Llama~2模型开始，在4.2B个特定领域的数据集上持续进行预训练。每个模型都在与气候科学家紧密合作创建的高质量、人工生成的特定领域数据集上进行指令微调。为了减少幻觉的数量，我们对模型进行了检索增强优化，并提出了分层检索策略。为了提高模型对非英语使用者的可访问性，我们提出了使用级联机器翻译的方法，并表明这种方法在性能上可以与原生多语言模型相媲美，同时更容易扩展到大量语言。此外，为了解决气候变化固有的跨学科特性，我们考虑了不同的研究视角。因此，该模型除了提供整体答案外，还能产生关注不同视角的深入答案。我们提出了一套自动化的气候特定基准来评估大型语言模型。在这些基准上，ClimateGPT-7B的表现与十倍大的Llama-2-70B聊天模型相当，同时在一般领域基准上的结果没有下降。我们的人评确认了我们在基准测试中观察到的趋势。所有模型均使用可再生能源进行训练和评估，并公开发布。</p>
<h4 id="_56">一句话总结：</h4>
<p>ClimateGPT是一种综合跨学科气候变化研究的大规模语言模型，能够提供深入且多视角的气候问题答案，并通过优化和扩展策略提高其可用性和准确性。</p>
<hr />
<h2 id="bibsonomy-meets-chatllms-for-publication-management-from-chat-to-publication-management-organizing-your-related-work-using-bibsonomy-llms"><a href="http://arxiv.org/abs/2401.09092v1">BibSonomy Meets ChatLLMs for Publication Management: From Chat to Publication Management: Organizing your related work using BibSonomy &amp; LLMs</a></h2>
<p>发布时间：2024-01-17</p>
<p>作者：Tom Völker, Jan Pfister, Tobias Koopmann, Andreas Hotho</p>
<h4 id="_57">中文摘要：</h4>
<p>随着科学文献库的不断增长，研究人员在发现、管理和标注相关出版物方面面临着重大挑战。传统的平台如Semantic Scholar、BibSonomy和Zotero提供了文献管理工具，但大部分需要手动、费时且易出错地输入标签和元数据。在这里，我们介绍了一种新颖的检索增强生成系统，该系统利用基于聊天的（large language models，LLMs）来简化和增强出版物管理的过程。它提供了一个统一的基于聊天的界面，允许直观地与各种后端进行交互，包括Semantic Scholar、BibSonomy和Zotero Webscraper。它支持两个主要用例：（1）探索性搜索与检索——利用LLMs搜索和检索特定和一般的科学出版物，同时解决内容幻觉和数据过时的问题；（2）编目与管理——通过自动化添加元数据和标签来帮助组织个人出版物库（在此例中为BibSonomy），同时便于手动编辑和更新。我们将在三个不同的设置中比较我们的系统与不同的LLM模型，包括用户研究，并展示其在不同指标上的优势。</p>
<h4 id="_58">一句话总结：</h4>
<p>该系统利用基于聊天的LLMs简化并增强科学文献的管理，提供探索性搜索、检索和编目功能，同时解决内容幻觉和数据过时问题。</p>
<hr />
<h2 id="knowledge-pyramid-a-novel-hierarchical-reasoning-structure-for-generalized-knowledge-augmentation-and-inference"><a href="http://arxiv.org/abs/2401.09070v1">Knowledge Pyramid: A Novel Hierarchical Reasoning Structure for Generalized Knowledge Augmentation and Inference</a></h2>
<p>发布时间：2024-01-17</p>
<p>作者：Qinghua Huang, Yongzhen Wang</p>
<h4 id="_59">中文摘要：</h4>
<p>知识图谱（KG）推理被认为是分析语义网络的有效手段，在信息检索、推荐、决策和人与机器交互等领域具有极大的实用价值。它在推荐、决策、问答、搜索等领域得到广泛应用。然而，先前的研究主要使用知识图谱中的低级知识进行推理，这可能导致推理泛化能力不足和鲁棒性差。为此，本文提出了一种新的推理方法，使用一种新颖的知识增强策略来提高知识图谱的泛化能力。该框架从低级知识中提取高级金字塔知识，并将其应用于多级层次知识图谱中的推理，本文中称为知识金字塔。我们使用所提出的方法测试了一些医学数据集，实验结果表明，所提出的知识金字塔提高了知识推理性能，具有更好的泛化能力。特别是，当训练样本较少时，推理精度可以显著提高。</p>
<h4 id="_60">一句话总结：</h4>
<p>本文提出了一种基于知识金字塔的知识增强推理方法，有效提高了知识图谱的泛化能力和推理精度。</p>
<hr />
<h2 id="rag-vs-fine-tuning-pipelines-tradeoffs-and-a-case-study-on-agriculture"><a href="http://arxiv.org/abs/2401.08406v3">RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture</a></h2>
<p>发布时间：2024-01-16</p>
<p>作者：Angels Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto de M. Estevão Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp, Bruno Silva, Swati Sharma, Vijay Aski, Ranveer Chandra</p>
<h4 id="_61">中文摘要：</h4>
<p>在构建大型语言模型（LLMs）的应用时，开发者通常采用两种方式来整合专有数据和领域特定数据：检索增强生成（RAG）和微调。RAG通过外部数据增强提示，而微调则将额外知识融入模型本身。然而，这两种方法的优缺点并未得到充分理解。本文提出了一种微调和RAG的流水线，并针对包括Llama2-13B、GPT-3.5和GPT-4在内的多个流行LLMs，展示了它们的权衡。我们的流水线包括从PDF中提取信息、生成问答、用于微调以及利用GPT-4评估结果等多个阶段。我们提出了评估RAG和微调流水线不同阶段性能的指标。我们对一个农业数据集进行了深入研究。农业行业在人工智能领域的渗透并不明显，我们研究了潜在的颠覆性应用——如果我们能够为农民提供特定地点的见解会怎样？我们的结果表明，我们的数据集生成流水线在捕捉地理特定知识方面是有效的，以及RAG和微调的定量和定性益处。在微调模型时，我们观察到准确率提高了超过6个百分点，并且这与RAG相结合，进一步提高了5个百分点的准确率。在特定实验中，我们还展示了微调模型利用来自不同地理区域的信息来回答特定问题，将答案相似度从47%提高到72%。总体而言，结果指出了如何使用LLMs构建的系统可以适应响应和整合特定行业关键维度的知识，为LLMs在其他工业领域的进一步应用铺平了道路。</p>
<h4 id="_62">一句话总结：</h4>
<p>本文提出了一种基于LLMs的微调和RAG流水线，通过在农业数据集上的应用，证明了其在特定领域知识获取和模型性能提升方面的有效性。</p>
<hr />
<h2 id="code-based-english-models-surprising-performance-on-chinese-qa-pair-extraction-task"><a href="http://arxiv.org/abs/2401.10286v3">Code-Based English Models Surprising Performance on Chinese QA Pair Extraction Task</a></h2>
<p>发布时间：2024-01-16</p>
<p>作者：Linghan Zheng, Hui Liu, Xiaojun Lin, Jiayuan Dong, Yue Sheng, Gang Shi, Zhiwei Liu, Hongwei Chen</p>
<h4 id="_63">中文摘要：</h4>
<p>在先前的研究中，基于代码的模型在推理密集型场景中始终优于基于文本的模型。在为检索增强生成（RAG）任务生成我们的知识库时，我们发现基于代码的模型在中文问答对提取任务中也表现出色。此外，我们的实验和设计的指标发现，包含一定量中文数据的基于代码的模型实现了更好的性能。此外，基于代码的英语模型在特定中文任务中的能力为关于哲学“中文房间”思想实验的讨论提供了独特的视角。</p>
<h4 id="_64">一句话总结：</h4>
<p>基于代码的模型在中文问答对提取任务中表现出色，并提供了对“中文房间”哲学实验的独特见解。</p>
<hr />
<h2 id="justilm-few-shot-justification-generation-for-explainable-fact-checking-of-real-world-claims"><a href="http://arxiv.org/abs/2401.08026v1">JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims</a></h2>
<p>发布时间：2024-01-16</p>
<p>作者：Fengzhu Zeng, Wei Gao</p>
<h4 id="_65">中文摘要：</h4>
<p>事实核查中的论证是指支持对某个断言真实性赋予的解释。然而，论证生成任务之前被过度简化为事实核查文章的摘要。因此，我们提出了一种基于检索到的证据生成论证的实用方法。我们提出了一个新的基准数据集ExClaim，用于解释现实世界中的断言（Explainable fact-checking of real-world Claims），并引入了JustiLM，这是一种基于检索增强的语言模型（retrieval-augmented Language Model），在训练过程中仅使用事实核查文章作为辅助资源进行少量样本的论证生成。实验表明，与强大的基线相比，JustiLM在论证生成方面取得了令人鼓舞的性能，并且还可以通过简单的扩展来增强真实性分类。</p>
<h4 id="_66">一句话总结：</h4>
<p>本文提出了一种基于检索增强语言模型的论证生成方法，通过使用事实核查文章作为辅助资源，在事实核查任务中实现了良好的性能。</p>
<hr />
<h2 id="the-chronicles-of-rag-the-retriever-the-chunk-and-the-generator"><a href="http://arxiv.org/abs/2401.07883v1">The Chronicles of RAG: The Retriever, the Chunk and the Generator</a></h2>
<p>发布时间：2024-01-15</p>
<p>作者：Paulo Finardi, Leonardo Avila, Rodrigo Castaldoni, Pedro Gengo, Celio Larcher, Marcos Piau, Pablo Costa, Vinicius Caridá</p>
<h4 id="_67">中文摘要：</h4>
<p>检索增强生成（RAG）已成为使大型语言模型（LLMs）访问外部数据的最流行范式之一，同时也是作为一种机制来降低幻觉的风险。在实施RAG时，可能会遇到诸如检索模型的有效集成、高效表示学习、数据多样性、计算效率优化、评估以及文本生成质量等挑战。鉴于所有这些挑战，每天都有新的技术出现以改进RAG，这使得对您的问题进行所有组合实验变得不切实际。在此背景下，本文提出了针对巴西葡萄牙语实施、优化和评估RAG的良好实践，重点关注建立简单的推理和实验流程。我们探索了多种方法来回答关于《哈利·波特》第一本书的问题。为了生成答案，我们使用了OpenAI的gpt-4、gpt-4-1106-preview、gpt-3.5-turbo-1106和Google的Gemini Pro。针对检索器的质量，我们的方法与基线相比，MRR@10提升了35.4%。在优化应用中的输入大小时，我们发现可以进一步将其提升2.4%。最后，我们展示了包含我们建议的RAG完整架构。</p>
<h4 id="_68">一句话总结：</h4>
<p>本文提出了一套针对巴西葡萄牙语的RAG实施、优化和评估的良好实践，显著提升了问答系统的性能。</p>
<hr />
<h2 id="flexibly-scaling-large-language-models-contexts-through-extensible-tokenization"><a href="http://arxiv.org/abs/2401.07793v1">Flexibly Scaling Large Language Models Contexts Through Extensible Tokenization</a></h2>
<p>发布时间：2024-01-15</p>
<p>作者：Ninglu Shao, Shitao Xiao, Zheng Liu, Peitian Zhang</p>
<h4 id="_69">中文摘要：</h4>
<p>大型语言模型（LLMs）需要足够的信息上下文来处理许多关键应用，例如检索增强生成和少样本学习。然而，由于窗口大小的限制，LLMs只能访问有限上下文内的信息。尽管可以通过微调来扩展上下文窗口的大小，但这将在训练和推理阶段带来巨大的成本。在本文中，我们提出了可扩展标记化作为实现LLMs上下文灵活缩放的一种替代方法。可扩展标记化充当标记化上下文和LLM之间的中间件，将原始标记嵌入转换为可扩展嵌入。这些嵌入为长上下文提供了更紧凑的表示，在此基础上，LLM能够在相同的上下文窗口中感知到更多信息。可扩展标记化还以其灵活性为特点：缩放因子可以在可行的范围内灵活确定，从而在推理时扩展任意上下文长度。此外，可扩展标记化作为一个即插即用的组件，不仅可以无缝地插入LLM本身，还可以插入其微调后的衍生版本，在完全保留LLM现有能力的同时，引入扩展的上下文信息。我们在长上下文语言建模和理解任务上进行了全面的实验，验证了可扩展标记化作为一种有效、高效、灵活且兼容的扩展LLM上下文的方法。我们的模型和源代码将公开提供。</p>
<h4 id="_70">一句话总结：</h4>
<p>本文提出了一种名为可扩展标记化的方法，通过灵活扩展上下文窗口，有效提升了大型语言模型在处理长上下文信息时的性能和效率。</p>
<hr />
<h2 id="graph-database-while-computationally-efficient-filters-out-quickly-the-esg-integrated-equities-in-investment-management"><a href="http://arxiv.org/abs/2401.07483v1">Graph database while computationally efficient filters out quickly the ESG integrated equities in investment management</a></h2>
<p>发布时间：2024-01-15</p>
<p>作者：Partha Sen, Sumana Sen</p>
<h4 id="_71">中文摘要：</h4>
<p>本研究评估了SQL、No-SQL和图数据库的数据库，以比较和对比其效率和性能。为了进行这项实验，数据从多个来源收集，包括股价和金融新闻。Python用作接口来连接和查询数据库（根据数据源文件结构创建数据库结构，将数据加载到表格、对象中，读取数据，连接PostgreSQL、ElasticSearch、Neo4j）。目的在于，现代应用（如LLM（大型语言模型）的RAG（检索增强生成）与机器学习、深度学习、NLP（自然语言处理）或决策分析）在计算上成本高昂。寻找更好的选项以减少资源消耗和时间来获得结果。研究发现，ESG（环境、社会和治理）的图数据库相对较好，可以考虑用于扩展分析，以将ESG整合到商业和投资中。实际意义在于，可以引入具有RAG架构模型的图机器学习，作为一个新的框架，在股票筛选过程中应用于投资组合管理，以降低LLM应用的计算成本。原创性/价值在于，从任何证券交易所的2000多家上市公司中筛选出选择性股票进行积极投资，尤其是在整合人工智能和ESG到商业和投资中，消耗更少的资源消耗，特别是内存和能源。</p>
<h4 id="_72">一句话总结：</h4>
<p>本研究通过评估不同数据库的效率，提出了一种新的框架，以降低大型语言模型在股票筛选过程中的计算成本，并实现ESG在商业和投资中的整合。</p>
<hr />
<h2 id="bridging-the-preference-gap-between-retrievers-and-llms"><a href="http://arxiv.org/abs/2401.06954v2">Bridging the Preference Gap between Retrievers and LLMs</a></h2>
<p>发布时间：2024-01-13</p>
<p>作者：Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky</p>
<h4 id="_73">中文摘要：</h4>
<p>大型语言模型（LLMs）在众多任务中表现出卓越的结果，检索增强生成（RAG）通过定位相关信息并将其放置到LLM的上下文窗口中，是一种有效的性能提升方法。然而，在RAG中检索器与LLM之间的关系仍被研究不足。大多数现有工作将检索器和LLM视为独立的组件，在检索“人友好”信息和组装“LLM友好”上下文之间留下差距。在本工作中，我们考察了一种新颖的桥梁机制。我们验证了在RAG背景下检索器的排名和选择假设，并提出了一种框架，该框架将监督学习和强化学习相结合，以训练一个优化检索器与LLM之间连接的桥梁模型。实证结果表明，我们的方法在问答和个性化生成任务中都表现出有效性。</p>
<h4 id="_74">一句话总结：</h4>
<p>本研究提出了一种结合监督学习和强化学习的桥梁模型，以优化检索器与大型语言模型之间的连接，从而在问答和个性化生成任务中提升RAG的性能。</p>
<hr />
<h2 id="fine-grained-hallucination-detection-and-editing-for-language-models"><a href="http://arxiv.org/abs/2401.06855v3">Fine-grained Hallucination Detection and Editing for Language Models</a></h2>
<p>发布时间：2024-01-12</p>
<p>作者：Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi</p>
<h4 id="_75">中文摘要：</h4>
<p>大型语言模型（LMs）容易产生事实错误，这些错误通常被称为幻觉。在本文中，我们介绍了一个关于幻觉的全面分类法，并认为幻觉以多种形式表现出来，每种形式都需要不同程度的仔细评估来验证其真实性。我们提出了一种新的自动细粒度幻觉检测任务，并构建了一个新的评估基准FavaBench，该基准包含约一千条针对三个LM在不同领域输出的细粒度人工判断。我们的分析显示，在信息搜索场景中，ChatGPT和Llama2-Chat（70B，7B）的大多数输出都表现出多种类型的幻觉。我们通过精心创建合成数据来检测和纠正细粒度幻觉，训练了FAVA，这是一种检索增强型LM。在我们的基准测试中，我们的自动和人工评估表明，FAVA在细粒度幻觉检测方面显著优于ChatGPT和GPT-4，而FAVA提出的编辑建议提高了LM生成文本的真实性。</p>
<h4 id="_76">一句话总结：</h4>
<p>本文提出了一种新的细粒度幻觉检测方法，并通过实验证明其能够有效提高大型语言模型生成文本的真实性。</p>
<hr />
<h2 id="seven-failure-points-when-engineering-a-retrieval-augmented-generation-system"><a href="http://arxiv.org/abs/2401.05856v1">Seven Failure Points When Engineering a Retrieval Augmented Generation System</a></h2>
<p>发布时间：2024-01-11</p>
<p>作者：Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek</p>
<h4 id="_77">中文摘要：</h4>
<p>软件工程师越来越多地通过一种称为检索增强生成（RAG）的策略，将语义搜索功能添加到应用程序中。RAG系统涉及找到与查询语义匹配的文档，然后将这些文档传递给大型语言模型（LLM）如ChatGPT，以使用LLM提取正确答案。RAG系统的目标包括：a) 减少LLM产生的幻觉回答问题，b) 将来源/参考文献与生成的回答联系起来，以及c) 消除对文档进行元数据标注的需求。然而，RAG系统受到信息检索系统固有的局限性和对LLM的依赖性的影响。在这篇论文中，我们通过三个来自不同领域（研究、教育和生物医学）的案例研究，对RAG系统的失败点进行了经验报告。我们分享了所学到的经验，并提出了在设计RAG系统时需要考虑的7个失败点。我们工作的两个关键启示是：1) RAG系统的验证只能在运行期间进行，2) RAG系统的鲁棒性是逐步演化的，而不是在开始时设计的。我们最后列出了软件工程社区在RAG系统上的潜在研究方向。</p>
<h4 id="_78">一句话总结：</h4>
<p>本文通过案例研究分析了RAG系统的失败点，强调了RAG系统在设计和验证过程中的关键考虑因素。</p>
<hr />
<h2 id="reinforcement-learning-for-optimizing-rag-for-domain-chatbots"><a href="http://arxiv.org/abs/2401.06800v1">Reinforcement Learning for Optimizing RAG for Domain Chatbots</a></h2>
<p>发布时间：2024-01-10</p>
<p>作者：Mandar Kulkarni, Praveen Tangarajan, Kyung Kim, Anusua Trivedi</p>
<h4 id="_79">中文摘要：</h4>
<p>随着大型语言模型（LLM）的出现，对话助手在特定领域应用中变得普遍。LLM通过训练获得了上下文问答的能力，而检索增强生成（RAG）进一步使机器人能够回答特定领域的问答。本文描述了一种基于RAG构建聊天机器人的方法，该机器人使用常见问题（FAQ）数据来回答用户的查询。我们使用infoNCE损失训练了一个内部检索嵌入模型，实验结果表明，与已知的通用公共嵌入模型相比，我们的内部模型在检索准确性和域外（OOD）查询检测方面都显著更好。作为一个LLM，我们使用了基于开放API的付费ChatGPT模型。我们注意到，之前检索到的上下文可以用于生成针对特定查询模式/序列的答案（例如，后续查询）。因此，有优化LLM标记数量和成本的空间。假设检索模型和LLM是固定的，我们使用强化学习（RL）来优化LLM标记的数量。具体来说，我们提出了一种RAG之外的基于策略的模型，它通过策略动作与RAG管道交互，并更新策略以优化成本。策略模型可以执行两种动作：获取FAQ上下文或跳过检索。我们使用基于开放API的GPT-4作为奖励模型。然后，我们使用多个训练聊天会话在策略梯度上训练策略模型。作为一个策略模型，我们尝试了公开的gpt-2模型和内部BERT模型。通过结合基于RL的优化和相似度阈值，我们能够在略微提高准确度的同时实现显著的成本节约。尽管我们展示了FAQ聊天机器人的结果，但提出的RL方法是一般的，可以与任何现有的RAG管道进行实验。</p>
<h4 id="_80">一句话总结：</h4>
<p>本文提出了一种基于RAG和强化学习的聊天机器人构建方法，通过优化LLM标记数量和成本，实现了在FAQ问答场景中的成本节约和准确度提升。</p>
<hr />
<h2 id="rewriting-the-code-a-simple-method-for-large-language-model-augmented-code-search"><a href="http://arxiv.org/abs/2401.04514v2">Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search</a></h2>
<p>发布时间：2024-01-09</p>
<p>作者：Haochen Li, Xin Zhou, Zhiqi Shen</p>
<h4 id="_81">中文摘要：</h4>
<p>在代码搜索中，生成增强检索（GAR）框架通过生成示例代码片段来增强查询，已成为解决代码片段与自然语言查询之间模态不匹配这一主要挑战的有前景策略，尤其是在大型语言模型（LLMs）展示的代码生成能力方面。然而，我们的初步调查表明，这种LLM增强框架带来的改进是有限的。这种限制可能归因于生成代码虽然在功能上准确，但通常在代码库中的真实代码风格上表现出明显的偏差。在本文中，我们扩展了基础GAR框架，并提出了一种简单而有效的方法，该方法在代码库内对代码进行重写（ReCo），以实现风格归一化。实验结果表明，ReCo在多种搜索场景中显著提高了检索准确性，包括稀疏检索（高达35.7%）、零样本密集检索（高达27.6%）和微调密集检索（高达23.6%）。为了进一步阐明ReCo的优势并激发代码风格归一化研究，我们引入了代码风格相似度，这是第一个用于量化代码风格相似度的指标。值得注意的是，我们的实证研究发现，现有指标在捕捉风格细微差别方面存在不足。源代码和数据可在\url{https://github.com/Alex-HaochenLi/ReCo}获取。</p>
<h4 id="_82">一句话总结：</h4>
<p>本文提出了一种基于代码库重写的代码风格归一化方法（ReCo），显著提高了代码搜索的检索准确性。</p>
<hr />
<h2 id="unsupervised-hard-negative-augmentation-for-contrastive-learning"><a href="http://arxiv.org/abs/2401.02594v1">Unsupervised hard Negative Augmentation for contrastive learning</a></h2>
<p>发布时间：2024-01-05</p>
<p>作者：Yuxuan Shu, Vasileios Lampos</p>
<h4 id="_83">中文摘要：</h4>
<p>我们提出了无监督硬负样本增强（Unsupervised hard Negative Augmentation，简称UNA）方法，该方法基于词频-逆文档频率（Term Frequency-Inverse Document Frequency，简称TF-IDF）检索模型生成合成负样本。UNA利用TF-IDF分数来确定句子中词语的感知重要性，然后通过替换这些词语来生成负样本。我们的实验表明，使用UNA训练的模型在语义文本相似度任务中的整体性能得到提升。当将UNA与释义增强相结合时，可以获得额外的性能提升。进一步的结果显示，我们的方法与不同的骨干模型兼容。消融研究也支持在负样本增强中采用TF-IDF驱动的控制。</p>
<h4 id="_84">一句话总结：</h4>
<p>UNA通过TF-IDF模型生成负样本，有效提升了语义文本相似度任务的模型性能。</p>
<hr />
<h2 id="generative-large-language-models-are-autonomous-practitioners-of-evidence-based-medicine"><a href="http://arxiv.org/abs/2401.02851v1">Generative Large Language Models are autonomous practitioners of evidence-based medicine</a></h2>
<p>发布时间：2024-01-05</p>
<p>作者：Akhil Vaid, Joshua Lampert, Juhee Lee, Ashwin Sawant, Donald Apakama, Ankit Sakhuja, Ali Soroush, Denise Lee, Isotta Landi, Nicole Bussola, Ismail Nabeel, Robbie Freeman, Patricia Kovatch, Brendan Carr, Benjamin Glicksberg, Edgar Argulian, Stamatios Lerakis, Monica Kraft, Alexander Charney, Girish Nadkarni</p>
<h4 id="_85">中文摘要：</h4>
<p>背景：循证医学（EBM）是现代临床实践的基础，要求临床医生不断更新知识并应用最佳临床证据进行患者护理。由于医学研究的快速发展，循证医学的实践面临着挑战，导致临床医生信息过载。人工智能（AI），特别是生成式大型语言模型（LLMs），为管理这种复杂性提供了一种有希望的解决方案。
方法：本研究涉及对各种专业领域的真实临床案例进行整理，将其转换为.json文件进行分析。使用了包括ChatGPT 3.5和4、Gemini Pro等专有模型以及LLaMA v2和Mixtral-8x7B等开源模型。这些模型配备了从案例文件中检索信息和做出类似临床医生在现实世界中必须进行的临床决策的工具。模型性能根据最终答案的正确性、工具的合理使用、符合指南的程度以及对幻觉的抵抗力进行评估。
结果：GPT-4在临床环境中的自主操作能力最强，通常在安排相关检查和符合临床指南方面更为有效。观察到模型在处理复杂指南和诊断细微差别方面的局限性。检索增强生成使推荐更加符合患者和医疗保健系统。
结论：LLMs可以被设计成循证医学的自主从业者。它们利用工具的能力可以被利用来与真实世界医疗保健系统的基础设施互动，并以指南为导向的方式执行患者管理任务。提示工程可能有助于进一步发挥这种潜力，并改变临床医生和患者的医疗保健。</p>
<h4 id="_86">一句话总结：</h4>
<p>本研究表明，生成式大型语言模型（LLMs）在循证医学实践中具有潜在的应用价值，能够辅助临床医生进行患者管理和决策。</p>
<hr />
<h2 id="beyond-extraction-contextualising-tabular-data-for-efficient-summarisation-by-language-models"><a href="http://arxiv.org/abs/2401.02333v3">Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models</a></h2>
<p>发布时间：2024-01-04</p>
<p>作者：Uday Allu, Biddwan Ahmed, Vishesh Tripathi</p>
<h4 id="_87">中文摘要：</h4>
<p>传统的检索增强生成（RAG）架构在从各种文档中检索信息方面已被证明是有效的。然而，在处理复杂的表格查询时，尤其是在包含复杂表格结构的PDF文档中，会面临挑战。本研究提出了一种创新的方法来提高基于RAG系统的复杂表格查询的准确性。我们的方法涉及将PDF存储在检索数据库中，并单独提取表格内容。提取的表格经过上下文丰富处理，将标题与相应的值连接起来。为了确保对丰富数据的全面理解，我们在RAG架构中使用了微调后的Llama-2-chat语言模型进行摘要。此外，我们通过一次提示使用ChatGPT 3.5 API通过上下文感知增强表格数据。然后，这些丰富数据与其他PDF一起输入到检索数据库中。我们的方法旨在显著提高复杂表格查询的精确度，为信息检索中长期存在的挑战提供了一种有希望的解决方案。</p>
<h4 id="_88">一句话总结：</h4>
<p>本研究通过结合上下文丰富和先进的语言模型，显著提升了基于RAG架构的复杂表格查询的准确性。</p>
<hr />
<h2 id="instruct-imagen-image-generation-with-multi-modal-instruction"><a href="http://arxiv.org/abs/2401.01952v1">Instruct-Imagen: Image Generation with Multi-modal Instruction</a></h2>
<p>发布时间：2024-01-03</p>
<p>作者：Hexiang Hu, Kelvin C. K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, Ming-Wei Chang, Xuhui Jia</p>
<h4 id="_89">中文摘要：</h4>
<p>本文提出了一种名为instruct-imagen的模型，该模型旨在解决异构图像生成任务，并能泛化到未见过的任务。我们引入了用于图像生成的多模态指令，这是一种精确的任务表示，能够用自然语言将不同的模态（例如，文本、边缘、风格、主题等）融合在一起，从而能够以统一格式标准化丰富的生成意图。然后，我们通过使用两阶段框架微调一个预训练的文本到图像扩散模型来构建instruct-imagen。首先，我们使用检索增强训练来调整模型，以增强模型基于外部多模态上下文进行生成的能力。随后，我们在需要视觉语言理解的各种图像生成任务上微调调整后的模型（例如，主题驱动生成等），每个任务都配有一个包含任务本质的多模态指令。在多个图像生成数据集上的人评结果显示，instruct-imagen在特定领域内与先前任务特定的模型相当，并在未见过的和更复杂的任务中展现出有前景的泛化能力。</p>
<h4 id="_90">一句话总结：</h4>
<p>本文提出的instruct-imagen模型通过多模态指令实现了图像生成任务的泛化，并在未见过的复杂任务中展现出良好的性能。</p>
<hr />
<h2 id="question-answering-based-summarization-of-electronic-health-records-using-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2401.01469v1">Question-Answering Based Summarization of Electronic Health Records using Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-01-03</p>
<p>作者：Walid Saba, Suzanne Wendelken, James. Shanahan</p>
<h4 id="_91">中文摘要：</h4>
<p>电子健康记录（EHRs）的摘要可以显著减少患者和医务人员在屏幕上的时间。近年来，EHRs的摘要采用了最先进的神经模型构建的机器学习流程。然而，这些模型产生的结果并不令人满意，这归因于获取足够标注数据用于训练的困难。此外，由于现代大型语言模型（LLMs）中的注意力机制在输入规模方面增加了二次复杂性，因此在摘要中考虑EHR的整个内容导致了性能不佳。我们提出了一种方法，通过结合语义搜索、检索增强生成（RAG）和利用最新LLMs的问答来缓解这些缺点。在我们的方法中，摘要是对由领域专家（SMEs）认为重要的特定问题的答案的提取。我们的方法非常高效；需要最少的训练甚至不需要训练；不遭受LLMs的“幻觉”问题；并且确保了多样性，因为摘要将不会包含重复的内容，而是对特定问题的不同答案。</p>
<h4 id="_92">一句话总结：</h4>
<p>本文提出了一种基于语义搜索、RAG和问答的EHR摘要方法，通过提取领域专家认为重要的特定问题的答案，有效缓解了传统方法中存在的标注数据不足和模型复杂度问题。</p>
<hr />
<h2 id="concurrent-brainstorming-hypothesis-satisfying-an-iterative-framework-for-enhanced-retrieval-augmented-generation-r2cbr3h-sr"><a href="http://arxiv.org/abs/2401.01835v1">Concurrent Brainstorming &amp; Hypothesis Satisfying: An Iterative Framework for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR)</a></h2>
<p>发布时间：2024-01-03</p>
<p>作者：Arash Shahmansoori</p>
<h4 id="_93">中文摘要：</h4>
<p>针对综合信息检索的复杂性，本研究提出了一种创新性的、迭代的检索增强生成系统。我们的方法独特地将基于向量空间的重排序机制与并发头脑风暴相结合，以加速检索高度相关的文档，从而简化潜在查询的生成。这为我们的新颖混合流程奠定了基础，该流程协同结合假设制定与令人满意的决策策略，以确定内容充分性，利用基于思维链的提示技术。这一统一的假设-满意阶段智能地提炼信息，以确定用户查询是否得到了满意的解答。达到这一标准后，系统将其输出精炼为简洁的表示，以最小的冗余最大化概念密度。工作流程的迭代性质提高了过程效率和准确性。关键的是，头脑风暴阶段的并发性显著加速了递归操作，促进了快速收敛到解决方案的满意度。与传统方法相比，我们的系统在计算时间和成本效益方面表现出显著改进。这项研究推进了智能检索系统的前沿技术，为知识密集型应用中的资源高效信息提取和抽象设定了新的基准。</p>
<h4 id="_94">一句话总结：</h4>
<p>本研究提出了一种创新的检索增强生成系统，通过结合重排序机制和头脑风暴，显著提高了信息检索的效率和准确性。</p>
<hr />
<h2 id="de-hallucinator-mitigating-llm-hallucinations-in-code-generation-tasks-via-iterative-grounding"><a href="http://arxiv.org/abs/2401.01701v3">De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks via Iterative Grounding</a></h2>
<p>发布时间：2024-01-03</p>
<p>作者：Aryaz Eghbali, Michael Pradel</p>
<h4 id="_95">中文摘要：</h4>
<p>大型语言模型（LLMs）在公开可用的源代码数据集上训练后，在代码生成任务中达到了新的技术水平。然而，这些模型大多不了解特定项目中的代码，这阻碍了模型充分利用现有API。相反，LLMs经常发明或“虚构”不存在的API或产生已存在代码的变体。本文提出了一种名为De-Hallucinator的技术，通过检索合适的API引用和迭代地在提示中查询模型，以越来越合适的上下文信息来使LLM的预测更加可靠。这种方法利用了观察到的现象，即LLM的预测往往与期望的代码相似，但它们未能正确引用已存在的API。De-Hallucinator自动识别与模型初始预测相关的特定项目API引用，并将这些引用添加到提示中。与检索增强生成（RAG）不同，我们的方法使用模型的初始预测来迭代检索越来越合适的API引用。我们的评估将这种方法应用于两个任务：预测Python中的API使用和生成JavaScript测试。我们表明，De-Hallucinator在五个LLMs上持续改进了生成的代码。特别是，这种方法将编辑距离提高了23.3-50.6%，并将正确预测的API使用的召回率提高了23.9-61.0%，对于代码补全，并且将因虚构而最初失败的修复测试数量提高了63.2%，从而将测试生成的语句覆盖率提高了15.5%。</p>
<h4 id="_96">一句话总结：</h4>
<p>De-Hallucinator通过结合API引用检索和上下文信息迭代查询，有效减少了大型语言模型在代码生成任务中的虚构现象，显著提升了代码质量和测试覆盖率。</p>
<hr />
<h2 id="enhancing-multilingual-information-retrieval-in-mixed-human-resources-environments-a-rag-model-implementation-for-multicultural-enterprise"><a href="http://arxiv.org/abs/2401.01511v1">Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A RAG Model Implementation for Multicultural Enterprise</a></h2>
<p>发布时间：2024-01-03</p>
<p>作者：Syed Rameel Ahmad</p>
<h4 id="_97">中文摘要：</h4>
<p>大型语言模型的兴起彻底改变了信息检索领域，开启了知识获取的新时代。尽管这些模型在提供开放世界知识方面表现出色，但在不同语言环境和不同文化素养水平下有效地提取答案仍然是一个巨大的挑战。检索增强生成（RAG）作为一种有潜力的解决方案，弥合了信息可用性和多语言理解之间的差距。然而，在现实场景中部署RAG模型需要仔细考虑各种因素。本文探讨了在多元文化环境中实施RAG模型所面临的关键挑战。我们深入探讨了包括数据喂养策略、及时更新、幻觉缓解、错误响应预防和交付速度优化等基本考虑因素。我们的工作涉及整合各种工具，精心组合以促进RAG模型在多元文化组织环境中跨语言和文化素养水平的无缝采用。通过在方法上的战略调整，我们不仅实现了有效性，还实现了效率，确保以符合多语言和多文化环境独特需求的方式加速和准确传递信息。</p>
<h4 id="_98">一句话总结：</h4>
<p>本文提出了一种在多元文化环境中实施检索增强生成（RAG）模型的方法，以优化信息检索和生成过程，满足多语言和文化素养水平的需求。</p>
<hr />
<h2 id="a-comprehensive-survey-of-hallucination-mitigation-techniques-in-large-language-models"><a href="http://arxiv.org/abs/2401.01313v3">A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models</a></h2>
<p>发布时间：2024-01-02</p>
<p>作者：S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das</p>
<h4 id="_99">中文摘要：</h4>
<p>随着大型语言模型（LLMs）在生成类似人类文本的能力上不断进步，一个关键挑战仍然存在，即它们倾向于产生看似事实但缺乏根据的内容，这种现象被称为幻觉。这个问题可以说是阻碍这些强大的LLMs安全部署到影响人们生活的现实世界生产系统中的最大障碍。在将LLMs广泛应用于实际环境中的过程中，很大程度上依赖于解决和减轻幻觉问题。与专注于有限任务的传统AI系统不同，LLMs在训练过程中接触到了大量的在线文本数据。虽然这使它们能够展现出令人印象深刻的语言流畅性，但也意味着它们能够从训练数据的偏差中推断信息，误解模糊的提示，或者修改信息以表面上与输入相匹配。当我们依赖于语言生成能力进行敏感应用，如总结医疗记录、财务分析报告等时，这变得非常令人担忧。本文对超过32种旨在减轻LLMs中幻觉的技术进行了全面综述。其中值得注意的是检索增强生成（Lewis et al, 2021）、知识检索（Varshney et al,2023）、CoNLI（Lei et al, 2023）和CoVe（Dhuliawala et al, 2023）。此外，我们引入了一个详细的分类法，根据各种参数对这些方法进行分类，如数据集利用、常见任务、反馈机制和检索器类型。这种分类有助于区分针对LLMs中幻觉问题设计的各种不同方法。此外，我们还分析了这些技术固有的挑战和局限性，为未来在LLMs领域解决幻觉和相关现象的研究提供了坚实的基础。</p>
<h4 id="_100">一句话总结：</h4>
<p>本文全面综述了减轻大型语言模型幻觉的多种技术，并分析了其挑战和局限性，为未来研究提供了基础。</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
    
  </body>
</html>