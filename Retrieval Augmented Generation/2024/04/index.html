
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../05/">
      
      
        <link rel="next" href="../03/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.31">
    
    
      
        <title>2024-04(91) - ArXiv Tracker</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="ArXiv Tracker" class="md-header__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ArXiv Tracker
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2024-04(91)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Emission%20Trading%20System/2024/06/" class="md-tabs__link">
          
  
  Emission Trading System

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../08/" class="md-tabs__link">
          
  
  Retrieval Augmented Generation

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="ArXiv Tracker" class="md-nav__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ArXiv Tracker
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Emission Trading System
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Emission Trading System
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2016/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-02(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Retrieval Augmented Generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Retrieval Augmented Generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-08(17)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(130)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(158)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-05(123)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2024-04(91)
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-03(72)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(89)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(53)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(36)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-11(42)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(53)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(23)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-08(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-07(14)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(15)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(38)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-04(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-02(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-01(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(9)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(20)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-09(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-08(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-07(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-05(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-04(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-03(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-01(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-12(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-09(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-08(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-07(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-06(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-09(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-06(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-04(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-12(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-05(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-04(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-12(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-04(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2015/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2012/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2012-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2009/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2009-03(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="retrieval-augmented-generation-202404">Retrieval Augmented Generation - 2024年04月</h1>
<h2 id="towards-a-search-engine-for-machines-unified-ranking-for-multiple-retrieval-augmented-large-language-models"><a href="http://arxiv.org/abs/2405.00175v1">Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models</a></h2>
<p>发布时间：2024-04-30</p>
<p>作者：Alireza Salemi, Hamed Zamani</p>
<h4 id="_1">中文摘要：</h4>
<p>本文介绍了一个名为uRAG的框架，该框架具有统一的检索引擎，为多个下游检索增强生成（RAG）系统提供服务。每个RAG系统都会根据独特目的消费检索结果，例如开放域问答、事实核查、实体链接和关系抽取。我们引入了一种通用的训练指南，该指南标准化了搜索引擎与参与优化检索模型的下游RAG系统之间的通信。这为我们构建了一个大规模实验生态系统奠定了基础，该生态系统由18个参与训练的RAG系统和18个使用uRAG作为搜索引擎新用户的未知RAG系统组成。利用这个实验生态系统，我们回答了多个基本研究问题，这些问题有助于我们更好地理解开发面向机器的搜索引擎的机遇和挑战。</p>
<h4 id="_2">一句话总结：</h4>
<p>本文提出了一种名为uRAG的统一检索增强生成框架，通过构建大规模实验生态系统，深入探讨了面向机器的搜索引擎开发中的机遇与挑战。</p>
<hr />
<h2 id="rag-and-rau-a-survey-on-retrieval-augmented-language-model-in-natural-language-processing"><a href="http://arxiv.org/abs/2404.19543v1">RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing</a></h2>
<p>发布时间：2024-04-30</p>
<p>作者：Yucheng Hu, Yuxing Lu</p>
<h4 id="_3">中文摘要：</h4>
<p>大型语言模型（LLMs）在自然语言处理（NLP）领域推动了显著的进步，但它们仍面临幻觉和需要特定领域知识等挑战。为了缓解这些问题，最近的方法将来自外部资源的信息与LLMs相结合，显著提高了它们在NLP任务中的性能。这篇综述论文针对检索增强语言模型（RALMs）的缺乏全面概述的问题，包括检索增强生成（RAG）和检索增强理解（RAU），深入探讨了它们的范式、演变、分类和应用。论文讨论了RALMs的基本组件，包括检索器、语言模型和增强，以及它们之间的交互如何导致不同的模型结构和应用。RALMs在各种任务中显示出其效用，从翻译和对话系统到知识密集型应用。综述包括了RALMs的几种评估方法，强调了在评估中稳健性、准确性和相关性的重要性。它还承认了RALMs的局限性，尤其是在检索质量和计算效率方面，并为未来的研究提供了方向。总之，这篇综述旨在为RALMs、它们的潜力以及它们在NLP未来发展的途径提供一个结构化的洞察。</p>
<h4 id="_4">一句话总结：</h4>
<p>本文综述了检索增强语言模型（RALMs）的范式、演变、分类和应用，并探讨了其在NLP任务中的潜力和局限性。</p>
<hr />
<h2 id="grammar-grounded-and-modular-methodology-for-assessment-of-closed-domain-retrieval-augmented-language-model"><a href="http://arxiv.org/abs/2404.19232v5">GRAMMAR: Grounded and Modular Methodology for Assessment of Closed-Domain Retrieval-Augmented Language Model</a></h2>
<p>发布时间：2024-04-30</p>
<p>作者：Xinzhe Li, Ming Liu, Shang Gao</p>
<h4 id="_5">中文摘要：</h4>
<p>检索增强生成（RAG）系统在各种行业中得到了积极的研究和部署，用于查询特定领域的知识库。然而，由于特定领域查询和相应真实情况的稀缺，以及缺乏诊断失败原因的系统方法——无论是知识缺陷还是与系统鲁棒性相关的问题——评估这些系统面临着独特的挑战。为了解决这些挑战，我们引入了GRAMMAR（基于和模块化方法评估RAG），这是一个包含两个关键要素的评估框架：1）一个数据生成过程，利用关系数据库和大型语言模型（LLMs）高效地生成可扩展的查询-答案对以进行评估。这种方法促进了查询逻辑与语言变体的分离，使得可以测试与非鲁棒文本形式相关的假设；2）一个评估框架，它区分了知识差距和鲁棒性，并能够识别有缺陷的模块。我们的实证结果强调了当前无参考评估方法的局限性以及GRAMMAR准确识别模型脆弱性的可靠性。有关实现细节，请参阅我们的GitHub仓库：https://github.com/xinzhel/grammar。</p>
<h4 id="_6">一句话总结：</h4>
<p>GRAMMAR是一个评估RAG系统鲁棒性和知识差距的框架，通过生成可扩展的查询-答案对和区分评估，能够准确识别模型脆弱性。</p>
<hr />
<h2 id="privcomp-kg-leveraging-knowledge-graph-and-large-language-models-for-privacy-policy-compliance-verification"><a href="http://arxiv.org/abs/2404.19744v1">PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for Privacy Policy Compliance Verification</a></h2>
<p>发布时间：2024-04-30</p>
<p>作者：Leon Garza, Lavanya Elluri, Anantaa Kotal, Aritran Piplai, Deepti Gupta, Anupam Joshi</p>
<h4 id="_7">中文摘要：</h4>
<p>数据保护和隐私在数字时代变得越来越重要。许多公司依赖第三方供应商和服务提供商来执行其运营中的关键功能，包括数据处理和存储等任务。然而，这种依赖引入了潜在的安全漏洞，因为这些供应商的安全措施和实践可能并不总是符合监管机构预期的标准。企业通常在法律制裁的威胁下，需要确保遵守不断变化的监管规则。由于这些规则的复杂性，解释和实施这些规则带来了挑战。监管文件内容广泛，需要大量的努力来解释，而供应商制定的隐私政策通常缺乏全面法律合规所需的详细内容，导致模糊性。为了确保对监管要求的简洁解释以及组织隐私政策与这些规定的合规性，我们提出了一种基于大型语言模型（LLM）和语义网的方法，用于隐私合规。在本文中，我们开发了新型的隐私政策合规验证知识图谱，PrivComp-KG。它被设计用来高效地存储和检索有关隐私政策、监管框架以及与隐私法律环境相关的特定领域知识的综合信息。利用检索增强生成技术，我们识别了隐私政策中与相应监管规则相对应的相关部分。这些关于个别隐私政策的信息被填充到PrivComp-KG中。结合领域上下文和规则，PrivComp-KG可以被查询以检查每个供应商对相关政策法规的隐私政策合规性。我们通过验证不同组织的隐私政策文件合规性来证明PrivComp-KG的相关性。</p>
<h4 id="_8">一句话总结：</h4>
<p>本文提出了一种基于大型语言模型和语义网的知识图谱方法，用于验证企业隐私政策的合规性，以应对数字时代数据保护和隐私的挑战。</p>
<hr />
<h2 id="ecc-analyzer-extract-trading-signal-from-earnings-conference-calls-using-large-language-model-for-stock-performance-prediction"><a href="http://arxiv.org/abs/2404.18470v1">ECC Analyzer: Extract Trading Signal from Earnings Conference Calls using Large Language Model for Stock Performance Prediction</a></h2>
<p>发布时间：2024-04-29</p>
<p>作者：Yupeng Cao, Zhi Chen, Qingyun Pei, Prashant Kumar, K. P. Subbalakshmi, Papa Momar Ndiaye</p>
<h4 id="_9">中文摘要：</h4>
<p>在金融分析领域，利用非结构化数据，如收益电话会议（ECCs），来预测股票表现是一个关键的挑战，这个挑战吸引了学术界和投资者的关注。尽管先前的研究已经使用基于深度学习的模型来获得ECCs的一般视图，但它们往往无法捕捉到详细和复杂的信息。我们的研究引入了一个新颖的框架：\textbf{ECC Analyzer}（ECC分析器），它结合了大型语言模型（LLMs）和多模态技术来提取更丰富、更具预测性的见解。该模型首先通过检测音频中的音调和语调变化来总结转录本的结构和分析说话者的模式和信心水平，从而帮助投资者形成对ECCs的整体感知。此外，该模型使用基于检索增强生成（RAG）的方法，从专家的角度细致地提取对股票表现有重大影响的重点，提供更针对性的分析。该模型更进一步，通过添加额外的分析层，如情感和音频片段特征，来丰富这些提取的重点。通过整合这些见解，ECC Analyzer执行了股票表现的多元任务预测，包括波动性、风险价值（VaR）和不同时间间隔的回报。结果显示，我们的模型优于传统的分析基准，证实了在金融分析中使用高级LLM技术的有效性。</p>
<h4 id="_10">一句话总结：</h4>
<p>本研究提出的ECC分析器通过结合LLMs和多模态技术，有效地从非结构化数据中提取预测性见解，从而提高了金融分析中股票表现的预测能力。</p>
<hr />
<h2 id="clarinet-augmenting-language-models-to-ask-clarification-questions-for-retrieval"><a href="http://arxiv.org/abs/2405.15784v1">CLARINET: Augmenting Language Models to Ask Clarification Questions for Retrieval</a></h2>
<p>发布时间：2024-04-28</p>
<p>作者：Yizhou Chi, Jessy Lin, Kevin Lin, Dan Klein</p>
<h4 id="_11">中文摘要：</h4>
<p>用户经常提出模糊不清的请求，需要进一步澄清。本研究探讨了在信息检索环境中提出澄清问题的方法，在这种环境中，系统常常面临模糊的搜索查询，并且将检索模型中的不确定性转化为自然语言问题具有挑战性。我们提出了CLARINET系统，该系统能够通过选择能够最大化正确候选者确定性的问题来提出具有信息量的澄清问题。我们的方法通过增强大型语言模型（LLM）以对检索分布进行条件化，并端到端微调以生成在每个回合中最大化真实候选者排名的问题。在评估用户搜索书籍的真实检索数据集时，我们的系统在检索成功率方面优于传统的启发式方法，如信息增益，提高了17%，并且相对于简单的提示LLM提高了39%。</p>
<h4 id="_12">一句话总结：</h4>
<p>CLARINET系统通过增强大型语言模型，提出具有信息量的澄清问题，显著提高了信息检索的成功率。</p>
<hr />
<h2 id="tabular-embedding-model-tem-finetuning-embedding-models-for-tabular-rag-applications"><a href="http://arxiv.org/abs/2405.01585v1">Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications</a></h2>
<p>发布时间：2024-04-28</p>
<p>作者：Sujit Khanna, Shishir Subedi</p>
<h4 id="_13">中文摘要：</h4>
<p>近年来，大型语言模型在数学、代码生成和通用推理等领域展现出惊人的能力。然而，对于需要解析和分析大量数值或表格数据的特定领域，即使是当前最先进的（SOTA）模型也面临挑战。在本文中，我们提出了一种新的方法来解决特定领域的表格数据分析任务，通过展示一个独特的RAG（检索-增强-生成）工作流程来缓解现有表格LLM（大型语言模型）解决方案的可扩展性问题。具体来说，我们提出了表格嵌入模型（TEM），这是一种针对表格检索-增强-生成（RAG）应用进行微调嵌入模型的新方法。嵌入模型是RAG工作流程中的关键组成部分，即使是当前的SOTA嵌入模型也难以胜任，因为它们主要在文本数据集上训练，因此在涉及复杂表格数据的场景中表现不佳。评估结果表明，我们的方法不仅在该领域优于当前的SOTA嵌入模型，而且采用了明显更小、更有效的模型结构。</p>
<h4 id="_14">一句话总结：</h4>
<p>本文提出了一种新的表格嵌入模型（TEM），通过独特的RAG工作流程，有效提升了特定领域表格数据分析任务的性能。</p>
<hr />
<h2 id="semi-supervised-text-based-person-search"><a href="http://arxiv.org/abs/2404.18106v1">Semi-supervised Text-based Person Search</a></h2>
<p>发布时间：2024-04-28</p>
<p>作者：Daming Gao, Yang Bai, Min Cao, Hao Dou, Mang Ye, Min Zhang</p>
<h4 id="_15">中文摘要：</h4>
<p>文本基于的人脸搜索（TBPS）旨在根据自然语言描述从大量图像库中检索特定人物的照片。现有方法依赖于大量标注的图像-文本数据，以在完全监督学习中获得令人满意的性能。在实践中，由于从监控视频中获取人物图像相对容易，而获取标注文本具有挑战性，因此这构成了一个重大的挑战。本文开展了一项开创性的研究，旨在探索半监督设置下的TBPS。在半监督设置中，只有少量的人物图像被标注了文本描述，而大多数图像缺乏标注。我们提出了一种基于生成-检索的两阶段基本解决方案，用于半监督TBPS。生成阶段通过应用图像字幕模型为未标注图像生成伪文本来丰富标注数据。随后，检索阶段使用增强数据执行完全监督的检索学习。值得注意的是，考虑到伪文本对检索学习的噪声干扰，我们提出了一种噪声鲁棒的检索框架，增强了检索模型处理噪声数据的能力。该框架集成了两个关键策略：混合补丁-通道掩码（PC-Mask）以细化模型架构，以及噪声引导的渐进训练（NP-Train）以增强训练过程。PC-Mask在补丁级和通道级对输入数据进行掩码处理，以防止噪声监督的过拟合。NP-Train引入了一种基于伪文本噪声水平的渐进训练计划，以促进噪声鲁棒学习。在多个TBPS基准上的大量实验表明，所提出的框架在半监督设置下实现了有希望的性能。</p>
<h4 id="_16">一句话总结：</h4>
<p>本文提出了一种基于生成-检索的半监督文本人脸搜索框架，通过噪声鲁棒的检索策略和渐进训练方法，在有限的标注数据下实现了令人满意的人脸检索性能。</p>
<hr />
<h2 id="generative-ai-for-low-carbon-artificial-intelligence-of-things-with-large-language-models"><a href="http://arxiv.org/abs/2404.18077v2">Generative AI for Low-Carbon Artificial Intelligence of Things with Large Language Models</a></h2>
<p>发布时间：2024-04-28</p>
<p>作者：Jinbo Wen, Ruichen Zhang, Dusit Niyato, Jiawen Kang, Hongyang Du, Yang Zhang, Zhu Han</p>
<h4 id="_17">中文摘要：</h4>
<p>通过将人工智能（AI）与物联网（IoT）相结合，人工智能物联网（AIoT）在许多领域实现了革命性的变革。然而，由于移动技术的持续进步，AIoT面临着能耗和碳排放的挑战。幸运的是，生成式人工智能（GAI）凭借其卓越的推理和生成能力，在减少AIoT碳排放方面具有巨大的潜力。在本文中，我们探讨了GAI在减少碳排放方面的潜力，并提出了一种新型的GAI赋能的低碳AIoT解决方案。具体而言，我们首先研究了导致AIoT碳排放的主要影响，然后介绍了GAI技术及其与碳排放的关系。接着，我们探讨了GAI在低碳AIoT中的应用前景，重点关注GAI如何减少网络组件的碳排放。随后，我们提出了一种基于大型语言模型（LLM）的碳排放优化框架，其中我们设计了可插拔的LLM和检索增强生成（RAG）模块，以生成更准确和可靠的优化问题。此外，我们利用生成扩散模型（GDMs）来识别碳减排的最佳策略。数值结果表明，所提出的框架是有效的。最后，我们提出了低碳AIoT的开放研究方向。</p>
<h4 id="_18">一句话总结：</h4>
<p>本文提出了一种基于生成式人工智能的低碳AIoT解决方案，通过优化网络组件的碳排放策略，有效降低AIoT的能耗和碳排放。</p>
<hr />
<h2 id="tool-calling-enhancing-medication-consultation-via-retrieval-augmented-large-language-models"><a href="http://arxiv.org/abs/2404.17897v1">Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented Large Language Models</a></h2>
<p>发布时间：2024-04-27</p>
<p>作者：Zhongzhen Huang, Kui Xue, Yongqi Fan, Linjie Mu, Ruoyu Liu, Tong Ruan, Shaoting Zhang, Xiaofan Zhang</p>
<h4 id="_19">中文摘要：</h4>
<p>大规模语言模型（LLMs）在各种语言任务中取得了显著的成功，但存在幻觉和时序错位的问题。为了缓解这些不足，检索增强生成（RAG）被用于提供外部知识以促进答案生成。然而，由于缺乏领域特定知识和现实场景的复杂性，将这些模型应用于医学领域面临几个挑战。在本研究中，我们探索了具有RAG框架的LLMs在医学领域的知识密集型任务中的应用。为了评估LLMs的能力，我们引入了MedicineQA，这是一个多轮对话基准，模拟了现实世界的药物咨询场景，并要求LLMs使用从药物数据库检索到的证据来回答问题。MedicineQA包含300个多轮问答对，每个问答对都嵌入在一个详细的对话历史中，突出了这一知识密集型任务对当前LLMs提出的挑战。我们进一步提出了一种新的\textit{Distill-Retrieve-Read}框架，而不是之前的\textit{Retrieve-then-Read}。具体来说，蒸馏和检索过程利用一个称为机制来制定搜索查询，模拟搜索引擎使用的基于关键词的查询。通过实验结果，我们表明我们的框架带来了显著的性能提升，在证据检索准确率方面超过了之前的对等框架。这一进步为将RAG应用于医学领域提供了启示。</p>
<h4 id="_20">一句话总结：</h4>
<p>本研究提出了一种新的Distill-Retrieve-Read框架，显著提升了大规模语言模型在医学领域知识密集型任务中的检索准确率。</p>
<hr />
<h2 id="retrieval-augmented-generation-with-knowledge-graphs-for-customer-service-question-answering"><a href="http://arxiv.org/abs/2404.17723v2">Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering</a></h2>
<p>发布时间：2024-04-26</p>
<p>作者：Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, Zheng Li</p>
<h4 id="_21">中文摘要：</h4>
<p>在客户服务技术支持中，快速准确地检索相关历史问题对于高效解决客户咨询至关重要。传统的检索增强生成（RAG）方法在处理大型语言模型（LLMs）中的大量历史问题跟踪票据时，将其视为普通文本，忽略了关键的问题内结构和问题间关系，这限制了性能。我们提出了一种新颖的客户服务问答方法，该方法将RAG与知识图谱（KG）相结合。我们的方法从历史问题中构建一个KG用于检索，保留了问题内结构和问题间关系。在问答阶段，我们的方法解析消费者查询并从KG中检索相关子图以生成答案。这种KG的集成不仅通过保留客户服务结构信息提高了检索准确性，而且通过减轻文本分割的影响提升了答案质量。在我们的基准数据集上的实证评估，利用关键检索（MRR、Recall@K、NDCG@K）和文本生成（BLEU、ROUGE、METEOR）指标，显示我们的方法在MRR上优于基线77.6%，在BLEU上优于基线0.32。我们的方法已在LinkedIn的客户服务团队中部署约六个月，并将每个问题的平均解决时间减少了28.6%。</p>
<h4 id="_22">一句话总结：</h4>
<p>该方法通过结合知识图谱与检索增强生成技术，显著提高了客户服务技术支持中问题检索和答案生成的准确性与效率。</p>
<hr />
<h2 id="exploring-the-distinctiveness-and-fidelity-of-the-descriptions-generated-by-large-vision-language-models"><a href="http://arxiv.org/abs/2404.17534v1">Exploring the Distinctiveness and Fidelity of the Descriptions Generated by Large Vision-Language Models</a></h2>
<p>发布时间：2024-04-26</p>
<p>作者：Yuhang Huang, Zihan Wu, Chongyang Gao, Jiawei Peng, Xu Yang</p>
<h4 id="_23">中文摘要：</h4>
<p>大型视觉语言模型（LVLMs）因其处理和整合视觉和文本数据的卓越能力而受到关注。尽管它们很受欢迎，但LVLMs生成精确、细粒度文本描述的能力尚未得到充分探索。本研究通过关注（distinctiveness）和（fidelity），评估了Open-Flamingo、IDEFICS和MiniGPT-4等模型如何区分相似对象并准确描述视觉特征。我们提出了文本检索增强分类（TRAC）框架，通过利用其生成能力，使我们能够更深入地分析细粒度视觉描述生成。这项研究为LVLMs的生成质量提供了宝贵的见解，增强了人们对多模态语言模型的理解。值得注意的是，MiniGPT-4在生成细粒度描述方面表现出色，在这方面优于其他两个模型。代码可在\url{https://anonymous.4open.science/r/Explore_FGVDs-E277}找到。</p>
<h4 id="_24">一句话总结：</h4>
<p>本研究通过提出TRAC框架，深入分析了大型视觉语言模型在生成细粒度视觉描述方面的能力，并发现MiniGPT-4在生成细粒度描述方面表现最佳。</p>
<hr />
<h2 id="inspectorraget-an-introspection-platform-for-rag-evaluation"><a href="http://arxiv.org/abs/2404.17347v1">InspectorRAGet: An Introspection Platform for RAG Evaluation</a></h2>
<p>发布时间：2024-04-26</p>
<p>作者：Kshitij Fadnis, Siva Sankalp Patel, Odellia Boni, Yannis Katsis, Sara Rosenthal, Benjamin Sznajder, Marina Danilevsky</p>
<h4 id="_25">中文摘要：</h4>
<p>大型语言模型（LLM）已成为实现检索增强生成（RAG）系统的流行方法，大量精力被投入到构建优秀模型和指标上。尽管对RAG系统严格评估的需求得到了越来越多的认可，但现有的工具很少能超越模型输出创建和自动计算。我们提出了InspectorRAGet，这是一个用于RAG评估的反思平台。InspectorRAGet允许用户分析RAG系统的聚合和实例级性能，使用人类和算法指标以及标注者质量。InspectorRAGet适用于多种用例，并向社区公开提供。演示视频可在https://youtu.be/MJhe8QIXcEc查看。</p>
<h4 id="_26">一句话总结：</h4>
<p>InspectorRAGet是一个用于评估RAG系统性能的反思平台，它允许用户通过人类和算法指标来分析RAG系统的聚合和实例级性能。</p>
<hr />
<h2 id="human-imperceptible-retrieval-poisoning-attacks-in-llm-powered-applications"><a href="http://arxiv.org/abs/2404.17196v1">Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications</a></h2>
<p>发布时间：2024-04-26</p>
<p>作者：Quan Zhang, Binqi Zeng, Chijin Zhou, Gwihwan Go, Heyuan Shi, Yu Jiang</p>
<h4 id="_27">中文摘要：</h4>
<p>目前，借助先进的LLM应用开发框架，越来越多的LLM驱动的应用能够通过检索增强生成（RAG）技术轻松地利用外部内容来增强LLMs的知识。然而，这些框架的设计并未充分考虑外部内容的风险，从而使得攻击者能够破坏使用这些框架开发的应用。在本文中，我们揭示了一种针对LLM驱动的应用的新威胁，称为检索中毒（retrieval poisoning），攻击者可以通过引导应用在RAG过程中产生恶意响应。具体来说，通过分析LLM应用框架，攻击者可以制作出与良性文档在视觉上无法区分的文档。尽管这些文档提供了正确的信息，但一旦它们被用作RAG的参考源，应用就会被误导生成错误的响应。我们的初步实验表明，攻击者以88.33%的成功率误导LLM，并在现实世界应用中实现了66.67%的成功率，展示了检索中毒的潜在影响。</p>
<h4 id="_28">一句话总结：</h4>
<p>本文揭示了LLM应用中检索中毒的新威胁，攻击者通过制作视觉上难以区分的文档，在RAG过程中误导LLM生成恶意响应。</p>
<hr />
<h2 id="understanding-privacy-risks-of-embeddings-induced-by-large-language-models"><a href="http://arxiv.org/abs/2404.16587v1">Understanding Privacy Risks of Embeddings Induced by Large Language Models</a></h2>
<p>发布时间：2024-04-25</p>
<p>作者：Zhihao Zhu, Ninglu Shao, Defu Lian, Chenwang Wu, Zheng Liu, Yi Yang, Enhong Chen</p>
<h4 id="_29">中文摘要：</h4>
<p>大型语言模型（LLMs）显示出人工通用智能的早期迹象，但面临着幻觉问题。缓解这些幻觉的一个有希望的解决方案是将外部知识作为嵌入存储，帮助LLMs在检索增强生成中。然而，这种解决方案可能会损害隐私，因为最近的研究实验表明，原始文本可以通过预训练语言模型从文本嵌入中部分重建。LLMs相较于传统预训练模型的显著优势可能会加剧这些担忧。为此，我们研究了当使用LLMs时，从这些嵌入中重建原始知识和预测实体属性的有效性。实证发现表明，LLMs在两个评估任务上的准确性显著高于预训练模型，无论文本是在分布内还是分布外。这强调了LLMs对用户隐私构成威胁的潜在风险，突显了它们广泛使用的负面后果。我们进一步讨论了减轻这种风险的初步策略。</p>
<h4 id="_30">一句话总结：</h4>
<p>LLMs在提高生成任务准确性的同时，也增加了泄露用户隐私的风险。</p>
<hr />
<h2 id="domain-specific-improvement-on-psychotherapy-chatbot-using-assistant"><a href="http://arxiv.org/abs/2404.16160v1">Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant</a></h2>
<p>发布时间：2024-04-24</p>
<p>作者：Cheng Kang, Daniel Novak, Katerina Urbanova, Yuqing Cheng, Yong Hu</p>
<h4 id="_31">中文摘要：</h4>
<p>大型语言模型（LLMs）在特定任务上展示了令人印象深刻的泛化能力，尤其是在人类编写的指令数据支持下。然而，这类指令数据的数量有限、多样性不足以及专业知识的局限性，引发了关于当LLMs接收到特定领域指令时在心理治疗任务中表现如何的担忧。为了解决这个问题，我们首先提出了基于亚历山大街心理治疗的特定领域助手指令，其次，我们采用了一种适应性的微调方法和检索增强生成方法来改进预训练的LLMs。通过使用自动和人工评估对语言质量进行量化评估，我们发现针对心理治疗助手指令的预训练LLMs在响应基线方面优于最先进的LLMs。我们的助手指令方法提供了一种半标注方法，以使预训练的LLMs与指令对齐，并为预训练的LLMs提供更多的心理治疗知识。</p>
<h4 id="_32">一句话总结：</h4>
<p>本研究提出了一种基于特定领域助手指令的LLMs改进方法，通过半标注和检索增强生成技术，显著提升了LLMs在心理治疗任务中的表现。</p>
<hr />
<h2 id="from-local-to-global-a-graph-rag-approach-to-query-focused-summarization"><a href="http://arxiv.org/abs/2404.16130v1">From Local to Global: A Graph RAG Approach to Query-Focused Summarization</a></h2>
<p>发布时间：2024-04-24</p>
<p>作者：Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson</p>
<h4 id="_33">中文摘要：</h4>
<p>本文提出了一种名为Graph RAG的问答方法，用于在私有文本语料库上进行问答。该方法结合了检索增强生成（RAG）和查询聚焦摘要（QFS）的优势，能够根据用户问题的普遍性和待索引源文本的数量进行扩展。Graph RAG利用大型语言模型（LLM）分两步构建基于图的文本索引：首先从源文档中提取实体知识图谱，然后为所有紧密相关的实体群体预先生成社区摘要。给定一个问题，每个社区摘要用于生成部分响应，最后将所有部分响应汇总为对用户的最终响应。对于1百万个标记范围内的数据集的全球意义构建问题，研究表明Graph RAG在生成答案的全面性和多样性方面，相较于简单的RAG基线有显著提升。Graph RAG的全球和本地实现将开源，基于Python，并可在https://aka.ms/graphrag上获取。</p>
<h4 id="_34">一句话总结：</h4>
<p>Graph RAG通过构建基于图的文本索引，实现了对私有文本语料库中全局意义构建问题的有效问答。</p>
<hr />
<h2 id="studying-large-language-model-behaviors-under-realistic-knowledge-conflicts"><a href="http://arxiv.org/abs/2404.16032v1">Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts</a></h2>
<p>发布时间：2024-04-24</p>
<p>作者：Evgenii Kortukov, Alexander Rubinstein, Elisa Nguyen, Seong Joon Oh</p>
<h4 id="_35">中文摘要：</h4>
<p>检索增强生成（RAG）缓解了许多完全参数化语言模型的问题，例如时间退化、幻觉和缺乏扎根。在RAG中，模型的知识可以从上下文中提供的文档中更新。这导致模型参数化知识与上下文信息之间出现冲突的情况，模型可能不会总是更新其知识。先前的研究通过创建与模型正确参数化答案相矛盾的合成文档来研究知识冲突。我们提出了一种在现实设置中研究知识冲突的框架。我们使用真实的冲突文档更新错误的参数化知识，这反映了知识冲突在实际中是如何出现的。在这个现实场景中，我们发现知识更新失败的情况比之前报道的要少。在模型仍然未能更新其答案的情况下，我们发现存在参数化偏差：上下文中出现的错误参数化答案使得知识更新更可能失败。这些结果表明，LLMs的事实参数化知识可能会对其阅读能力和行为产生负面影响。我们的代码可在https://github.com/kortukov/realistic_knowledge_conflicts/找到。</p>
<h4 id="_36">一句话总结：</h4>
<p>本研究通过引入真实冲突文档，揭示了LLMs在知识更新过程中可能出现的参数化偏差，并指出其可能对阅读能力和行为产生负面影响。</p>
<hr />
<h2 id="telco-rag-navigating-the-challenges-of-retrieval-augmented-language-models-for-telecommunications"><a href="http://arxiv.org/abs/2404.15939v2">Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications</a></h2>
<p>发布时间：2024-04-24</p>
<p>作者：Andrei-Laurentiu Bornea, Fadhel Ayed, Antonio De Domenico, Nicola Piovesan, Ali Maatouk</p>
<h4 id="_37">中文摘要：</h4>
<p>本文探讨了在电信领域应用大型语言模型（LLMs）和检索增强生成（RAG）系统所面临的独特挑战，这些挑战主要源于电信标准文档的复杂性和该领域的快速演变。文章介绍了Telco-RAG，这是一个开源的RAG框架，旨在满足电信标准（尤其是第三代合作伙伴计划（3GPP）文档）的特定需求。Telco-RAG解决了在高度技术性内容上实施RAG管道的关键挑战，为在电信领域应用LLMs铺平了道路，并为其他技术领域的RAG实施提供了指导。</p>
<h4 id="_38">一句话总结：</h4>
<p>本文提出了Telco-RAG，一个专门针对电信标准文档的RAG框架，以解决在电信领域应用LLMs和RAG系统时遇到的挑战。</p>
<hr />
<h2 id="wiki-llava-hierarchical-retrieval-augmented-generation-for-multimodal-llms"><a href="http://arxiv.org/abs/2404.15406v2">Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs</a></h2>
<p>发布时间：2024-04-23</p>
<p>作者：Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</p>
<h4 id="_39">中文摘要：</h4>
<p>多模态大型语言模型（Multimodal LLMs）是大型语言模型（LLMs）的自然演进，它们扩展了模型的能力，使其能够超越纯文本模态进行工作。随着研究人员正在设计新的架构和视觉-语言适配器，本文我们专注于赋予此类模型回答需要外部知识的问题的能力。我们的方法，称为Wiki-LLaVA，旨在整合一个多模态文档的外部知识源，该知识源通过一个分层检索管道进行访问。使用这种方法，相关段落从外部知识源检索出来，并作为LLM的额外上下文使用，从而增强了生成对话的有效性和精确度。我们在针对具有外部数据的视觉问答数据集上进行了广泛的实验，并证明了我们方法的有效性。</p>
<h4 id="_40">一句话总结：</h4>
<p>本文提出了一种名为Wiki-LLaVA的方法，通过整合外部知识源来增强多模态大型语言模型在视觉问答任务中的性能和精确度。</p>
<hr />
<h2 id="meddr-diagnosis-guided-bootstrapping-for-large-scale-medical-vision-language-learning"><a href="http://arxiv.org/abs/2404.15127v1">MedDr: Diagnosis-Guided Bootstrapping for Large-Scale Medical Vision-Language Learning</a></h2>
<p>发布时间：2024-04-23</p>
<p>作者：Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, Hao Chen</p>
<h4 id="_41">中文摘要：</h4>
<p>大规模视觉-语言模型的快速发展在各种任务中展示了惊人的能力。然而，医学领域缺乏广泛且高质量的图像-文本数据，这极大地阻碍了大规模医学视觉-语言模型的发展。在本研究中，我们提出了一种诊断引导的引导策略，该策略利用图像和标签信息来构建视觉-语言数据集。基于构建的数据集，我们开发了MedDr，这是一个适用于医疗保健领域的基础模型，能够处理包括放射学、病理学、皮肤病学、视网膜检查和内窥镜在内的多种医学数据模式。此外，在推理过程中，我们提出了一种简单但有效的检索增强医疗诊断策略，该策略增强了模型的一般化能力。在视觉问答、医疗报告生成和医学图像诊断方面的广泛实验证明了我们方法的优势。</p>
<h4 id="_42">一句话总结：</h4>
<p>本研究提出了一种基于诊断引导的引导策略，构建了适用于医疗保健领域的MedDr基础模型，并通过检索增强策略提升了模型在医学诊断任务中的泛化能力。</p>
<hr />
<h2 id="from-matching-to-generation-a-survey-on-generative-information-retrieval"><a href="http://arxiv.org/abs/2404.14851v3">From Matching to Generation: A Survey on Generative Information Retrieval</a></h2>
<p>发布时间：2024-04-23</p>
<p>作者：Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, Zhicheng Dou</p>
<h4 id="_43">中文摘要：</h4>
<p>信息检索（IR）系统是用户获取信息的关键工具，广泛应用于搜索引擎、问答系统和推荐系统等场景。传统的基于相似度匹配返回文档排名列表的IR方法一直是可靠的信息获取手段，多年来主导着IR领域。随着预训练语言模型的发展，生成式信息检索（GenIR）作为一种新的范式出现，近年来受到越来越多的关注。目前，GenIR的研究可以分为两个方面：生成式文档检索（GR）和可靠响应生成。GR利用生成模型的参数来记忆文档，通过直接生成相关文档标识符来实现检索，而不需要进行显式的索引。另一方面，可靠响应生成则使用语言模型直接生成用户所需的信息，突破了传统IR在文档粒度和相关性匹配方面的限制，提供了更多的灵活性、效率和创造力，从而更好地满足实际需求。本文旨在系统地回顾GenIR的最新研究进展。我们将总结GR在模型训练、文档标识符、增量学习、下游任务适应、多模态GR和生成式推荐等方面的进展，以及可靠响应生成在内部知识记忆、外部知识增强、带有引用和个人信息助手生成响应等方面的进展。我们还回顾了GenIR系统的评估、挑战和未来前景。本综述旨在为GenIR领域的学者提供一个全面的参考资料，鼓励该领域进一步发展。</p>
<h4 id="_44">一句话总结：</h4>
<p>本文全面回顾了生成式信息检索（GenIR）的最新研究进展，包括生成式文档检索和可靠响应生成，旨在为该领域的研究者提供参考并促进进一步发展。</p>
<hr />
<h2 id="a-survey-of-large-language-models-on-generative-graph-analytics-query-learning-and-applications"><a href="http://arxiv.org/abs/2404.14809v1">A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications</a></h2>
<p>发布时间：2024-04-23</p>
<p>作者：Wenbo Shang, Xin Huang</p>
<h4 id="_45">中文摘要：</h4>
<p>本文对现有关于图数据的语言模型（LLMs）研究进行了全面调查，总结了高级LLM模型解决的相应图分析任务，并指出了现有挑战和未来方向。具体而言，我们研究了基于LLM的生成图分析（LLM-GGA）的关键问题，分为三类：基于LLM的图查询处理（LLM-GQP）、基于LLM的图推理与学习（LLM-GIL）以及图-LLM应用。LLM-GQP侧重于将图分析技术和LLM提示相结合，包括基于图理解和知识图谱（KG）的增强检索，而LLM-GIL则关注在图上进行学习和推理，包括图学习、图形成推理和图表示。我们总结了LLM中融入的不同图下游任务的有用提示。此外，我们还对LLM模型评估、基准数据集/任务进行了总结，并对LLM模型的优缺点进行了深入分析。我们还探讨了LLMs和图分析这一激动人心的跨学科研究领域的开放问题和未来方向。</p>
<h4 id="_46">一句话总结：</h4>
<p>本文综述了LLMs在图数据分析中的应用，探讨了其优势、挑战和未来研究方向。</p>
<hr />
<h2 id="simulating-task-oriented-dialogues-with-state-transition-graphs-and-large-language-models"><a href="http://arxiv.org/abs/2404.14772v1">Simulating Task-Oriented Dialogues with State Transition Graphs and Large Language Models</a></h2>
<p>发布时间：2024-04-23</p>
<p>作者：Chris Samarinas, Pracha Promthaw, Atharva Nijasure, Hansi Zeng, Julian Killingback, Hamed Zamani</p>
<h4 id="_47">中文摘要：</h4>
<p>本文探讨了SynTOD，这是一种新的合成数据生成方法，用于开发能够处理复杂任务（如意图分类、槽填充、对话式问答和检索增强型响应生成）的端到端任务导向对话（TOD）系统，而不依赖于众包或真实世界数据。SynTOD利用状态转换图来定义TOD系统的期望行为，并通过随机游走和大型语言模型（LLMs）的响应模拟生成多样化的结构化对话。在我们的实验中，使用图引导的响应模拟与简单的单提示模拟对话相比，在意图分类、槽填充和响应相关性方面取得了显著的改进。我们还研究了不同基础和指令调整的LLMs在构建的合成对话中的端到端TOD效果。最后，我们探讨了各种LLMs如何评估TOD系统中的响应，以及它们与人类判断的相关性。我们的发现为快速开发和评估特定领域的TOD系统铺平了道路。我们发布了我们的数据集、模型和代码供研究之用。</p>
<h4 id="_48">一句话总结：</h4>
<p>本文提出了一种基于状态转换图和大型语言模型的合成数据生成方法，用于开发高效的任务导向对话系统。</p>
<hr />
<h2 id="llms-know-what-they-need-leveraging-a-missing-information-guided-framework-to-empower-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2404.14043v1">LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-04-22</p>
<p>作者：Keheng Wang, Feiyu Duan, Peiguang Li, Sirui Wang, Xunliang Cai</p>
<h4 id="_49">中文摘要：</h4>
<p>检索增强生成（RAG）通过为大型语言模型（LLMs）提供更新和相关的知识，在缓解过时知识或幻觉方面显示出巨大的价值。然而，RAG在理解复杂的多跳查询和检索相关文档方面仍存在一些困难，这需要LLMs进行推理和逐步检索。受人类推理过程的启发，其中他们逐步搜索所需信息，自然会问LLMs是否能够注意到每个推理步骤中缺失的信息。在这项工作中，我们首先通过实验验证了LLMs提取信息和识别缺失信息的能力。基于上述发现，我们提出了一个缺失信息引导的检索-提取-解决范式（MIGRES），其中我们利用缺失信息的识别来生成一个有针对性的查询，从而引导后续的知识检索。此外，我们设计了一种句子级重排序过滤方法，从文档中过滤掉无关内容，同时利用LLMs的信息提取能力从清理后的文档中提取有用信息，从而增强RAG的整体效果。在多个公共数据集上进行的广泛实验揭示了所提出的MIGRES方法的优越性，而分析实验证明了我们提出的模块的有效性。</p>
<h4 id="_50">一句话总结：</h4>
<p>该研究提出了一种基于缺失信息引导的检索-提取-解决范式（MIGRES），有效提升了检索增强生成（RAG）在处理复杂查询和知识检索中的性能。</p>
<hr />
<h2 id="tree-of-reviews-a-tree-based-dynamic-iterative-retrieval-framework-for-multi-hop-question-answering"><a href="http://arxiv.org/abs/2404.14464v1">Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering</a></h2>
<p>发布时间：2024-04-22</p>
<p>作者：Li Jiapeng, Liu Runze, Li Yabo, Zhou Tong, Li Mingling, Chen Xiang</p>
<h4 id="_51">中文摘要：</h4>
<p>多跳问答是一个知识密集型的复杂问题。大型语言模型（LLMs）利用其思维链（CoT）能力逐步推理复杂问题，而检索增强可以有效缓解LLMs中由于过时和未知知识引起的事实错误。最近的研究将检索增强引入到CoT推理中，以解决多跳问答问题。然而，这些链式方法存在以下问题：1）检索到的无关段落可能会误导推理；2）链结构中的错误可能导致错误连锁反应。在本文中，我们提出了一种名为“评论树”（ToR）的动态检索框架，其中根节点是问题，其他节点是从检索中获取的段落，从根节点扩展到其他节点的不同推理路径。我们的框架根据推理路径上的段落动态决定是否启动新的搜索、拒绝或接受。与相关工作相比，我们引入了树结构来单独处理每个检索到的段落，减轻了无关段落对推理路径的误导作用；推理路径扩展的多样性减少了单个推理错误对整体的影响。我们在三个不同的多跳问答数据集上进行了实验。结果表明，与基线方法相比，ToR在检索和响应生成方面都达到了最先进的性能。此外，我们提出了两种基于树的搜索优化策略，即剪枝和有效扩展，以减少时间开销并增加路径扩展的多样性。我们将发布我们的代码。</p>
<h4 id="_52">一句话总结：</h4>
<p>本文提出了一种名为“评论树”（ToR）的动态检索框架，通过树结构优化检索和推理路径，显著提升了多跳问答的性能。</p>
<hr />
<h2 id="typos-that-broke-the-rags-back-genetic-attack-on-rag-pipeline-by-simulating-documents-in-the-wild-via-low-level-perturbations"><a href="http://arxiv.org/abs/2404.13948v1">Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations</a></h2>
<p>发布时间：2024-04-22</p>
<p>作者：Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, Taeho Hwang, Jong C. Park</p>
<h4 id="_53">中文摘要：</h4>
<p>近年来，随着大型语言模型（LLMs）在各个领域和实际应用中的适用性不断扩大，其鲁棒性变得越来越重要。检索增强生成（RAG）是解决LLMs局限性的有前景的解决方案，但现有关于RAG鲁棒性的研究往往忽视了RAG组件之间的相互关系或现实世界中普遍存在的潜在威胁，如轻微的文本错误。在本工作中，我们研究了评估RAG鲁棒性时两个未被充分探索的方面：1）对低级扰动的噪声文档的脆弱性；2）对RAG鲁棒性的全面评估。此外，我们引入了一种新的攻击方法，即针对这些方面的遗传攻击RAG（GARAG）。具体来说，GARAG旨在揭示每个组件中的漏洞并测试整体系统功能对噪声文档的抵抗力。我们通过将我们的GARAG应用于标准问答数据集，结合了多种检索器和LLMs来验证RAG的鲁棒性。实验结果表明，GARAG始终能够实现高攻击成功率。此外，它显著破坏了每个组件及其协同作用，突显了轻微文本不准确在现实世界中破坏RAG系统所具有的实质性风险。</p>
<h4 id="_54">一句话总结：</h4>
<p>本研究提出了一种新的攻击方法GARAG，用于评估RAG的鲁棒性，揭示了RAG系统在现实世界中面临的风险。</p>
<hr />
<h2 id="self-bootstrapped-visual-language-model-for-knowledge-selection-and-question-answering"><a href="http://arxiv.org/abs/2404.13947v2">Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering</a></h2>
<p>发布时间：2024-04-22</p>
<p>作者：Dongze Hao, Qunbo Wang, Longteng Guo, Jie Jiang, Jing Liu</p>
<h4 id="_55">中文摘要：</h4>
<p>尽管大型预训练视觉语言模型在传统的视觉问答基准测试中表现出令人鼓舞的结果，但它们在回答需要多样世界知识的复杂视觉问答问题时仍然面临挑战。受自然语言处理领域中检索增强生成研究启发，我们使用密集篇章检索（Dense Passage Retrieval，DPR）来检索相关知识以帮助模型回答问题。然而，DPR在自然语言空间中进行检索，这可能无法确保全面获取图像信息。因此，检索到的知识并不能真正有助于回答问题，从而影响了整个系统的性能。为了解决这个问题，我们提出了一种新颖的框架，该框架利用视觉语言模型来选择DPR检索到的关键知识并回答问题。该框架由两个模块组成：选择器（Selector）和回答者（Answerer），两者都由多语言语言模型（MLLM）初始化，并通过自引导进行参数高效微调：使用选择器在检索到的知识文档中找到关键知识，然后使用这些知识来微调回答者以预测答案；根据回答者的预测和弱监督标签获得关键知识文档的伪标签，然后微调选择器以选择关键知识；重复此过程。我们的框架显著提高了在具有挑战性的开放域基于知识的视觉问答基准测试OK-VQA上基线模型的性能，实现了62.83%的最优准确率。</p>
<h4 id="_56">一句话总结：</h4>
<p>该研究提出了一种基于视觉语言模型的框架，通过密集篇章检索和自引导微调，显著提升了视觉问答系统的性能。</p>
<hr />
<h2 id="retrieval-augmented-audio-deepfake-detection"><a href="http://arxiv.org/abs/2404.13892v2">Retrieval-Augmented Audio Deepfake Detection</a></h2>
<p>发布时间：2024-04-22</p>
<p>作者：Zuheng Kang, Yayun He, Botao Zhao, Xiaoyang Qu, Junqing Peng, Jing Xiao, Jianzong Wang</p>
<h4 id="_57">中文摘要：</h4>
<p>随着语音合成技术，包括文本到语音（TTS）和声音转换（VC）系统的进步，能够生成超逼真的音频深度伪造，人们对它们的潜在滥用越来越担忧。然而，大多数深度伪造（DF）检测方法仅依赖于单个模型学习的模糊知识，导致性能瓶颈和透明度问题。受检索增强生成（RAG）的启发，我们提出了一种检索增强检测（RAD）框架，通过为测试样本添加相似检索样本来增强检测效果。我们还扩展了多融合注意力分类器，将其与我们的RAD框架集成。大量实验表明，所提出的RAD框架在基线方法之上表现出优越的性能，在ASVspoof 2021 DF数据集上实现了最先进的结果，在2019年和2021年LA数据集上取得了具有竞争力的结果。进一步的样本分析表明，检索器始终检索出与查询音频声学特征高度一致的同一说话者的样本，从而提高了检测性能。</p>
<h4 id="_58">一句话总结：</h4>
<p>本文提出了一种基于检索增强的深度伪造检测框架，显著提高了检测性能，并在多个数据集上取得了优异的结果。</p>
<hr />
<h2 id="evaluating-retrieval-quality-in-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2404.13781v1">Evaluating Retrieval Quality in Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-04-21</p>
<p>作者：Alireza Salemi, Hamed Zamani</p>
<h4 id="_59">中文摘要：</h4>
<p>评估检索增强生成（RAG）系统面临挑战，尤其是其中的检索模型。传统的端到端评估方法计算成本高昂。此外，基于查询-文档相关性标签对检索模型性能的评估与RAG系统的下游性能相关性较小。我们提出了一种新的评估方法eRAG，其中RAG系统中的大型语言模型单独利用检索列表中的每份文档。然后，根据下游任务的真值标签评估每份文档生成的输出。这样，每份文档的下游性能作为其相关性标签。我们采用各种下游任务指标来获取文档级标注，并使用集合或排名指标进行汇总。在广泛的数据集上的大量实验表明，与基线方法相比，eRAG与下游RAG性能的相关性更高，Kendall's $\tau$ 相关性从0.168提高到0.494。此外，eRAG提供了显著的计算优势，提高了运行时间，并且比端到端评估消耗的GPU内存少高达50倍。</p>
<h4 id="_60">一句话总结：</h4>
<p>eRAG通过利用大型语言模型对检索列表中的每份文档进行单独评估，实现了对检索增强生成系统性能的更有效评估，同时大幅降低了计算成本。</p>
<hr />
<h2 id="retrieval-augmented-generation-based-relation-extraction"><a href="http://arxiv.org/abs/2404.13397v1">Retrieval-Augmented Generation-based Relation Extraction</a></h2>
<p>发布时间：2024-04-20</p>
<p>作者：Sefika Efeoglu, Adrian Paschke</p>
<h4 id="_61">中文摘要：</h4>
<p>信息提取（IE）是一种转换过程，通过使用实体和关系提取（RE）方法将非结构化文本数据转换为结构化格式。在这个框架中，识别一对实体之间的关系起着至关重要的作用。尽管存在各种关系提取技术，但它们的效力很大程度上依赖于对标记数据的访问和大量的计算资源。为了解决这些挑战，大型语言模型（LLMs）成为了一种有希望的解决方案；然而，由于它们自己的训练数据，它们可能会返回幻觉响应。为了克服这些限制，本研究提出了基于检索增强生成的关系提取（RAG4RE），为提高关系提取任务性能提供了一条途径。本研究评估了我们RAG4RE方法的有效性，使用了不同的LLMs。通过利用如TACRED、TACREV、Re-TACRED和SemEval RE数据集等既定基准，我们的目标是全面评估我们RAG4RE方法的有效性。特别是，我们在研究中利用了Flan T5、Llama2和Mistral等突出的LLMs。我们的研究结果证明，我们的RAG4RE方法在TACRED数据集及其变体中超越了仅基于LLMs的传统关系提取方法的性能。此外，与TACRED和TACREV数据集上的先前关系提取方法相比，我们的方法表现出显著的性能，突显了其有效性和在自然语言处理中推进关系提取任务的前景。</p>
<h4 id="_62">一句话总结：</h4>
<p>本研究提出的基于检索增强生成的关系提取方法（RAG4RE）在自然语言处理中的关系提取任务上超越了传统方法，并显示出显著性能提升。</p>
<hr />
<h2 id="unlocking-multi-view-insights-in-knowledge-dense-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2404.12879v1">Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-04-19</p>
<p>作者：Guanhua Chen, Wenhan Yu, Lei Sha</p>
<h4 id="_63">中文摘要：</h4>
<p>尽管检索增强生成（RAG）在大型语言模型（LLMs）的应用中扮演着关键角色，但现有在法律和医学等知识密集型领域中的检索方法仍然缺乏多角度的视角，而这些视角对于提高可解释性和可靠性至关重要。先前关于多视角检索的研究往往只关注查询的不同语义形式，而忽略了特定领域知识视角的表达。本文提出了一种新颖的多视角RAG框架，MVRAG，专门针对知识密集型领域，该框架利用从多个领域视角出发的意图感知查询重写，以增强检索精度，从而提高最终推理的有效性。在法律和医学案例检索上的实验表明，我们的框架在召回率和精确率上都有显著提升。我们的多视角检索方法释放了多视角信息增强RAG任务的潜力，加速了LLMs在知识密集型领域的进一步应用。</p>
<h4 id="_64">一句话总结：</h4>
<p>本文提出的多视角RAG框架MVRAG通过多角度查询重写，显著提升了知识密集型领域的检索精度，推动了LLMs在专业领域的应用。</p>
<hr />
<h2 id="dubo-sql-diverse-retrieval-augmented-generation-and-fine-tuning-for-text-to-sql"><a href="http://arxiv.org/abs/2404.12560v1">Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for Text-to-SQL</a></h2>
<p>发布时间：2024-04-19</p>
<p>作者：Dayton G. Thorpe, Andrew J. Duberstein, Ian A. Kinsey</p>
<h4 id="_65">中文摘要：</h4>
<p>目前自动文本到SQL（text-to-SQL）转换的最新技术水平（SOTA）在BIRD-SQL基准测试的执行准确度（EX）方面，仍然远远落后于专家级人类的表现。最准确的方法也相对较慢且成本高昂。为了在降低成本的同时提高速度，我们探索了低成本微调、新颖的多样化检索增强生成（RAG）方法和新的输入输出格式，这些方法有助于大型语言模型（LLMs）实现更高的执行准确度（EX）。我们引入了两种新的方法，Dubo-SQL v1和v2。Dubo-SQL v1在BIRD-SQL的保留测试集上设定了新的执行准确度记录。Dubo-SQL v2在BIRD-SQL的开发集上实现了更高的性能。Dubo-SQL v1依赖于OpenAI的LLMs，但使用低成本GPT-3.5 Turbo，其性能超过了使用更昂贵GPT-4的下一个最佳模型。Dubo-SQL v1使用GPT-3.5的性能超过了下一个最佳模型超过20%。Dubo-SQL v2使用GPT-4 Turbo和RAG代替微调，以进一步提高执行准确度。</p>
<h4 id="_66">一句话总结：</h4>
<p>本研究通过结合低成本微调、新颖的检索增强生成方法和新的输入输出格式，显著提高了自动文本到SQL转换的执行准确度。</p>
<hr />
<h2 id="generating-test-scenarios-from-nl-requirements-using-retrieval-augmented-llms-an-industrial-study"><a href="http://arxiv.org/abs/2404.12772v1">Generating Test Scenarios from NL Requirements using Retrieval-Augmented LLMs: An Industrial Study</a></h2>
<p>发布时间：2024-04-19</p>
<p>作者：Chetan Arora, Tomas Herda, Verena Homm</p>
<h4 id="_67">中文摘要：</h4>
<p>测试场景是特定测试用例的具体实例，用于描述验证特定软件功能的行为。通过概述软件运行的条件和预期结果，测试场景确保软件功能以集成的方式进行测试。测试场景对于在多种条件下，包括边缘情况，系统地测试应用程序至关重要，以识别潜在问题并保证整体性能和可靠性。指定测试场景是一项繁琐的工作，需要深入理解软件功能及其底层领域。这还要求来自已经时间和预算受限的需求工程师和测试团队的大量努力和投资。本文提出了一种自动化的测试场景生成方法（RAGTAG），该方法使用检索增强生成（RAG）和大型语言模型（LLMs）进行。RAG允许将特定领域知识与LLMs的生成能力相结合。我们在奥地利邮政的两个工业项目中评估了RAGTAG，这些项目具有德语和英语的双语需求。我们对四位专家进行的访谈调查结果从五个维度——相关性、覆盖率、正确性、连贯性和可行性——证实了RAGTAG在自动化测试场景生成方面的潜力。具体来说，我们的结果表明，尽管分析双语需求是一项艰巨的任务，但RAGTAG能够生成与底层需求高度一致的场景，并覆盖预期功能的不同方面。生成的场景对专家来说易于理解，并在项目环境中易于测试。整体正确性被认为是令人满意的；然而，在捕捉精确动作序列和领域细微差别方面仍存在差距，这强调了在应用LLMs时需要领域专业知识。</p>
<h4 id="_68">一句话总结：</h4>
<p>本文提出了一种基于RAG和LLMs的自动化测试场景生成方法RAGTAG，有效提高了测试场景生成的效率和准确性。</p>
<hr />
<h2 id="the-solution-for-the-cvpr2024-nice-image-captioning-challenge"><a href="http://arxiv.org/abs/2404.12739v2">The Solution for the CVPR2024 NICE Image Captioning Challenge</a></h2>
<p>发布时间：2024-04-19</p>
<p>作者：Longfei Huang, Shupeng Zhong, Xiangyu Wu, Ruoxuan Li</p>
<h4 id="_69">中文摘要：</h4>
<p>本报告介绍了针对2024年NICE（零样本图像描述新前沿）竞赛中“主题1：零样本图像描述”的解决方案。与NICE 2023数据集相比，此次挑战涉及人类标注的新内容，其中描述风格和内容存在显著差异。因此，我们通过检索增强和描述评分方法有效地提升了图像描述。在数据层面，我们利用图像描述模型生成的优质描述作为训练数据，以解决文本风格上的差距。在模型层面，我们采用基于手工模板的大规模视觉-语言预训练模型OFA（Open-Ended Framelet-based Visual Language Pre-training）来完成图像描述任务。随后，我们针对图像描述模型生成的优质描述数据提出了描述级策略，并将其与检索增强策略整合到模板中，迫使模型根据检索增强提示生成更高质量、更匹配且语义丰富的描述。我们的方法实现了CIDEr评分234.11。</p>
<h4 id="_70">一句话总结：</h4>
<p>本研究通过检索增强和描述评分方法，结合OFA模型，有效提升了零样本图像描述的质量，实现了CIDEr评分234.11。</p>
<hr />
<h2 id="ragcache-efficient-knowledge-caching-for-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2404.12457v2">RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-04-18</p>
<p>作者：Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, Xin Jin</p>
<h4 id="_71">中文摘要：</h4>
<p>检索增强生成（RAG）通过整合大型语言模型（LLMs）和外部知识数据库的优势，在多种自然语言处理任务中取得了显著的改进。然而，RAG引入了长序列生成，导致计算和内存成本高昂。我们提出了RAGCache，这是一种针对RAG的新型多级动态缓存系统。我们的分析对当前的RAG系统进行了基准测试，确定了性能瓶颈（即由于知识注入而导致的长序列）和优化机会（即缓存知识的中间状态）。基于这些洞察，我们设计了RAGCache，它将检索到的知识的中间状态组织在知识树中，并在GPU和主机内存层次结构中缓存它们。RAGCache提出了一种替换策略，该策略了解LLM推理特征和RAG检索模式。它还动态地重叠检索和推理步骤，以最小化端到端延迟。我们在vLLM（一种最先进的LLM推理系统）和Faiss（一种最先进的向量数据库）上实现了RAGCache，并对其进行了评估。实验结果表明，与Faiss集成的vLLM相比，RAGCache将首次标记时间（TTFT）减少了高达4倍，并将吞吐量提高了高达2.1倍。</p>
<h4 id="_72">一句话总结：</h4>
<p>RAGCache通过优化知识缓存和动态重叠检索推理步骤，显著提升了RAG系统的性能和效率。</p>
<hr />
<h2 id="ragar-your-falsehood-radar-rag-augmented-reasoning-for-political-fact-checking-using-multimodal-large-language-models"><a href="http://arxiv.org/abs/2404.12065v2">RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models</a></h2>
<p>发布时间：2024-04-18</p>
<p>作者：M. Abdul Khaliq, P. Chang, M. Ma, B. Pflugfelder, F. Miletić</p>
<h4 id="_73">中文摘要：</h4>
<p>随着虚假信息的日益严峻挑战，尤其是在政治话语中，迫切需要高级的事实核查解决方案；在多模态主张的更复杂场景中，这一点更为明显。我们通过结合多模态大型语言模型和检索增强生成（RAG）来解决这个问题，并引入了两种新颖的推理技术：链式RAG（CoRAG）和树形RAG（ToRAG）。它们通过提取文本和图像内容、检索外部信息以及根据先前证据推理后续要回答的问题来进行多模态主张的事实核查。我们实现了0.85的加权F1分数，比基线推理技术高出0.14个百分点。人工评估证实，我们生成的绝大多数事实核查解释都包含了来自黄金标准数据的所有信息。</p>
<h4 id="_74">一句话总结：</h4>
<p>本研究提出了一种结合多模态大型语言模型和RAG的先进事实核查方法，显著提高了事实核查的准确性。</p>
<hr />
<h2 id="irag-an-incremental-retrieval-augmented-generation-system-for-videos"><a href="http://arxiv.org/abs/2404.12309v1">iRAG: An Incremental Retrieval Augmented Generation System for Videos</a></h2>
<p>发布时间：2024-04-18</p>
<p>作者：Md Adnan Arefeen, Biplob Debnath, Md Yusuf Sarwar Uddin, Srimat Chakradhar</p>
<h4 id="_75">中文摘要：</h4>
<p>检索增强生成（RAG）系统结合了语言生成和信息检索的优势，为聊天机器人等许多现实世界应用提供动力。使用RAG来理解文本、图像和视频等多模态数据具有吸引力，但存在两个关键限制：一次性、预先将大量多模态数据捕获为文本描述需要很高的处理时间，并且丰富的多模态数据中并非所有信息都通常包含在文本描述中。由于用户查询事先未知，因此开发一个用于多模态到文本转换和交互式查询多模态数据系统具有挑战性。为了解决这些限制，我们提出了iRAG，它通过一种新颖的增量工作流程来增强RAG，从而能够交互式查询大量多模态数据。与传统的RAG不同，iRAG可以快速索引大量多模态数据存储库，在增量工作流程中，它使用索引来有选择地从多模态数据中选择部分内容提取更多细节，以检索与交互式用户查询相关的上下文。这种增量工作流程避免了长时间的多模态到文本转换，通过按需查询特定提取多模态数据中的细节来克服信息丢失问题，并确保对交互式用户查询的响应质量，这些查询通常事先未知。据我们所知，iRAG是第一个通过增量工作流程增强RAG以支持高效交互式查询大型、现实世界多模态数据的系统。在真实世界长视频上的实验结果表明，视频到文本的摄入速度提高了23倍到25倍，同时确保了交互式用户查询的响应质量与传统RAG相当，其中所有视频数据在查询之前都预先转换为文本。</p>
<h4 id="_76">一句话总结：</h4>
<p>iRAG通过增量工作流程增强RAG，实现了对大型多模态数据的快速交互式查询，同时保证了查询响应的质量。</p>
<hr />
<h2 id="aligning-actions-and-walking-to-llm-generated-textual-descriptions"><a href="http://arxiv.org/abs/2404.12192v1">Aligning Actions and Walking to LLM-Generated Textual Descriptions</a></h2>
<p>发布时间：2024-04-18</p>
<p>作者：Radu Chivereanu, Adrian Cosma, Andy Catruna, Razvan Rughinis, Emilian Radoi</p>
<h4 id="_77">中文摘要：</h4>
<p>大型语言模型（LLMs）在各个领域，包括数据增强和合成数据生成中，展现出了惊人的能力。本研究探讨了利用LLMs生成运动序列丰富文本描述的方法，这些描述涵盖了动作和行走模式。我们利用LLMs的表达能力，将运动表示与高级语言提示对齐，解决了两个不同的任务：动作识别和基于外观属性的行走序列检索。在动作识别方面，我们使用LLMs生成BABEL-60数据集中动作的文本描述，从而促进运动序列与语言表示的对齐。在步态分析领域，我们通过使用LLMs从DenseGait数据集中生成运动序列的文本描述，来研究外观属性对行走模式的影响。这些描述捕捉了受服装选择和鞋类等因素影响的行走风格的微妙变化。我们的方法展示了LLMs在增强结构化运动属性和对齐多模态表示方面的潜力。这些发现有助于推动全面运动理解的发展，并为在运动分析中利用LLMs进行多模态对齐和数据增强开辟了新的途径。我们将在https://github.com/Radu1999/WalkAndText上公开代码。</p>
<h4 id="_78">一句话总结：</h4>
<p>本研究利用大型语言模型生成运动序列的文本描述，以促进动作识别和步态分析中的多模态对齐和数据增强。</p>
<hr />
<h2 id="exploring-the-landscape-of-large-language-models-foundations-techniques-and-challenges"><a href="http://arxiv.org/abs/2404.11973v1">Exploring the landscape of large language models: Foundations, techniques, and challenges</a></h2>
<p>发布时间：2024-04-18</p>
<p>作者：Milad Moradi, Ke Yan, David Colwell, Matthias Samwald, Rhona Asgari</p>
<h4 id="_79">中文摘要：</h4>
<p>本文综述了大型语言模型（LLMs）的领域，涵盖了其基础原理、多样化应用和细微的训练过程。文章揭示了情境学习的机制和一系列微调方法，特别关注优化参数使用效率的方法。此外，它探讨了如何通过创新的强化学习框架和其他结合人类反馈的新方法，使LLMs更接近人类偏好。文章还考察了检索增强生成这一新兴技术，将外部知识整合到LLMs中。讨论了LLMs部署的伦理维度，强调了谨慎和负责任应用的重要性。最后，本文展望了未来的研究轨迹，为LLMs不断发展的领域中当前状态和新兴趋势提供了一个简洁而全面的概述，为人工智能领域的研究人员和从业者提供了有洞察力的指南。</p>
<h4 id="_80">一句话总结：</h4>
<p>本文全面概述了大型语言模型（LLMs）的现状和趋势，探讨了其原理、应用、训练方法以及伦理问题，为人工智能领域的研究和实践提供了指导。</p>
<hr />
<h2 id="from-language-models-to-practical-self-improving-computer-agents"><a href="http://arxiv.org/abs/2404.11964v1">From Language Models to Practical Self-Improving Computer Agents</a></h2>
<p>发布时间：2024-04-18</p>
<p>作者：Alex Sheng</p>
<h4 id="_81">中文摘要：</h4>
<p>我们开发了一种简单直接的方法来创建能够执行各种计算机任务并通过开发工具和增强功能来自我改进的AI计算机代理。由于大型语言模型（LLMs）已被证明可以从非参数增强中受益，最近的研究工作大量集中在开发增强LLMs各种能力的软件。我们提出，而不是通过人工工程手动开发静态软件来增强LLMs，一个LLM代理可以系统地生成软件来增强自身。通过几个案例研究，我们表明，一个最小查询循环和适当的提示工程允许LLM生成和使用各种增强，自由扩展其自身能力以执行现实世界的计算机任务。从仅终端访问开始，我们提示LLM代理通过检索、互联网搜索、网页导航和文本编辑能力来增强自身。该代理有效地使用这些各种工具来解决包括自动化软件开发和基于网页的任务在内的问题。</p>
<h4 id="_82">一句话总结：</h4>
<p>本研究提出了一种利用LLM自我生成和利用增强功能以执行复杂计算机任务的方法。</p>
<hr />
<h2 id="memllm-finetuning-llms-to-use-an-explicit-read-write-memory"><a href="http://arxiv.org/abs/2404.11672v1">MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory</a></h2>
<p>发布时间：2024-04-17</p>
<p>作者：Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schütze</p>
<h4 id="_83">中文摘要：</h4>
<p>尽管当前的大型语言模型（LLMs）在知识密集型任务中展现出一些能力，但它们依赖于参数作为隐式存储机制的限制性使得它们难以处理不常见知识和时间退化问题。此外，参数化记忆的不可解释性使得理解和预防幻觉变得具有挑战性。参数化内存池和模型编辑只是部分解决方案。检索增强生成（RAG）虽然是非参数化的，但它也有其局限性：它缺乏结构，复杂了可解释性，并使得有效管理存储知识变得困难。在本文中，我们引入了MemLLM，这是一种通过整合一个结构化和显式的读写内存模块来增强LLMs的新方法。MemLLM通过允许与内存的动态交互并提高LLMs使用存储知识的能力来解决上述挑战。我们的实验表明，MemLLM在语言建模的一般性和知识密集型任务的特殊性上都增强了LLMs的性能和可解释性。我们认为MemLLM是使LLMs通过记忆增强变得更加基于事实和真实的重要一步。</p>
<h4 id="_84">一句话总结：</h4>
<p>MemLLM通过引入结构化和显式的读写内存模块，增强了大型语言模型在知识密集型任务中的性能和可解释性。</p>
<hr />
<h2 id="position-engineering-boosting-large-language-models-through-positional-information-manipulation"><a href="http://arxiv.org/abs/2404.11216v1">Position Engineering: Boosting Large Language Models through Positional Information Manipulation</a></h2>
<p>发布时间：2024-04-17</p>
<p>作者：Zhiyuan He, Huiqiang Jiang, Zilong Wang, Yuqing Yang, Luna Qiu, Lili Qiu</p>
<h4 id="_85">中文摘要：</h4>
<p>大型语言模型（LLMs）的性能显著受到所提供提示质量的影响。为了应对这一问题，研究人员已经开发出大量的提示工程策略，旨在修改提示文本以提升任务性能。在本文中，我们介绍了一种名为位置工程的新技术，它提供了一种更高效的方式来引导大型语言模型。与需要大量努力修改提供给LLMs的文本的提示工程不同，位置工程仅涉及修改提示中的位置信息，而不修改文本本身。我们对位置工程在两个广泛使用的LLMs场景中进行了评估：检索增强生成（RAG）和情境学习（ICL）。我们的发现表明，位置工程在这两种情况下都显著优于基线。因此，位置工程代表了一种利用大型语言模型能力的有希望的全新策略。</p>
<h4 id="_86">一句话总结：</h4>
<p>位置工程是一种高效的新技术，通过修改提示中的位置信息来提升大型语言模型在检索增强生成和情境学习场景中的性能。</p>
<hr />
<h2 id="a-survey-on-retrieval-augmented-text-generation-for-large-language-models"><a href="http://arxiv.org/abs/2404.10981v1">A Survey on Retrieval-Augmented Text Generation for Large Language Models</a></h2>
<p>发布时间：2024-04-17</p>
<p>作者：Yizheng Huang, Jimmy Huang</p>
<h4 id="_87">中文摘要：</h4>
<p>检索增强生成（RAG）将检索方法与深度学习进展相结合，通过动态集成最新外部信息来解决大型语言模型（LLMs）的静态局限性。这种方法主要关注文本领域，为LLMs生成合理但错误的响应提供了一种成本效益高的解决方案，通过使用真实世界数据增强了其输出的准确性和可靠性。随着RAG的复杂性和包含影响其性能的多个概念的增多，本文将RAG范式分为四个类别：检索前、检索、检索后和生成，从检索的角度提供了一个详细的视角。它概述了RAG的演变，并通过分析重要研究讨论了该领域的进展。此外，本文介绍了RAG的评估方法，解决了面临的挑战，并提出了未来的研究方向。通过提供有组织的框架和分类，本研究旨在巩固现有的RAG研究，阐明其技术基础，并突出其扩大LLMs适应性和应用潜力的可能性。</p>
<h4 id="_88">一句话总结：</h4>
<p>本文对检索增强生成（RAG）范式进行了分类和总结，旨在提高大型语言模型的准确性和可靠性，并拓宽其应用范围。</p>
<hr />
<h2 id="sure-summarizing-retrievals-using-answer-candidates-for-open-domain-qa-of-llms"><a href="http://arxiv.org/abs/2404.13081v1">SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs</a></h2>
<p>发布时间：2024-04-17</p>
<p>作者：Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, Jinwoo Shin</p>
<h4 id="_89">中文摘要：</h4>
<p>大型语言模型（LLMs）在包括问答（QA）任务在内的各种自然语言处理任务中取得了显著进展。虽然通过检索相关段落来融入新信息是提高LLMs问答能力的一种有前景的方法，但现有方法通常需要额外的微调，这在最近的LLMs中变得不可行。通过提示增强检索到的段落具有解决这一局限性的潜力，但这一方向的研究尚处于有限探索。为此，我们设计了一个简单而有效的框架，基于总结检索（SuRe）来增强LLMs的开放域问答（ODQA）。SuRe帮助LLMs预测更准确的答案，这些答案得到了总结检索的支持，可以看作是从检索到的段落中提取的显式理由。具体来说，SuRe首先为每个多个答案候选者构建检索段落的摘要。然后，SuRe通过评估生成摘要的有效性和排名来确认候选集中最可能的答案。在多个ODQA基准上的实验结果表明，SuRe优于标准提示方法，在精确匹配（EM）方面提高了4.6%，在F1分数方面提高了4.0%。SuRe还可以与广泛的检索方法和LLMs集成。最后，SuRe生成的摘要显示出额外的优势，可以衡量检索段落的重要性，并作为模型和人类更偏好的理由。</p>
<h4 id="_90">一句话总结：</h4>
<p>SuRe通过总结检索段落为LLMs提供更准确的开放域问答答案，显著提升了问答系统的性能。</p>
<hr />
<h2 id="enhancing-qa-with-domain-specific-fine-tuning-and-iterative-reasoning-a-comparative-study"><a href="http://arxiv.org/abs/2404.11792v2">Enhancing Q&amp;A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study</a></h2>
<p>发布时间：2024-04-17</p>
<p>作者：Zooey Nguyen, Anthony Annunziata, Vinh Luong, Sang Dinh, Quynh Le, Anh Hai Ha, Chanh Le, Hong An Phan, Shruti Raghavan, Christopher Nguyen</p>
<h4 id="_91">中文摘要：</h4>
<p>本文研究了特定领域模型微调和推理机制对大型语言模型（LLMs）和检索增强生成（RAG）驱动的问答（Q&amp;A）系统性能的影响。利用FinanceBench SEC财务报告数据集，我们发现，对于RAG而言，将微调的嵌入模型与微调的LLM相结合比通用模型实现了更高的准确率，其中相对更大的收益归因于微调的嵌入模型。此外，在RAG之上应用推理迭代进一步提升了性能，使问答系统能够更接近人类专家水平。我们讨论了这些发现的含义，提出了一种结构化的技术设计空间，该空间捕捉了问答人工智能的主要技术组件，并提供了针对这些组件进行高影响力技术选择的建议。我们计划继续进行这项工作，为AI团队提供可操作的指南，并进一步研究RAG中特定领域增强的影响以及高级规划、推理等代理人工智能能力。</p>
<h4 id="_92">一句话总结：</h4>
<p>本文通过结合特定领域模型微调和推理机制，显著提升了基于大型语言模型和检索增强生成的问答系统的性能，使其更接近人类专家水平。</p>
<hr />
<h2 id="retrieval-augmented-embodied-agents"><a href="http://arxiv.org/abs/2404.11699v1">Retrieval-Augmented Embodied Agents</a></h2>
<p>发布时间：2024-04-17</p>
<p>作者：Yichen Zhu, Zhicai Ou, Xiaofeng Mou, Jian Tang</p>
<h4 id="_93">中文摘要：</h4>
<p>在复杂和不确定的环境中运行的具身智能体面临着巨大的挑战。虽然一些高级智能体能够熟练地处理复杂的操作任务，但它们的成功往往依赖于大量的训练数据来发展其能力。相比之下，人类通常依赖于回忆过去的经验和类似情况来解决新问题。为了在机器人学中模仿这种人类方法，我们引入了检索增强具身智能体（RAEA）。这个创新系统为机器人提供了一种共享记忆的形式，显著提高了它们的性能。我们的方法集成了一个策略检索器，允许机器人根据多模态输入从外部策略记忆库中访问相关策略。此外，还使用了一个策略生成器来将这些策略融入学习过程，使机器人能够制定有效的任务响应。在模拟和现实世界场景中对RAEA的广泛测试表明，其性能优于传统方法，代表了机器人技术的一个重大飞跃。</p>
<h4 id="_94">一句话总结：</h4>
<p>RAEA通过引入共享记忆和策略检索，显著提升了机器人在复杂环境中的任务执行能力。</p>
<hr />
<h2 id="minicheck-efficient-fact-checking-of-llms-on-grounding-documents"><a href="http://arxiv.org/abs/2404.10774v1">MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents</a></h2>
<p>发布时间：2024-04-16</p>
<p>作者：Liyan Tang, Philippe Laban, Greg Durrett</p>
<h4 id="_95">中文摘要：</h4>
<p>在自然语言处理（NLP）中，识别大型语言模型（LLM）输出是否基于证据是许多任务的关键，如检索增强生成、摘要、基于文档的对话等。目前针对此类“事实核查”的方法是基于使用LLM验证模型生成的每一部分与潜在证据的一致性。然而，这个过程可能非常计算密集，需要多次调用LLM来检查单个响应。在本研究中，我们展示了如何构建小型模型，这些模型具有GPT-4级别的性能，但成本仅为400倍以下。我们通过使用GPT-4构建合成训练数据来实现这一点，这涉及到通过结构化生成程序创建真实但具有挑战性的事实错误实例。在训练这些数据上，模型学习检查每个事实，并识别句子间信息合成。为了评估，我们将现有的数据集统一到一个基准LLM-AggreFact中，该基准来自最近关于事实核查和LLM生成基础的研究。我们最好的系统MiniCheck-FT5（770M参数）优于所有同等规模的系统，并达到了GPT-4的准确率。我们发布了LLM-AggreFact、数据合成的代码和模型。</p>
<h4 id="_96">一句话总结：</h4>
<p>本研究提出了一种高效的方法，通过构建小型模型并利用GPT-4生成合成训练数据，实现了对LLM输出的证据核查，同时显著降低了计算成本。</p>
<hr />
<h2 id="clasheval-quantifying-the-tug-of-war-between-an-llms-internal-prior-and-external-evidence"><a href="http://arxiv.org/abs/2404.10198v2">ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence</a></h2>
<p>发布时间：2024-04-16</p>
<p>作者：Kevin Wu, Eric Wu, James Zou</p>
<h4 id="_97">中文摘要：</h4>
<p>检索增强生成（RAG）常用于减轻幻觉并提供最新知识给大型语言模型（LLMs）。然而，鉴于文档检索是一个不精确的任务，有时会导致错误甚至有害的内容在上下文中呈现，这引发了关于LLMs如何处理检索信息的疑问：如果提供的内容是错误的，模型是否知道忽略它，或者它会重述错误？相反，当模型的初始响应是错误的，它是否总是知道使用检索信息来纠正自己，或者它坚持其错误的先前响应？为了回答这个问题，我们收集了一个包含超过1200个问题的数据集，这些问题涉及六个领域（例如，药物剂量、奥运记录、位置）以及与回答每个问题相关的相关内容。我们进一步对内容中的答案进行了精确的扰动，这些扰动从微小的错误到明显的错误都有。我们在该数据集上对六个表现最好的LLMs（包括GPT-4o）进行了基准测试，并发现LLMs容易采用错误的检索内容，超过60%的时间会覆盖它们自己的正确先前知识。然而，检索内容越不现实（即越偏离事实），模型采用它的可能性就越小。此外，模型对其初始响应（通过测量标记概率）越不自信，它采用检索内容中的信息的可能性就越大。我们利用这一发现，并展示了在存在冲突检索内容时提高模型准确性的简单方法。我们的结果突显了LLMs的一个困难任务和基准——即它们在正确检索内容的基础上正确判断自己何时错误，以及在提供的内容错误时拒绝这些情况的能力。</p>
<h4 id="_98">一句话总结：</h4>
<p>本研究发现，大型语言模型在处理检索信息时容易受到错误内容的影响，但它们对不切实际或偏离事实的检索内容更为敏感，且在初始响应不自信时更可能采用检索信息。</p>
<hr />
<h2 id="spiral-of-silence-how-is-large-language-model-killing-information-retrieval-a-case-study-on-open-domain-question-answering"><a href="http://arxiv.org/abs/2404.10496v4">Spiral of Silence: How is Large Language Model Killing Information Retrieval? -- A Case Study on Open Domain Question Answering</a></h2>
<p>发布时间：2024-04-16</p>
<p>作者：Xiaoyang Chen, Ben He, Hongyu Lin, Xianpei Han, Tianshu Wang, Boxi Cao, Le Sun, Yingfei Sun</p>
<h4 id="_99">中文摘要：</h4>
<p>本文研究了检索增强生成（RAG）实践，该实践将大型语言模型（LLMs）与检索系统相结合，并探讨了LLM生成的文本对RAG系统短期和长期影响。以流行的开放域问答（ODQA）任务为切入点，研究发现，LLM生成的文本在搜索排名中持续优于人类创作的内容，从而减少了人类贡献在网上的存在和影响，可能引发数字“沉默螺旋”效应。这种趋势可能导致信息生态系统的失衡，错误LLM生成内容的无序扩张可能会使准确信息边缘化。我们呼吁学术界关注这一潜在问题，确保数字信息景观的多样性和真实性。</p>
<h4 id="_100">一句话总结：</h4>
<p>本文揭示了LLM在RAG系统中的应用可能引发数字“沉默螺旋”效应，导致信息生态系统失衡，呼吁学术界关注并确保数字信息景观的多样性和真实性。</p>
<hr />
<h2 id="cross-data-knowledge-graph-construction-for-llm-enabled-educational-question-answering-system-acasestudyathcmut"><a href="http://arxiv.org/abs/2404.09296v1">Cross-Data Knowledge Graph Construction for LLM-enabled Educational Question-Answering System: A~Case~Study~at~HCMUT</a></h2>
<p>发布时间：2024-04-14</p>
<p>作者：Tuan Bui, Oanh Tran, Phuong Nguyen, Bao Ho, Long Nguyen, Thang Bui, Tho Quan</p>
<h4 id="_101">中文摘要：</h4>
<p>在当今快速发展的人工智能领域，大型语言模型（LLMs）已成为一个充满活力的研究课题。LLMs在各个领域都有应用，并做出了重大贡献。尽管它们拥有强大的语言能力，类似于预训练语言模型（PLMs），LLMs仍然面临记住事件、整合新信息和解决特定领域问题或幻觉的挑战。为了克服这些限制，研究人员提出了检索增强生成（RAG）技术，还有一些人提出了将LLMs与知识图谱（KGs）集成，以提供事实背景，从而提高性能并为用户查询提供更准确的反馈。教育在人类发展和进步中起着至关重要的作用。随着技术变革，传统教育正被数字或混合教育所取代。因此，数字环境中的教育数据每天都在增加。高等教育机构中的数据种类繁多，包括非结构化/结构化文本、关系数据库、基于网页/应用程序的API访问等多种来源。从这些跨数据源构建知识图谱并非易事。本文提出了一种从多个数据源自动构建知识图谱的方法，并讨论了知识图谱（KG）与LLMs结合在问答任务中的初步应用（实验试验）。</p>
<h4 id="_102">一句话总结：</h4>
<p>本文提出了一种自动构建知识图谱的方法，并将其应用于LLMs的问答任务中，以解决教育数据中的复杂性问题。</p>
<hr />
<h2 id="generative-ai-agents-with-large-language-model-for-satellite-networks-via-a-mixture-of-experts-transmission"><a href="http://arxiv.org/abs/2404.09134v2">Generative AI Agents with Large Language Model for Satellite Networks via a Mixture of Experts Transmission</a></h2>
<p>发布时间：2024-04-14</p>
<p>作者：Ruichen Zhang, Hongyang Du, Yinqiu Liu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Dong In Kim</p>
<h4 id="_103">中文摘要：</h4>
<p>为了应对6G全球通信的需求，卫星通信网络已成为关键解决方案。然而，卫星通信网络的大规模发展受到复杂系统模型的限制，这些模型的建模对大量用户来说具有挑战性。此外，卫星与用户之间的传输干扰严重影响了通信性能。为了解决这些问题，本文开发了用于模型制定的生成式人工智能（AI）代理，然后应用混合专家（MoE）方法设计传输策略。具体来说，我们利用大型语言模型（LLMs）构建一个交互式建模范式，并利用检索增强生成（RAG）提取支持数学建模的卫星专家知识。随后，通过整合多个专业组件的专长，我们提出了一种MoE近端策略优化（PPO）方法来解决制定的问题。每个专家可以通过其自身的网络进行专门训练，以优化其在其中表现优异的优化变量，然后通过门控网络将它们聚合起来以进行联合优化。仿真结果验证了使用生成代理进行问题制定的正确性和有效性。此外，所提出的MoE-PPO方法在解决制定问题方面优于其他基准，其适应各种定制建模问题的能力也得到了证明。</p>
<h4 id="_104">一句话总结：</h4>
<p>本文提出了一种基于MoE-PPO的卫星通信网络传输策略设计方法，通过生成式AI代理和混合专家技术，有效解决了大规模卫星通信网络中的建模和优化问题。</p>
<hr />
<h2 id="introducing-super-rags-in-mistral-8x7b-v1"><a href="http://arxiv.org/abs/2404.08940v1">Introducing Super RAGs in Mistral 8x7B-v1</a></h2>
<p>发布时间：2024-04-13</p>
<p>作者：Ayush Thakur, Raghav Gupta</p>
<h4 id="_105">中文摘要：</h4>
<p>本文介绍了一种名为Super Retrieval-Augmented Generation（Super RAGs）的新型方法，旨在通过将外部知识源与最小结构修改相结合，提升大型语言模型（LLMs）的性能。研究将Super RAGs集成到最先进的LLM Mistral 8x7B v1中，并分析了其在准确性、速度和用户满意度方面的改进。研究采用微调指令模型设置和缓存调整系统，确保高效且相关的数据检索。通过多个epoch的评估，所有指标均显示出显著提升。结果表明，Super RAGs可以有效增强LLMs，为更复杂和可靠的AI系统铺平道路。本研究通过提供Super RAGs益处的实证证据，并对其潜在应用提供见解，为该领域做出了贡献。</p>
<h4 id="_106">一句话总结：</h4>
<p>本研究通过将Super RAGs集成到先进的LLM中，显著提升了模型性能，为更高级和可靠的AI系统的发展提供了新的途径。</p>
<hr />
<h2 id="generative-ai-agent-for-next-generation-mimo-design-fundamentals-challenges-and-vision"><a href="http://arxiv.org/abs/2404.08878v1">Generative AI Agent for Next-Generation MIMO Design: Fundamentals, Challenges, and Vision</a></h2>
<p>发布时间：2024-04-13</p>
<p>作者：Zhe Wang, Jiayi Zhang, Hongyang Du, Ruichen Zhang, Dusit Niyato, Bo Ai, Khaled B. Letaief</p>
<h4 id="_107">中文摘要：</h4>
<p>下一代多输入多输出（MIMO）系统预计将具有智能和可扩展性。本文研究了由生成式人工智能（AI）代理驱动的下一代MIMO设计。首先，我们概述了下一代MIMO的发展、基本原理和挑战。接着，我们提出了生成式AI代理的概念，该代理能够在大型语言模型（LLM）和检索增强生成（RAG）的帮助下生成定制和专业的内容。然后，我们全面讨论了生成式AI代理框架的特征和优势。更重要的是，为了解决下一代MIMO的现有挑战，我们从性能分析、信号处理和资源分配的角度讨论了生成式AI代理驱动的下一代MIMO设计。此外，我们提出了两个引人入胜的案例研究，展示了在复杂配置场景中利用生成式AI代理进行性能分析的有效性。这些例子突出了生成式AI代理的集成如何显著提升下一代MIMO系统的分析和设计。最后，我们讨论了重要的潜在研究方向。</p>
<h4 id="_108">一句话总结：</h4>
<p>本文探讨了利用生成式人工智能代理来提升下一代MIMO系统的性能分析和设计。</p>
<hr />
<h2 id="reducing-hallucination-in-structured-outputs-via-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2404.08189v1">Reducing hallucination in structured outputs via Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-04-12</p>
<p>作者：Patrice Béchard, Orlando Marquez Ayala</p>
<h4 id="_109">中文摘要：</h4>
<p>生成式人工智能（GenAI）的一个常见且根本的局限性是其倾向于产生幻觉。尽管大型语言模型（LLM）已经席卷全球，但如果不能消除或至少减少幻觉，现实世界的GenAI系统在用户接受度方面可能会面临挑战。在部署一个基于自然语言需求生成工作流程的企业应用过程中，我们设计了一个系统，该系统利用检索增强生成（RAG）技术，显著提高了表示此类工作流程的结构化输出的质量。得益于我们对RAG的实施，我们提出的系统在输出中显著减少了幻觉，并提高了我们的LLM在域外设置中的泛化能力。此外，我们还表明，使用一个小型、训练有素的检索器编码器可以减小伴随的LLM的大小，从而使得基于LLM的系统部署更加资源高效。</p>
<h4 id="_110">一句话总结：</h4>
<p>通过采用检索增强生成（RAG）技术，本研究提出的方法显著降低了生成式人工智能系统中的幻觉，并提高了大型语言模型在域外场景下的泛化能力。</p>
<hr />
<h2 id="researchagent-iterative-research-idea-generation-over-scientific-literature-with-large-language-models"><a href="http://arxiv.org/abs/2404.07738v1">ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</a></h2>
<p>发布时间：2024-04-11</p>
<p>作者：Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang</p>
<h4 id="_111">中文摘要：</h4>
<p>科学研究，对于改善人类生活至关重要，但其固有的复杂性、缓慢的进度以及需要专业专家的需求限制了其生产力。为了提高其生产力，我们提出了一种研究代理（ResearchAgent），这是一种由大型语言模型驱动的科研想法写作代理，能够自动生成问题、方法和实验设计，并通过迭代地基于科学文献进行优化。具体来说，我们的研究代理从核心论文出发，作为生成想法的焦点，不仅通过连接学术图中的相关信息增加了相关出版物，还基于其潜在概念从以实体为中心的知识库中检索实体，这些实体是从众多论文中挖掘和共享的。此外，借鉴人类通过同行讨论迭代改进想法的方法，我们利用多个提供迭代审查和反馈的审查代理（ReviewingAgents）。进一步地，这些代理被实例化为与人类偏好一致的大型语言模型，其评估标准来源于实际的人类判断。我们在多个学科的科学研究出版物上进行了实验验证，展示了该代理在生成新颖、清晰和有效的研究想法方面的有效性，这些想法基于人类和基于模型的评估结果。</p>
<h4 id="_112">一句话总结：</h4>
<p>该研究提出了一种基于大型语言模型的研究代理，能够自动生成和优化科研想法，有效提高了科学研究的效率。</p>
<hr />
<h2 id="generative-information-retrieval-evaluation"><a href="http://arxiv.org/abs/2404.08137v2">Generative Information Retrieval Evaluation</a></h2>
<p>发布时间：2024-04-11</p>
<p>作者：Marwah Alaofi, Negar Arabzadeh, Charles L. A. Clarke, Mark Sanderson</p>
<h4 id="_113">中文摘要：</h4>
<p>本文是即将出版的一本关于生成式信息检索的书籍中的一个章节草案，由Chirag Shah和Ryen White共同编辑。在这一章节中，我们从两个相互关联但不同的视角来探讨生成式信息检索的评价。首先，大型语言模型（LLMs）本身正迅速成为评价工具，现有研究表明，在基本的相关性判断任务上，LLMs可能优于众包工作者和其他付费评估者。我们回顾了过去和正在进行的相关研究，包括对TREC等共享任务计划的未来以及持续需要人工评估的讨论。其次，我们考虑了基于LLM的生成式信息检索（GenIR）系统，包括检索增强生成（RAG）系统的评价。我们考虑了既关注GenIR系统的端到端评价，又关注作为RAG系统中元素的检索组件评价的方法。展望未来，我们预计GenIR系统的评价至少部分将基于基于LLM的评价，从而产生一种明显的循环性，即系统似乎在评估其自身的输出。我们以两种方式解决这种明显的循环性：1）将基于LLM的评价视为一种“慢速搜索”，即使用较慢的IR系统进行评价和训练快速的生产IR系统；2）认识到继续将评价建立在人工评估之上的必要性，即使该人工评估的特征必须改变。</p>
<h4 id="_114">一句话总结：</h4>
<p>本文探讨了基于大型语言模型的生成式信息检索评价，分析了LLMs在评价中的潜力及其与人工评估的关系。</p>
<hr />
<h2 id="llms-in-biomedicine-a-study-on-clinical-named-entity-recognition"><a href="http://arxiv.org/abs/2404.07376v2">LLMs in Biomedicine: A study on clinical Named Entity Recognition</a></h2>
<p>发布时间：2024-04-10</p>
<p>作者：Masoud Monajatipoor, Jiaxin Yang, Joel Stremmel, Melika Emami, Fazlolah Mohaghegh, Mozhdeh Rouhsedaghat, Kai-Wei Chang</p>
<h4 id="_115">中文摘要：</h4>
<p>大型语言模型（LLMs）在多种自然语言处理（NLP）任务中表现出卓越的灵活性，但在生物医学领域由于语言复杂性和数据稀缺性而面临独特的挑战。本文通过探索提升其命名实体识别（NER）任务性能的策略，研究了LLMs在生物医学领域的应用。我们的研究揭示了精心设计的提示在生物医学领域的重要性。在上下文中战略性地选择示例可以显著提高性能，为生物医学少样本NER的所有基准数据集提供了约15-20%的F1分数提升。此外，我们的结果表明，通过提示策略整合外部生物医学知识可以增强通用LLMs满足生物医学NER专业化需求的能力。利用医学知识库，我们提出的基于检索增强生成（RAG）的DiRAG方法可以提升LLMs在生物医学NER中的零样本F1分数。代码已发布在\url{https://github.com/masoud-monajati/LLM_Bio_NER}。</p>
<h4 id="_116">一句话总结：</h4>
<p>本文提出了一种基于检索增强生成的方法，通过精心设计的提示和外部生物医学知识整合，显著提升了大型语言模型在生物医学命名实体识别任务中的性能。</p>
<hr />
<h2 id="towards-robustness-of-text-to-visualization-translation-against-lexical-and-phrasal-variability"><a href="http://arxiv.org/abs/2404.07135v2">Towards Robustness of Text-to-Visualization Translation against Lexical and Phrasal Variability</a></h2>
<p>发布时间：2024-04-10</p>
<p>作者：Jinwei Lu, Yuanfeng Song, Haodi Zhang, Chen Zhang, Raymond Chi-Wing Wong</p>
<h4 id="_117">中文摘要：</h4>
<p>Text-to-Vis（文本到可视化）是自然语言处理（NLP）领域的一项新兴任务，旨在从自然语言问题（NLQs）自动生成数据可视化。尽管现有文本到可视化模型取得了一定的进展，但它们往往过度依赖问题中的单词与数据模式中的标记之间的词汇匹配。这种对词汇匹配的过度依赖可能导致模型对输入变化的鲁棒性降低。在本研究中，我们全面考察了当前文本到可视化模型的鲁棒性，这一领域之前尚未被探索。特别是，我们构建了第一个鲁棒性数据集nvBench-Rob，它基于原始文本到可视化基准nvBench包含了多样化的词汇和短语变化。然后，我们发现现有文本到可视化模型在这新数据集上的性能显著下降，这表明这些方法在整体上表现出不足的鲁棒性。最后，我们提出了一种基于检索增强生成（RAG）技术的全新框架，命名为GRED，专门设计用于解决这两种变体中的输入扰动问题。该框架由三个部分组成：NLQ-Retrieval Generator（自然语言问题检索生成器）、Visualization Query-Retrieval Retuner（可视化查询检索重调器）和Annotation-based Debugger（基于注释的调试器），分别用于解决自然语言变体、编程风格差异和数据模式变体带来的挑战。广泛的实验评估表明，与文本到可视化领域的最先进模型RGVisNet相比，GRED在模型鲁棒性方面表现更优，在提出的nvBench-Rob数据集上准确率提高了32%。</p>
<h4 id="_118">一句话总结：</h4>
<p>本研究提出了一种名为GRED的基于RAG技术的文本到可视化模型，显著提高了模型对输入扰动的鲁棒性，并在鲁棒性数据集上实现了更高的准确率。</p>
<hr />
<h2 id="groundedness-in-retrieval-augmented-long-form-generation-an-empirical-study"><a href="http://arxiv.org/abs/2404.07060v1">Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study</a></h2>
<p>发布时间：2024-04-10</p>
<p>作者：Alessandro Stolfo</p>
<h4 id="_119">中文摘要：</h4>
<p>我们通过对检索增强的大型语言模型（LLMs）在长格式问答（LFQA）中的扎根性进行了实证研究。具体来说，我们评估了每个生成的句子是否基于检索到的文档或模型的预训练数据。在3个数据集和4个模型家族中，我们的发现显示，相当一部分生成的句子始终缺乏扎根性，即使这些句子包含了正确的真实答案。此外，我们还考察了模型大小、解码策略和指令调整等因素对扎根性的影响。我们的结果表明，虽然较大的模型倾向于更有效地使输出扎根，但仍有相当一部分正确答案受到幻觉的影响。这项研究为LFQA中的扎根性挑战提供了新的见解，并强调了在LLMs中实施更稳健机制以减轻无扎根内容生成的必要性。</p>
<h4 id="_120">一句话总结：</h4>
<p>本研究揭示了长格式问答中大型语言模型生成句子缺乏扎根性的问题，并强调了改进LLMs以减少无根据内容生成的必要性。</p>
<hr />
<h2 id="superposition-prompting-improving-and-accelerating-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2404.06910v2">Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-04-10</p>
<p>作者：Thomas Merth, Qichen Fu, Mohammad Rastegari, Mahyar Najibi</p>
<h4 id="_121">中文摘要：</h4>
<p>尽管大型语言模型（LLMs）取得了成功，但它们在处理长文本时存在显著的缺点。它们的推理成本与序列长度呈平方级增长，这使得它们在检索增强生成（RAG）等一些实际文本处理应用中部署成本高昂。此外，LLMs还表现出“干扰现象”，即提示中的无关上下文会降低输出质量。为了解决这些缺点，我们提出了一种新的RAG提示方法，称为“叠加提示”，该方法可以直接应用于预训练的基于transformer的LLMs（无需微调）。从高层次来看，叠加提示允许LLM并行处理输入文档的提示路径，一旦认为路径无关，就丢弃这些路径。我们展示了我们的方法能够同时提高多种问答基准的时间效率，使用多个预训练的LLMs。此外，当检索到的上下文相对于模型训练的上下文较大时，我们的技术显著提高了准确性。例如，与简单的RAG相比，我们的方法在NaturalQuestions-Open数据集上使用MPT-7B指令微调模型，将计算时间减少了93倍，同时将准确性提高了43%。</p>
<h4 id="_122">一句话总结：</h4>
<p>提出了一种名为“叠加提示”的新RAG方法，显著提高了LLMs在处理长文本和检索增强生成任务中的效率和准确性。</p>
<hr />
<h2 id="enhancing-question-answering-for-enterprise-knowledge-bases-using-large-language-models"><a href="http://arxiv.org/abs/2404.08695v2">Enhancing Question Answering for Enterprise Knowledge Bases using Large Language Models</a></h2>
<p>发布时间：2024-04-10</p>
<p>作者：Feihu Jiang, Chuan Qin, Kaichun Yao, Chuyu Fang, Fuzhen Zhuang, Hengshu Zhu, Hui Xiong</p>
<h4 id="_123">中文摘要：</h4>
<p>高效的知识管理在提升企业和组织的运营效率和创新能力方面发挥着关键作用。通过向量化的方式对知识进行索引，涌现出多种知识检索方法，显著提高了知识管理系统的工作效率。近年来，生成式自然语言处理技术的快速发展为在检索到针对用户查询的相关文档后生成精确且连贯的答案铺平了道路。然而，对于企业知识库而言，由于私有数据的隐私和安全政策，从头开始收集大量的训练数据用于知识检索和生成是一项巨大的挑战，这往往伴随着高昂的成本。为了解决上述挑战，本文提出了一种基于大型语言模型（LLMs）的新型检索-生成框架（EKRG），该框架专门设计用于以有限的标注成本实现企业知识库的问答功能。具体而言，对于检索过程，我们首先介绍了一种使用LLM进行指令微调的方法，以生成足够的文档-问题对来训练知识检索器。这种方法通过精心设计的指令，有效地为企业和知识库生成多样化的问题，包括面向事实和面向解决方案的知识。此外，我们还开发了一种相关性感知的教师-学生学习策略，以进一步提高训练过程的效率。对于生成过程，我们提出了一种基于思维链（CoT）的微调方法，以赋予基于LLM的生成器使用检索到的文档巧妙地回答用户问题的能力。最后，在真实世界数据集上的广泛实验证明了我们提出框架的有效性。</p>
<h4 id="_124">一句话总结：</h4>
<p>本文提出了一种基于大型语言模型的企业知识库问答框架，通过有限的标注成本实现高效的知识检索和生成。</p>
<hr />
<h2 id="not-all-contexts-are-equal-teaching-llms-credibility-aware-generation"><a href="http://arxiv.org/abs/2404.06809v2">Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation</a></h2>
<p>发布时间：2024-04-10</p>
<p>作者：Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, Le Sun</p>
<h4 id="_125">中文摘要：</h4>
<p>本文针对大型语言模型快速发展的背景下，检索增强生成（RAG）技术的广泛应用及其在检索过程中可能引入的缺陷信息问题，提出了可信度感知生成（CAG）框架。该框架旨在通过数据可信度转换，使模型具备识别和处理信息可信度的能力，从而减轻RAG中缺陷信息的影响。为了准确评估CAG模型的能力，我们构建了一个涵盖三个关键现实场景的综合基准。实验结果表明，我们的模型能够有效理解和利用可信度进行生成，显著优于其他检索增强模型，并对噪声文档的干扰具有抗性，从而保持稳健的性能。此外，我们的模型支持定制可信度，具有广泛的应用潜力。</p>
<h4 id="_126">一句话总结：</h4>
<p>本文提出了一种可信度感知生成框架，有效提升了检索增强生成模型在处理信息可信度方面的能力，并展现出对噪声干扰的鲁棒性。</p>
<hr />
<h2 id="onco-retriever-generative-classifier-for-retrieval-of-ehr-records-in-oncology"><a href="http://arxiv.org/abs/2404.06680v1">Onco-Retriever: Generative Classifier for Retrieval of EHR Records in Oncology</a></h2>
<p>发布时间：2024-04-10</p>
<p>作者：Shashi Kant Gupta, Aditya Basu, Bradley Taylor, Anai Kothari, Hrituraj Singh</p>
<h4 id="_127">中文摘要：</h4>
<p>从电子健康记录（EHR）系统中检索信息对于回答关于患者旅程的具体问题以及改善临床护理的提供至关重要。尽管如此，大多数EHR系统仍然依赖于基于关键词的搜索。随着生成式大型语言模型（LLMs）的出现，信息检索可以带来更好的搜索和摘要能力。此类检索器还可以为检索增强生成（RAG）管道提供数据，以回答任何查询。然而，由于创建查询-文档支持对困难，从EHR系统中的真实世界临床数据中检索信息以解决多个下游用例的任务是具有挑战性的。我们提供了一个使用大型语言模型以经济方式创建此类数据集的蓝图。我们的方法产生了一个检索器，其F-1分数比Ada和Mistral等专有对手在肿瘤数据元素上高出30-50个百分点。我们还进一步将我们的模型（称为Onco-Retriever）与微调后的PubMedBERT模型进行了比较。我们对真实世界的EHR数据进行了广泛的人工评估，并对不同模型的延迟进行了分析，为医疗保健组织构建特定领域的检索器提供了前进的道路。</p>
<h4 id="_128">一句话总结：</h4>
<p>本研究提出了一种利用大型语言模型创建EHR信息检索数据集的方法，显著提升了检索性能，为医疗保健组织构建特定领域检索器提供了新的路径。</p>
<hr />
<h2 id="rar-b-reasoning-as-retrieval-benchmark"><a href="http://arxiv.org/abs/2404.06347v2">RAR-b: Reasoning as Retrieval Benchmark</a></h2>
<p>发布时间：2024-04-09</p>
<p>作者：Chenghao Xiao, G Thomas Hudson, Noura Al Moubayed</p>
<h4 id="_129">中文摘要：</h4>
<p>语义文本相似度（Semantic Textual Similarity, STS）和信息检索任务（Information Retrieval tasks, IR）是过去几年中记录嵌入模型进展的两个主要途径。在新兴的检索增强生成（Retrieval-augmented Generation, RAG）范式下，我们预见有必要评估嵌入模型的高级语言理解能力，并自觉地审视其中存储的推理能力。针对这一问题，我们提出了疑问：检索器能否解决推理问题？通过将推理任务转化为检索任务，我们发现，即使没有专门训练用于推理级别的语言理解，当前最先进的检索器模型可能仍然远未具备胜任辅助大型语言模型（LLMs）角色的能力，尤其是在推理密集型任务中。此外，尽管训练了指令意识，但指令感知的信息检索模型在推理时间内的推理任务中往往在没有指令的情况下表现更好，这为研究社区提供了一个被忽视的检索器-LLM行为差距，需要对其进行对齐。然而，基于解码器的嵌入模型在缩小这一差距方面显示出巨大的潜力，突显了嵌入模型实现推理级别语言理解的道路。我们还表明，尽管当前的现成再排序模型在这些任务上失败，但通过微调注入推理能力似乎比注入双编码器更容易，我们通过微调再排序模型在所有任务上实现了最先进的性能。我们发布了推理作为检索基准（Reasoning as Retrieval Benchmark, RAR-b），这是一个全面的任务和设置套件，用于评估检索器模型中存储的推理能力。RAR-b可在https://github.com/gowitheflow-1998/RAR-b获取。</p>
<h4 id="_130">一句话总结：</h4>
<p>本文提出了一种新的评估嵌入模型推理能力的方法，并通过实验表明，通过微调再排序模型可以缩小检索器与LLMs在推理任务上的能力差距。</p>
<hr />
<h2 id="dimensionality-reduction-in-sentence-transformer-vector-databases-with-fast-fourier-transform"><a href="http://arxiv.org/abs/2404.06278v1">Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform</a></h2>
<p>发布时间：2024-04-09</p>
<p>作者：Vitaly Bulgakov, Alec Segal</p>
<h4 id="_131">中文摘要：</h4>
<p>在向量数据库中进行维度降低对于简化人工智能数据管理至关重要，它能够实现高效的存储、快速的计算和提升模型性能。本文探讨了降低向量数据库维度的益处，重点关注计算效率和克服维度灾难。我们引入了快速傅里叶变换（FFT）在维度降低中的新颖应用，这是一种在此背景下尚未充分利用的方法。通过展示其在各种人工智能领域中的实用性，包括检索增强生成（RAG）模型和图像处理，基于FFT的方法有望改善数据检索过程，并提升人工智能解决方案的效率和可扩展性。FFT的引入不仅可能优化实时处理和推荐系统中的操作，还可以扩展到高级图像处理技术，其中维度降低可以显著提高性能和分析效率。本文倡导在向量数据库管理中更广泛地采用FFT，标志着在解决人工智能研究和应用中数据量和复杂性的挑战方面迈出了重要一步。与许多现有方法不同，我们直接处理模型在处理测试输入后产生的嵌入向量。</p>
<h4 id="_132">一句话总结：</h4>
<p>本文提出利用快速傅里叶变换（FFT）进行向量数据库的维度降低，以提升人工智能数据管理的效率和性能。</p>
<hr />
<h2 id="aisaq-all-in-storage-anns-with-product-quantization-for-dram-free-information-retrieval"><a href="http://arxiv.org/abs/2404.06004v1">AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval</a></h2>
<p>发布时间：2024-04-09</p>
<p>作者：Kento Tatsuno, Daisuke Miyashita, Taiga Ikeda, Kiyoshi Ishiyama, Kazunari Sumiyoshi, Jun Deguchi</p>
<h4 id="_133">中文摘要：</h4>
<p>在基于近似邻近图（approximate proximity graphs）的近似最近邻搜索（ANNS）方法中，DiskANN通过使用RAM和存储器，在大型数据集上实现了良好的召回率-速度平衡。尽管它通过产品量化（PQ）加载压缩向量来声称节省内存使用，但其内存使用量与数据集规模成比例增加。在本文中，我们提出了基于产品量化（Product Quantization）的存储全部数据（All-in-Storage）ANNS（AiSAQ），该方法将压缩向量卸载到存储器中。我们的方法即使在具有数十亿规模的数据集上进行查询搜索时，也能实现约10MB的内存使用，并且性能下降很小。AiSAQ还减少了查询搜索前的索引加载时间，这使得在多个数十亿规模的数据集之间切换索引成为可能，从而显著增强了检索增强生成（RAG）的灵活性。该方法适用于所有基于图的ANNS算法，并且未来可以与更高规格的ANNS方法相结合。</p>
<h4 id="_134">一句话总结：</h4>
<p>本文提出了一种基于存储全部数据的近似最近邻搜索方法，通过将压缩向量卸载到存储器中，实现了在大规模数据集上高效且低内存使用的近似最近邻搜索。</p>
<hr />
<h2 id="event-enhanced-retrieval-in-real-time-search"><a href="http://arxiv.org/abs/2404.05989v1">Event-enhanced Retrieval in Real-time Search</a></h2>
<p>发布时间：2024-04-09</p>
<p>作者：Yanan Zhang, Xiaoling Bai, Tianhua Zhou</p>
<h4 id="_135">中文摘要：</h4>
<p>基于嵌入的检索（EBR）方法在主流搜索引擎检索系统中被广泛使用，并在近期检索增强方法中对于消除大型语言模型（LLM）幻觉方面至关重要。然而，现有的EBR模型常常面临“语义漂移”问题，以及对关键信息的关注不足，导致后续步骤中检索结果的采用率较低。这一问题在实时搜索场景中尤为明显，因为互联网上对热门事件的多种表达使得实时检索高度依赖于关键事件信息。为了解决这一问题，本文提出了一种名为EER的新方法，通过改进传统EBR的双编码器模型来提升实时检索性能。我们引入对比学习以伴随成对学习进行编码器优化。此外，为了加强事件中对关键事件信息的关注，我们在文档编码器之后加入了解码模块，引入了基于提示调整的生成事件三元组提取方案，并通过比较学习将事件与查询编码器优化相关联。这个解码模块在推理过程中可以被移除。大量的实验表明，EER可以显著提高实时搜索检索性能。我们相信这种方法将为信息检索领域提供新的视角。（embedding-based retrieval, LLM illusions, semantic drift, key information, real-time search scenarios, contrastive learning, pairwise learning, decoder module, prompt-tuning, generative event triplet extraction, comparative learning）</p>
<h4 id="_136">一句话总结：</h4>
<p>本文提出的EER方法通过改进双编码器模型和引入对比学习，显著提升了实时搜索检索性能，为信息检索领域提供了新的视角。</p>
<hr />
<h2 id="optimization-methods-for-personalizing-large-language-models-through-retrieval-augmentation"><a href="http://arxiv.org/abs/2404.05970v1">Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation</a></h2>
<p>发布时间：2024-04-09</p>
<p>作者：Alireza Salemi, Surya Kallumadi, Hamed Zamani</p>
<h4 id="_137">中文摘要：</h4>
<p>本文研究了针对个性化大型语言模型（LLMs）的检索增强方法，这些方法可能对各种应用和领域产生重大影响。我们首次提出优化检索模型，以便为大型语言模型提供有限数量的个性化文档以实现个性化生成。我们开发了两种优化算法，这些算法从下游个性化生成任务中获取反馈以优化检索——一种基于强化学习，其奖励函数使用任何任意指标定义个性化生成；另一种基于从下游LLM到检索模型的知识蒸馏。本文还介绍了一种生成前后的检索器选择模型，该模型决定为每个LLM输入选择哪个检索器。在语言模型个性化（LaMP）基准测试中的各种任务上进行的广泛实验表明，在七个数据集中有六个数据集的统计意义显著改进。</p>
<h4 id="_138">一句话总结：</h4>
<p>本文提出了一种优化检索模型的方法，以提升大型语言模型的个性化生成能力，并在多个数据集上实现了显著的性能提升。</p>
<hr />
<h2 id="medexpqa-multilingual-benchmarking-of-large-language-models-for-medical-question-answering"><a href="http://arxiv.org/abs/2404.05590v2">MedExpQA: Multilingual Benchmarking of Large Language Models for Medical Question Answering</a></h2>
<p>发布时间：2024-04-08</p>
<p>作者：Iñigo Alonso, Maite Oronoz, Rodrigo Agerri</p>
<h4 id="_139">中文摘要：</h4>
<p>大型语言模型（LLMs）具有促进人工智能技术在医疗专家互动决策支持中发展的潜力，这在医疗问答（Medical QA）中的竞争表现中得到证明。然而，尽管表现令人印象深刻，但医疗应用所需的品质标准仍然远未达到。目前，LLMs面临着知识过时和生成幻觉内容的挑战。此外，大多数用于评估医疗知识的基准缺乏参考黄金解释，这意味着无法评估LLMs预测的推理。最后，如果我们考虑对英语以外的语言进行LLMs基准测试，情况尤其严峻，因为据我们所知，这是一个完全被忽视的话题。为了解决这些不足，在本文中，我们提出了MedExpQA，这是第一个基于医学考试的跨语言基准，用于评估LLMs在医疗问答中的表现。据我们所知，MedExpQA首次包括了由医生编写的参考黄金解释，这些解释可以用来建立各种基于黄金的比较LLMs性能的上限。使用黄金参考解释和检索增强生成（RAG）方法进行的全面多语言实验表明，LLMs的性能仍有很大的提升空间，尤其是在英语以外的语言。此外，尽管使用了最先进的RAG方法，我们的结果也表明，获取和整合可能对下游医疗问答评估结果产生积极影响的现有医疗知识具有难度。到目前为止，该基准已提供四种语言版本，但我们希望这项工作能够鼓励对其他语言的进一步发展。</p>
<h4 id="_140">一句话总结：</h4>
<p>MedExpQA是首个基于医学考试的跨语言基准，旨在评估LLMs在医疗问答中的表现，并揭示了LLMs在医疗知识获取和整合方面的挑战。</p>
<hr />
<h2 id="enhancing-software-related-information-extraction-via-single-choice-question-answering-with-large-language-models"><a href="http://arxiv.org/abs/2404.05587v2">Enhancing Software-Related Information Extraction via Single-Choice Question Answering with Large Language Models</a></h2>
<p>发布时间：2024-04-08</p>
<p>作者：Wolfgang Otto, Sharmila Upadhyaya, Stefan Dietze</p>
<h4 id="_141">中文摘要：</h4>
<p>本文描述了我们在软件提及消歧（SOMD）共享任务中的参与，重点关注通过单选题问答使用生成式大型语言模型（LLMs）来改进学术文本中的关系抽取。该方法优先考虑使用GLMs的上下文学习能力来提取软件相关实体及其描述性属性，例如分发信息。我们的方法使用检索增强生成（RAG）技术和GLMs进行命名实体识别（NER）和属性NER，以识别提取的软件实体之间的关系，为分析学术文献中的软件引用提供了一种结构化解决方案。本文详细描述了我们的方法，展示了在单选题问答范式中使用GLMs如何极大地增强信息抽取（IE）方法。我们在SOMD共享任务中的参与突出了精确软件引用实践的重要性，并展示了我们系统克服软件提及消歧和关系提取挑战的能力。这为该领域的未来研究和发展奠定了基础。</p>
<h4 id="_142">一句话总结：</h4>
<p>本文提出了一种基于GLMs和RAG技术的学术文本软件提及消歧方法，有效提升了关系抽取的准确性，为软件引用分析提供了结构化解决方案。</p>
<hr />
<h2 id="retrieval-augmented-open-vocabulary-object-detection"><a href="http://arxiv.org/abs/2404.05687v1">Retrieval-Augmented Open-Vocabulary Object Detection</a></h2>
<p>发布时间：2024-04-08</p>
<p>作者：Jooyeon Kim, Eulrang Cho, Sehyung Kim, Hyunwoo J. Kim</p>
<h4 id="_143">中文摘要：</h4>
<p>开放词汇目标检测（OVD）通过视觉-语言模型（VLMs）来检测超出预训练类别的全新物体。先前的方法通过使用带有额外“类别”名称的“正”伪标签来提高泛化能力，以扩展检测器的知识，例如袜子、iPod和鳄鱼。为了在两个方面扩展先前的方法，我们提出了检索增强损失和视觉特征（RALF）。我们的方法检索相关的“负”类别并增强损失函数。此外，视觉特征通过类别的“词汇化概念”进行增强，例如穿在脚上、手持音乐播放器和尖锐的牙齿。具体来说，RALF由两个模块组成：检索增强损失（RAL）和检索增强视觉特征（RAF）。RAL包含两个损失，反映了与负词汇的语义相似性。此外，RAF使用大型语言模型（LLM）中的词汇化概念来增强视觉特征。我们的实验证明了RALF在COCO和LVIS基准数据集上的有效性。我们在COCO数据集的新类别上实现了高达3.4 box AP$<em>{50}^{\text{N}}$的提升，在LVIS数据集上实现了3.6 mask AP$</em>{\text{r}}$的增益。代码可在https://github.com/mlvlab/RALF 获取。</p>
<h4 id="_144">一句话总结：</h4>
<p>该研究提出了一种名为RALF的开放词汇目标检测方法，通过检索增强损失和视觉特征，显著提高了检测新类别物体的性能。</p>
<hr />
<h2 id="havtr-improving-video-text-retrieval-through-augmentation-using-large-foundation-models"><a href="http://arxiv.org/abs/2404.05083v1">HaVTR: Improving Video-Text Retrieval Through Augmentation Using Large Foundation Models</a></h2>
<p>发布时间：2024-04-07</p>
<p>作者：Yimu Wang, Shuai Yuan, Xiangru Jian, Wei Pang, Mushi Wang, Ning Yu</p>
<h4 id="_145">中文摘要：</h4>
<p>尽管视频-文本检索在最近取得了进展，主要得益于强大的模型架构和训练策略的探索，但由于训练数据标注的低质量和稀缺性，视频-文本检索模型的表征学习能力仍然有限。为了解决这个问题，我们提出了一种新颖的视频-文本学习范式，称为HaVTR，该范式通过增强视频和文本数据来学习更通用的特征。具体来说，我们首先采用了一种简单的增强方法，通过随机复制或删除子词和帧来生成自相似数据。此外，受最近在视觉和语言生成模型方面的进展的启发，我们提出了一种更强大的增强方法，通过使用大型语言模型（LLMs）和视觉生成模型（VGMs）进行文本释义和视频风格化。进一步地，为了将更丰富的信息引入视频和文本，我们提出了一种基于幻觉的增强方法，其中我们使用LLMs和VGMs生成并添加新的相关信息到原始数据中。得益于增强后的数据，在多个视频-文本检索基准上的广泛实验表明，HaVTR优于现有方法。</p>
<h4 id="_146">一句话总结：</h4>
<p>HaVTR通过增强视频和文本数据，利用LLMs和VGMs生成新信息，显著提升了视频-文本检索模型的表征学习能力。</p>
<hr />
<h2 id="robomp2-a-robotic-multimodal-perception-planning-framework-with-multimodal-large-language-models"><a href="http://arxiv.org/abs/2404.04929v2">RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models</a></h2>
<p>发布时间：2024-04-07</p>
<p>作者：Qi Lv, Hao Li, Xiang Deng, Rui Shao, Michael Yu Wang, Liqiang Nie</p>
<h4 id="_147">中文摘要：</h4>
<p>多模态大型语言模型（MLLMs）在各种领域展示了令人印象深刻的推理能力和通用智能。这激励研究人员训练端到端的多模态大型语言模型，或利用大型模型生成具有人类选择提示的具身智能体的策略。然而，这些方法在未见过的任务或场景上表现出有限的泛化能力，并且忽略了对于机器人做出决策至关重要的多模态环境信息。在本文中，我们介绍了一种新颖的机器人多模态感知-规划（RoboMP$^2$）框架，用于机器人操作，该框架包括一个目标条件多模态感知器（GCMP）和一个检索增强多模态规划器（RAMP）。特别地，GCMP通过使用针对具身智能体定制的具有语义推理和定位能力的MLLMs来捕获环境状态。RAMP利用粗到细的检索方法找到最相关的$k$个策略作为情境演示来增强规划器。大量的实验表明，RoboMP$^2$在VIMA基准测试和真实世界任务上都优于基线，相较于基线有大约10%的提升。</p>
<h4 id="_148">一句话总结：</h4>
<p>本文提出了一种基于多模态感知和检索增强的机器人多模态规划框架，显著提升了机器人操作的性能。</p>
<hr />
<h2 id="information-retrieval-with-entity-linking"><a href="http://arxiv.org/abs/2404.08678v1">Information Retrieval with Entity Linking</a></h2>
<p>发布时间：2024-04-07</p>
<p>作者：Dahlia Shehata</p>
<h4 id="_149">中文摘要：</h4>
<p>尽管在低资源环境下具有优势，传统的稀疏检索器依赖于查询和集合的高维词袋（BoW）表示之间的精确匹配方法。因此，检索性能受到语义差异和词汇差距的限制。另一方面，基于transformer的密集检索器通过利用语料库的低维上下文表示，在信息检索任务中引入了显著的改进。虽然密集检索器因其相对有效性而闻名，但与稀疏检索器相比，它们在效率较低和泛化能力不足方面存在缺陷。对于轻量级检索任务，高计算资源和时间消耗是主要的障碍，这促使人们放弃密集模型，尽管可能带来收益。在这项工作中，我提出通过以下两种格式扩展查询和文档中的链接实体来提升稀疏检索器的性能：1）显式和2）哈希。采用零样本端到端密集实体链接系统进行实体识别和消歧，以增强语料库。通过利用先进的实体链接方法，我相信可以缩小稀疏检索器和密集检索器之间的有效性差距。实验在MS MARCO passage数据集上使用原始qrel集、MonoT5偏好的重排序qrels以及由DuoT5进一步重排序的后者集进行。由于我关注大型信息检索系统中级联排名架构的早期检索阶段，因此结果使用recall@1000进行评估。所提出的方法还能够检索出在先前工作中被认为特别困难的查询子集的文档。</p>
<h4 id="_150">一句话总结：</h4>
<p>通过结合实体链接技术和稀疏检索器，本研究提出了一种提高检索性能的新方法，旨在解决传统稀疏检索器在低资源环境下的局限性。</p>
<hr />
<h2 id="q-peft-query-dependent-parameter-efficient-fine-tuning-for-text-reranking-with-large-language-models"><a href="http://arxiv.org/abs/2404.04522v2">Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text Reranking with Large Language Models</a></h2>
<p>发布时间：2024-04-06</p>
<p>作者：Zhiyuan Peng, Xuyang Wu, Qifan Wang, Sravanthi Rajanala, Yi Fang</p>
<h4 id="_151">中文摘要：</h4>
<p>参数高效微调（PEFT）方法已被广泛用于大型语言模型（LLMs）中，以提高下游任务性能，而无需对整个LLMs进行微调。近期研究表明，如何有效地使用PEFT对LLMs在排序任务中进行微调，并取得了令人信服的性能；但仍存在一些局限性，包括学习到的提示对不同文档是固定的、对特定任务过度拟合以及低适应能力。在本文中，我们提出了一种查询依赖的参数高效微调（Q-PEFT）方法，用于文本重排序，以将真实查询的信息泄露给LLMs，从而使得从输入文档生成真实查询变得更加容易。具体来说，我们利用查询从拼接的文档中提取前$k$个标记，作为上下文线索。我们进一步通过用多头注意力层替换检索机制来增强Q-PEFT，实现端到端训练并覆盖文档中的所有标记，引导LLMs生成更多针对特定文档的合成查询，从而进一步提高重排序性能。在四个公开数据集上进行了大量实验，证明了我们提出方法的有效性。</p>
<h4 id="_152">一句话总结：</h4>
<p>本文提出了一种基于查询依赖的参数高效微调（Q-PEFT）方法，用于文本重排序，通过泄露真实查询信息给LLMs，有效提高了重排序性能。</p>
<hr />
<h2 id="iitk-at-semeval-2024-task-2-exploring-the-capabilities-of-llms-for-safe-biomedical-natural-language-inference-for-clinical-trials"><a href="http://arxiv.org/abs/2404.04510v1">IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials</a></h2>
<p>发布时间：2024-04-06</p>
<p>作者：Shreyasi Mandal, Ashutosh Modi</p>
<h4 id="_153">中文摘要：</h4>
<p>大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展现了最先进的性能，然而它们容易受到捷径学习和事实不一致性的影响。本研究调查了LLMs在执行自然语言推理（NLI）任务时，在SemEval 2024任务2的背景下，对乳腺癌临床试验报告（CTRs）的鲁棒性、一致性和忠实推理能力。我们考察了LLMs的推理能力和它们在逻辑问题解决方面的熟练程度。在零样本设置下，我们使用检索增强生成（RAG）框架，对预训练语言模型（PLMs）、GPT-3.5和Gemini Pro进行了比较分析，并整合了各种推理链。评估结果显示，在测试数据集上，F1分数为0.69，一致性为0.71，忠实度分数为0.90。</p>
<h4 id="_154">一句话总结：</h4>
<p>本研究评估了大型语言模型在乳腺癌临床试验报告自然语言推理任务中的鲁棒性、一致性和忠实推理能力，并发现它们在逻辑问题解决方面表现出一定的能力。</p>
<hr />
<h2 id="assisting-humans-in-complex-comparisons-automated-information-comparison-at-scale"><a href="http://arxiv.org/abs/2404.04351v1">Assisting humans in complex comparisons: automated information comparison at scale</a></h2>
<p>发布时间：2024-04-05</p>
<p>作者：Truman Yuen, Graham A. Watt, Yuri Lawryshyn</p>
<h4 id="_155">中文摘要：</h4>
<p>生成式大型语言模型（Generative Large Language Models）能够在知识领域内进行高效的统计分析，并在信息比较方面与人类专家相媲美。然而，由于在大型上下文中维护信息以及克服模型标记限制的困难，LLMs在信息比较中的应用面临着可扩展性的挑战。为了解决这些挑战，我们开发了新颖的抽象摘要与标准驱动的比较端点（Abstractive Summarization &amp; Criteria-driven Comparison Endpoint，简称ASC$^2$End）系统，以实现大规模的信息比较自动化。我们的系统采用语义文本相似性比较来生成有证据支持的分析。我们利用如抽象摘要和检索增强生成等经过验证的数据处理策略来克服标记限制，并在模型推理过程中保留相关信息。提示（Prompts）是通过零样本策略设计的，以实现信息语境化，从而提高模型推理能力。我们使用ROUGE评分对抽象摘要进行了评估，并使用调查反馈来评估生成的比较质量。在ASC$^2$End系统上评估的模型显示出令人满意的结果，提供了关于系统预期性能的见解。ASC$^2$End是一个新颖的系统工具，它能够在知识领域内实现准确、自动的大规模信息比较，克服了上下文长度和检索的限制。</p>
<h4 id="_156">一句话总结：</h4>
<p>ASC$^2$End系统通过抽象摘要和标准驱动的比较，实现了知识领域内大规模、准确的信息比较自动化。</p>
<hr />
<h2 id="extract-define-canonicalize-an-llm-based-framework-for-knowledge-graph-construction"><a href="http://arxiv.org/abs/2404.03868v1">Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction</a></h2>
<p>发布时间：2024-04-05</p>
<p>作者：Bowen Zhang, Harold Soh</p>
<h4 id="_157">中文摘要：</h4>
<p>在这项工作中，我们关注于从输入文本中自动创建知识图谱（KGC）的方法。大型语言模型（LLMs）的进展促使一系列近期工作将它们应用于KGC，例如通过零/少量提示。尽管这些模型在小领域特定数据集上取得了成功，但它们在扩展到许多实际应用中常见的文本时面临困难。一个主要问题是，在先前的方法中，知识图谱的架构必须包含在LLM提示中以生成有效的三元组；更大的和更复杂的架构很容易超过LLMs的上下文窗口长度。为了解决这个问题，我们提出了一种名为提取-定义-规范化的三阶段框架（EDC）：开放信息提取，随后是架构定义和后续规范化。EDC具有灵活性，可以应用于有预定义目标架构的设置，也可以在没有预定义架构的情况下应用；在后一种情况下，它自动构建架构并应用自我规范化。为了进一步提高性能，我们引入了一个训练组件，该组件检索与输入文本相关的架构元素；这种方式类似于检索增强生成，它提高了LLMs的提取性能。我们在三个KGC基准测试中展示了EDC能够提取高质量的三元组，无需任何参数调整，并且与先前工作相比，具有显著更大的架构。</p>
<h4 id="_158">一句话总结：</h4>
<p>提出了一种名为EDC的三阶段框架，通过开放信息提取、架构定义和规范化，实现从文本到知识图谱的自动创建，有效提高了大型语言模型在知识图谱创建任务中的性能。</p>
<hr />
<h2 id="a-comparison-of-methods-for-evaluating-generative-ir"><a href="http://arxiv.org/abs/2404.04044v2">A Comparison of Methods for Evaluating Generative IR</a></h2>
<p>发布时间：2024-04-05</p>
<p>作者：Negar Arabzadeh, Charles L. A. Clarke</p>
<h4 id="_159">中文摘要：</h4>
<p>信息检索系统越来越多地融合生成组件。例如，在检索增强生成（RAG）系统中，检索组件可能提供真实数据源，而生成组件则总结和增强其响应。在其他系统中，大型语言模型（LLM）可能直接生成响应而不咨询检索组件。尽管对生成信息检索（Gen-IR）系统的定义有多种，但本文我们专注于那些系统响应不是从固定文档或段落集合中抽取的系统。查询的响应可能完全是新文本。由于传统的信息检索评估方法在这种模型下失效，我们探讨了将传统离线评估方法扩展到Gen-IR环境中的各种方法。传统的离线信息检索评估通常采用付费的人类评估员，但越来越多的LLM正在取代人类评估，显示出与众包标签相似或更优的能力。鉴于Gen-IR系统不生成来自固定集合的响应，我们假设Gen-IR评估方法在很大程度上依赖于LLM生成的标签。除了基于二元和分级相关性的方法外，我们还探讨了基于显式子主题、成对偏好和嵌入的方法。我们首先将这些方法与TREC深度学习任务中的人类评估进行验证；然后，我们将这些方法应用于评估几个纯生成系统的输出。对于每种方法，我们考虑了其无需人类标签或其他输入即可自主行动的能力，以及其支持人类审计的能力。为了信任这些方法，我们必须确保其结果与人类评估一致。为了做到这一点，评估标准必须是透明的，以便人类评估员可以审计结果。</p>
<h4 id="_160">一句话总结：</h4>
<p>本文探讨了将传统信息检索评估方法扩展到生成信息检索环境，并评估了基于LLM生成标签的多种评估方法的有效性。</p>
<hr />
<h2 id="cbr-rag-case-based-reasoning-for-retrieval-augmented-generation-in-llms-for-legal-question-answering"><a href="http://arxiv.org/abs/2404.04302v1">CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering</a></h2>
<p>发布时间：2024-04-04</p>
<p>作者：Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-Orji, Ruvan Weerasinghe, Anne Liret, Bruno Fleisch</p>
<h4 id="_161">中文摘要：</h4>
<p>检索增强生成（RAG）通过提供先验知识作为输入的上下文来增强大型语言模型（LLM）的输出。这对于需要证据来验证生成文本输出的知识密集型和专家依赖型任务，如法律问答，是有益的。我们强调，基于案例推理（CBR）为在LLM中将检索作为RAG过程的一部分提供了关键机会。我们引入了CBR-RAG，其中CBR周期的初始检索阶段、其索引词汇和相似性知识容器被用来增强LLM查询与上下文相关的案例。这种集成增强了原始LLM查询，提供了一个更丰富的提示。我们展示了CBR-RAG的评估，并检查了不同表示（即通用和领域特定嵌入）以及比较方法（即交互、内部和混合相似性）在法律问答任务上的应用。我们的结果表明，CBR案例重用的上下文提供了相关问题相关组件和证据库之间的相似性，从而显著提高了生成答案的质量。</p>
<h4 id="_162">一句话总结：</h4>
<p>CBR-RAG通过结合基于案例推理的检索策略，显著提升了大型语言模型在法律问答任务上生成答案的质量。</p>
<hr />
<h2 id="conflare-conformal-large-language-model-retrieval"><a href="http://arxiv.org/abs/2404.04287v1">CONFLARE: CONFormal LArge language model REtrieval</a></h2>
<p>发布时间：2024-04-04</p>
<p>作者：Pouria Rouzrokh, Shahriar Faghani, Cooper U. Gamble, Moein Shariatnia, Bradley J. Erickson</p>
<h4 id="_163">中文摘要：</h4>
<p>检索增强生成（RAG）框架使大型语言模型（LLMs）能够从知识库中检索相关信息并将其纳入上下文中以生成响应。这有助于减少幻觉并允许在不重新训练LLM的情况下更新知识。然而，如果检索未能识别必要的作为响应生成上下文的信息，RAG并不能保证生成有效的响应。此外，如果存在矛盾内容，RAG的响应可能只会反映两种可能响应中的一种。因此，量化检索过程中的不确定性对于确保RAG的可靠性至关重要。在本报告中，我们介绍了一种将符合预测应用于量化RAG框架中检索不确定性的四步框架。首先，构建一个可从知识库中回答的问题校准集。将每个问题的嵌入与文档嵌入进行比较，以识别包含答案的最相关文档片段并记录它们的相似度分数。给定用户指定的错误率（α），然后分析这些相似度分数以确定相似度分数截止阈值。在推理过程中，检索所有超过此阈值的片段以向LLM提供上下文，确保以（1-α）的置信水平捕获真实答案。我们提供了一个Python包，使用户能够仅使用LLMs且无需人工干预实现我们工作中提出的整个工作流程。</p>
<h4 id="_164">一句话总结：</h4>
<p>本研究提出了一种基于符合预测的框架，用于量化RAG框架中的检索不确定性，以提高LLM生成响应的可靠性。</p>
<hr />
<h2 id="towards-standards-compliant-assistive-technology-product-specifications-via-llms"><a href="http://arxiv.org/abs/2404.03122v1">Towards Standards-Compliant Assistive Technology Product Specifications via LLMs</a></h2>
<p>发布时间：2024-04-04</p>
<p>作者：Chetan Arora, John Grundy, Louise Puli, Natasha Layton</p>
<h4 id="_165">中文摘要：</h4>
<p>在辅助技术（AT）这一快速发展的领域，确保产品符合国家和国际标准对于用户的安全、有效性和可及性至关重要。在本篇愿景论文中，我们介绍了CompliAT，这是一个创新的框架，旨在通过创新地使用大型语言模型（LLMs）来简化辅助技术产品规范与这些标准的合规过程。CompliAT解决三个关键任务：检查术语一致性、根据标准对产品进行分类以及追踪关键产品规范到标准要求。我们应对术语一致性的挑战，以确保产品规范中使用的语言与相关标准相符，减少误解和非合规风险。我们提出了一种新颖的产品分类方法，利用检索增强生成模型，即使在训练数据稀疏的情况下，也能准确地将辅助技术产品分类到符合国际标准的类别中。最后，CompliAT从关键产品规范到标准要求实现了可追溯性和合规性机制，确保辅助技术产品的所有方面都经过彻底的审查。通过半自动化这些流程，CompliAT旨在显著减少辅助技术产品标准合规所需的时间和努力，并维护质量和安全标准。我们概述了CompliAT的计划实施和评估方案。</p>
<h4 id="_166">一句话总结：</h4>
<p>CompliAT通过利用大型语言模型，创新性地简化了辅助技术产品规范的合规过程，旨在提高产品安全性和有效性。</p>
<hr />
<h2 id="retrieving-examples-from-memory-for-retrieval-augmented-neural-machine-translation-a-systematic-comparison"><a href="http://arxiv.org/abs/2404.02835v1">Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison</a></h2>
<p>发布时间：2024-04-03</p>
<p>作者：Maxime Bouthors, Josep Crego, Francois Yvon</p>
<h4 id="_167">中文摘要：</h4>
<p>本文研究了检索增强神经机器翻译（RAMT）架构中，从记忆中检索示例以指导生成过程的效果。虽然大多数研究在这一趋势中探索了利用检索示例的新方法，但上游检索步骤却很少被探索。在本文中，我们研究了不同检索方法对几种翻译架构的影响，以更好地理解这两个过程之间的相互作用。我们在多领域设置中针对两种语言对进行了实验，并考虑了基于标准自回归模型、基于编辑的模型以及具有上下文学习的的大型语言模型等几种下游架构。我们的实验表明，检索技术的选择会影响翻译分数，且在不同架构之间存在差异。我们还讨论了增加示例数量和多样性的影响，这些影响总体上是积极的。</p>
<h4 id="_168">一句话总结：</h4>
<p>本文通过实验研究了不同检索方法对神经机器翻译性能的影响，并发现检索技术的选择对翻译质量有显著影响。</p>
<hr />
<h2 id="utebc-nlp-at-semeval-2024-task-9-can-llms-be-lateral-thinkers"><a href="http://arxiv.org/abs/2404.02474v1">uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?</a></h2>
<p>发布时间：2024-04-03</p>
<p>作者：Pouya Sadeghi, Amirhossein Abaskohi, Yadollah Yaghoobzadeh</p>
<h4 id="_169">中文摘要：</h4>
<p>受到人类认知的启发，江等人（2023c）创建了一个用于评估大型语言模型（LLMs）横向思维（跳出思维定式）能力的基准。在此基础上，我们研究了不同的提示方法如何增强LLMs在此任务上的表现，以揭示其内在的跳出思维定式的能力。通过参加SemEval-2024任务9的句子拼图子任务，我们探索了提示工程方法：思维链（CoT）和直接提示，通过信息性描述进行增强，并使用检索增强生成（RAG）管道进行上下文提示。我们的实验涉及包括GPT-3.5、GPT-4和Zephyr-7B-beta在内的三个LLMs。我们使用GPT-4生成了一组谜题和选项之间的思维路径数据集，并由人类验证其质量。研究发现，压缩的信息性提示可以提升性能。动态的上下文学习显著提升了模型性能。此外，在数据集上微调Zephyr可以提升其他常识数据集上的性能，强调了创新思维的价值。</p>
<h4 id="_170">一句话总结：</h4>
<p>本研究通过探索不同的提示方法，揭示了大型语言模型在跳出思维定式方面的潜力，并证明了信息性提示和动态上下文学习对模型性能的显著提升作用。</p>
<hr />
<h2 id="concept-guided-llm-agents-for-human-ai-safety-codesign"><a href="http://arxiv.org/abs/2404.15317v1">Concept-Guided LLM Agents for Human-AI Safety Codesign</a></h2>
<p>发布时间：2024-04-03</p>
<p>作者：Florian Geissler, Karsten Roscher, Mario Trapp</p>
<h4 id="_171">中文摘要：</h4>
<p>生成式人工智能在软件工程领域，包括安全工程领域，正变得越来越重要，其使用确保软件不会对人类造成伤害。这也对生成式人工智能提出了高质量的要求。因此，仅仅简单使用大型语言模型（LLMs）是无法满足这些质量需求的。开发更高级和复杂的策略，以有效解决软件系统的复杂性和安全问题是至关重要的。最终，人类必须理解和承担生成式人工智能提供的建议的责任，以确保系统安全。为此，我们提出了一种高效、混合的策略，以利用LLMs进行安全分析和人机协同设计。具体来说，我们开发了一个定制的LLM代理，该代理使用提示工程、启发式推理和检索增强生成等元素来解决与预定义安全概念相关的任务，并与系统模型图进行交互。推理过程由一系列微决策的级联引导，有助于保持结构化信息。我们进一步提出了一种图语言化方法，它作为系统模型的中间表示，以促进LLM与图之间的交互。与安全分析相关的提示和响应的选定对例展示了我们用于简化自动驾驶系统用例的方法。</p>
<h4 id="_172">一句话总结：</h4>
<p>本文提出了一种结合LLMs和人类专家知识的混合策略，以实现软件系统的安全分析和人机协同设计。</p>
<hr />
<h2 id="symbolic-prompt-program-search-a-structure-aware-approach-to-efficient-compile-time-prompt-optimization"><a href="http://arxiv.org/abs/2404.02319v2">Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization</a></h2>
<p>发布时间：2024-04-02</p>
<p>作者：Tobias Schnabel, Jennifer Neville</p>
<h4 id="_173">中文摘要：</h4>
<p>在许多现代大型语言模型（LLM）应用中，如检索增强生成（RAG），提示（prompts）已经变成了程序本身。在这些场景中，提示程序会根据不同的用户查询或数据实例被反复调用。一个重大的实际挑战是优化这样的提示程序。最近的研究大多集中在简单的提示程序上，或者假设提示程序的一般结构是固定的。我们引入了SAMMO，这是一个用于执行符号级提示程序搜索的框架，以实现提示程序的编译时优化。SAMMO在符号级别上表示提示程序，这使得在优化过程中可以搜索到丰富的转换集。我们展示了SAMMO如何泛化先前的方法，并提高了多个不同LLM上复杂提示程序在（1）指令调整、（2）RAG管道调整和（3）提示压缩方面的性能。我们将在https://github.com/microsoft/sammo上提供所有代码的开源版本。</p>
<h4 id="_174">一句话总结：</h4>
<p>SAMMO是一种用于优化LLM提示程序的框架，通过符号级搜索和转换，提高了复杂提示程序在不同任务上的性能。</p>
<hr />
<h2 id="clapnq-cohesive-long-form-answers-from-passages-in-natural-questions-for-rag-systems"><a href="http://arxiv.org/abs/2404.02103v1">CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems</a></h2>
<p>发布时间：2024-04-02</p>
<p>作者：Sara Rosenthal, Avirup Sil, Radu Florian, Salim Roukos</p>
<h4 id="_175">中文摘要：</h4>
<p>检索增强生成（Retrieval Augmented Generation，RAG）已成为大型语言模型的一个流行应用。成功的RAG系统应提供准确且无幻觉的答案，这些答案基于段落中的具体内容。虽然构建完整的RAG流程需要大量工作，但能够评估性能同样重要。我们提出了ClapNQ，这是一个用于完整RAG流程的基准长格式问答数据集。ClapNQ包括来自自然问题（Natural Questions，NQ）的具有基于段落的具体答案的长答案和一个用于执行检索、生成或完整RAG流程的语料库。ClapNQ的答案简洁，比完整段落小3倍，且具有连贯性，包含段落中不连续的多部分内容。RAG模型必须适应这些特性才能在ClapNQ上取得成功。我们展示了ClapNQ的基线实验和分析，突出了在基于段落的RAG中仍有很大改进空间的地方。CLAPNQ可在https://github.com/primeqa/clapnq公开获取。</p>
<h4 id="_176">一句话总结：</h4>
<p>ClapNQ是一个用于评估和改进基于段落的检索增强生成（RAG）模型性能的基准长格式问答数据集。</p>
<hr />
<h2 id="improving-retrieval-augmented-open-domain-question-answering-with-vectorized-contexts"><a href="http://arxiv.org/abs/2404.02022v2">Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts</a></h2>
<p>发布时间：2024-04-02</p>
<p>作者：Zhuo Chen, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Kewei Tu</p>
<h4 id="_177">中文摘要：</h4>
<p>在大型语言模型的时代，应用检索增强生成等技术可以更好地解决开放域问答问题。由于模型大小和计算资源等限制，上下文长度通常受限，这使得在回答开放域问题时覆盖过长的上下文变得具有挑战性。本文提出了一种通用且便捷的方法，用于在开放域问答任务中覆盖更长的上下文。该方法利用一个小的编码语言模型，该模型能够有效地编码上下文，并且编码过程中应用了与原始输入的交叉注意力。使用我们的方法，原始语言模型可以在保持计算需求接近基线的情况下覆盖数倍于基线的上下文长度。我们的实验表明，经过微调后，在两个保留数据集、四个保留数据集以及两个上下文学习设置中均取得了性能提升。</p>
<h4 id="_178">一句话总结：</h4>
<p>本文提出了一种基于小编码语言模型的方法，有效扩展了开放域问答任务中上下文的覆盖范围，同时保持了较低的计算需求。</p>
<hr />
<h2 id="towards-better-generalization-in-open-domain-question-answering-by-mitigating-context-memorization"><a href="http://arxiv.org/abs/2404.01652v1">Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization</a></h2>
<p>发布时间：2024-04-02</p>
<p>作者：Zixuan Zhang, Revanth Gangi Reddy, Kevin Small, Tong Zhang, Heng Ji</p>
<h4 id="_179">中文摘要：</h4>
<p>开放域问答（Open-domain Question Answering，OpenQA）旨在使用外部大规模知识库来回答事实性问题。然而，现实世界的知识并非静态，它持续更新和演变。这种知识的动态特性对模型提出了重大挑战，因为训练好的模型需要不断适应最新信息以确保答案的准确性。此外，OpenQA模型能否很好地迁移到完全不同的知识领域仍然不明确。在本文中，我们研究了检索增强问答模型在两种特定场景下的泛化性能：1）适应同一知识库的更新版本；2）切换到完全不同的知识领域。我们发现，OpenQA模型的泛化挑战源于读者对从外部语料库中记忆知识的过度依赖，这阻碍了模型泛化到新的知识语料库。我们引入了语料库不变调整（Corpus-Invariant Tuning，CIT），这是一种简单但有效的训练策略，通过控制训练过程中检索上下文的概率来减轻知识的过度记忆。在多个OpenQA基准上的大量实验结果表明，CIT在保持模型在其原始语料库和领域性能的同时，实现了显著更好的泛化能力。</p>
<h4 id="_180">一句话总结：</h4>
<p>本文提出了一种名为CIT的训练策略，通过控制检索上下文的概率来减轻OpenQA模型的知识过度记忆，从而显著提高了模型在不同知识领域的泛化能力。</p>
<hr />
<h2 id="aragog-advanced-rag-output-grading"><a href="http://arxiv.org/abs/2404.01037v1">ARAGOG: Advanced RAG Output Grading</a></h2>
<p>发布时间：2024-04-01</p>
<p>作者：Matouš Eibich, Shivay Nagpal, Alexander Fred-Ojala</p>
<h4 id="_181">中文摘要：</h4>
<p>检索增强生成（Retrieval-Augmented Generation，RAG）对于将外部知识整合到大型语言模型（Large Language Model，LLM）输出中至关重要。尽管关于RAG的文献正在增长，但它主要关注对现有最先进（State-of-the-Art，SoTA）技术及其前身进行的系统综述和比较，而在广泛的实验比较方面存在差距。本研究开始填补这一空白，通过评估各种RAG方法对检索精度和答案相似性的影响。我们发现，假设文档嵌入（Hypothetical Document Embedding，HyDE）和LLM重排序显著提高了检索精度。然而，最大边际相关度（Maximal Marginal Relevance，MMR）和Cohere重排序并没有在基线朴素RAG系统之上显示出显著优势，而多查询方法表现不佳。句子窗口检索在检索精度方面表现出最为有效，尽管其在答案相似性上的表现不稳定。该研究证实了文档摘要索引作为一种有能力的检索方法的可能性。所有与这项研究相关的资源都可通过我们的GitHub仓库ARAGOG（https://github.com/predlico/ARAGOG）公开访问，以供进一步研究。我们欢迎社区进一步探索RAG系统。</p>
<h4 id="_182">一句话总结：</h4>
<p>本研究通过实验比较，评估了不同RAG方法对检索精度和答案相似性的影响，并探讨了文档摘要索引在检索中的潜力。</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
    
  </body>
</html>