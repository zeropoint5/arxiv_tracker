
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../03/">
      
      
        <link rel="next" href="../01/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.31">
    
    
      
        <title>2024-02(87) - ArXiv Tracker</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="ArXiv Tracker" class="md-header__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ArXiv Tracker
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2024-02(87)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Emission%20Trading%20System/2024/09/" class="md-tabs__link">
          
  
  Emission Trading System

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../09/" class="md-tabs__link">
          
  
  Retrieval Augmented Generation

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="ArXiv Tracker" class="md-nav__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ArXiv Tracker
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Emission Trading System
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Emission Trading System
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2016/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2011/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2011-10(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Retrieval Augmented Generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Retrieval Augmented Generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-09(17)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-08(115)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(134)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(161)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-05(126)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-04(86)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-03(70)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2024-02(87)
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(50)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(33)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-11(38)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(43)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(20)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-08(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-07(11)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(33)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-04(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-02(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(15)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-09(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-07(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-05(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-04(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-12(9)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-09(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-08(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-06(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-06(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-04(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-12(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2015/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2012/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2012-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2009/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2009-03(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="retrieval-augmented-generation-202402">Retrieval Augmented Generation - 2024年02月</h1>
<h2 id="eyegpt-ophthalmic-assistant-with-large-language-models"><a href="http://arxiv.org/abs/2403.00840v1">EyeGPT: Ophthalmic Assistant with Large Language Models</a></h2>
<p>发布时间：2024-02-29</p>
<p>作者：Xiaolan Chen, Ziwei Zhao, Weiyi Zhang, Pusheng Xu, Le Gao, Mingpu Xu, Yue Wu, Yinwen Li, Danli Shi, Mingguang He</p>
<h4 id="_1">中文摘要：</h4>
<p>人工智能（AI）在医疗咨询领域因其能够改善临床工作流程和增强医疗沟通的潜力而受到广泛关注。然而，由于医疗信息的复杂性，仅用通用世界知识训练的大语言模型（LLM）可能不具备在专家级别处理与医学相关任务的能力。在此，我们介绍了EyeGPT，这是一种专门为眼科设计的LLM，采用了包括角色扮演、微调和检索增强生成在内的三种优化策略。特别是，我们提出了一种综合评估框架，该框架涵盖了多样化的数据集，包括眼科的各个亚专科、不同用户和多样化的查询意图。此外，我们还考虑了多个评估指标，包括准确性、可理解性、可靠性、同理心和幻觉比例。通过评估不同EyeGPT变体的性能，我们确定了最有效的一种，其可理解性、可靠性和同理心水平与人类眼科医生相当（所有P值均大于0.05）。总体而言，我们的研究为未来的研究提供了宝贵的见解，促进了眼科领域不同策略开发专门LLM的全面比较和评估。潜在的好处包括提升眼科护理的患者体验和优化眼科医生的服务。</p>
<h4 id="_2">一句话总结：</h4>
<p>本研究提出了一种专门针对眼科的LLM（EyeGPT），通过多种优化策略和综合评估框架，实现了与人类眼科医生相当的可理解性、可靠性和同理心水平。</p>
<hr />
<h2 id="retrieval-augmented-generation-for-ai-generated-content-a-survey"><a href="http://arxiv.org/abs/2402.19473v6">Retrieval-Augmented Generation for AI-Generated Content: A Survey</a></h2>
<p>发布时间：2024-02-29</p>
<p>作者：Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui</p>
<h4 id="_3">中文摘要：</h4>
<p>近年来，模型算法的进步、基础模型的增长以及高质量数据集的获取推动了人工智能生成内容（AIGC）的演变。尽管AIGC取得了显著的成功，但仍面临诸如更新知识、处理长尾数据、减轻数据泄露以及管理高训练和推理成本等挑战。检索增强生成（RAG）作为一种应对这些挑战的新范式，最近得到了广泛关注。特别是，RAG引入了信息检索过程，通过从可用的数据存储中检索相关对象来增强生成过程，从而提高了准确性和鲁棒性。在本文中，我们全面回顾了将RAG技术集成到AIGC场景中的现有努力。我们首先根据检索器如何增强生成器对RAG基础进行了分类，提炼了各种检索器和生成器的增强方法的基本抽象。这种统一的视角涵盖了所有RAG场景，阐明了有助于未来潜在进步的进展和关键技术。我们还总结了RAG的附加增强方法，促进了RAG系统的有效工程化和实施。然后从另一个角度，我们对RAG在不同模态和任务中的实际应用进行了调查，为研究人员和从业者提供了有价值的参考。此外，我们介绍了RAG的基准测试，讨论了当前RAG系统的局限性，并提出了未来研究的潜在方向。Github: https://github.com/PKU-DAIR/RAG-Survey。</p>
<h4 id="_4">一句话总结：</h4>
<p>本文全面分析了检索增强生成（RAG）在人工智能生成内容（AIGC）中的应用，探讨了其技术基础、增强方法、实际应用和未来研究方向。</p>
<hr />
<h2 id="crafting-knowledge-exploring-the-creative-mechanisms-of-chat-based-search-engines"><a href="http://arxiv.org/abs/2402.19421v1">Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines</a></h2>
<p>发布时间：2024-02-29</p>
<p>作者：Lijia Ma, Xingchen Xu, Yong Tan</p>
<h4 id="_5">中文摘要：</h4>
<p>在数字信息传播领域，搜索引擎作为连接信息寻求者与提供者的关键渠道。基于大型语言模型（LLMs）和检索增强生成（RAG）的聊天式搜索引擎的出现，如Bing Chat，标志着搜索生态系统的进化飞跃。它们在解释网络信息和以类似人类的理解和创造力构建响应方面展现出元认知能力。然而，LLMs的复杂本质使得其“认知”过程不透明，甚至对其设计者的理解也构成挑战。本研究旨在剖析由LLM驱动的聊天式搜索引擎（特别是Bing Chat）选择其响应信息源机制的细节。为此，通过与新Bing的互动，收集了一个庞大的数据集，记录了它引用的网站以及传统搜索引擎列出的网站。采用自然语言处理（NLP）技术，研究揭示了Bing Chat对内容的选择偏好，不仅内容可读且结构正式，而且具有较低的困惑度，表明其对由底层LLM可预测的文本有独特的倾向。进一步丰富我们的分析，我们通过与基于GPT-4的知识检索API的互动获取了额外的数据集，揭示了RAG API与Bing Chat之间一致的文本偏好。这一共识表明，这些文本偏好本质上源于底层语言模型，而不是由Bing Chat的开发者明确构建。此外，我们的研究还记录了RAG技术引用的网站与传统搜索引擎排名最高的网站之间的相似性更高。</p>
<h4 id="_6">一句话总结：</h4>
<p>本研究揭示了Bing Chat等聊天式搜索引擎在选择信息源时，其偏好源于底层语言模型，而非开发者明确构建。</p>
<hr />
<h2 id="grounding-language-models-for-visual-entity-recognition"><a href="http://arxiv.org/abs/2402.18695v2">Grounding Language Models for Visual Entity Recognition</a></h2>
<p>发布时间：2024-02-28</p>
<p>作者：Zilin Xiao, Ming Gong, Paola Cascante-Bonilla, Xingyao Zhang, Jie Wu, Vicente Ordonez</p>
<h4 id="_7">中文摘要：</h4>
<p>我们提出了AutoVER，这是一种用于视觉实体识别的自回归模型。该模型通过采用检索增强的约束生成，扩展了自回归的多模态大型语言模型。它减轻了在域外实体上的低性能，同时在需要视觉情境推理的查询中表现出色。我们的方法通过在序列到序列目标的同时并行对比训练硬负对，学习在广阔的标签空间内区分相似实体。在推理过程中，一组检索到的候选答案明确地指导语言生成，通过移除无效的解码路径。该方法在最近提出的Oven-Wiki基准测试的不同数据集分割中实现了显著的改进。在“实体看到”分割上的准确率从32.7%提升到61.5%。它还在未见过和查询分割上以大幅的两位数差距展示了优越的性能。</p>
<h4 id="_8">一句话总结：</h4>
<p>AutoVER通过检索增强的约束生成，显著提升了视觉实体识别的准确率，特别是在需要视觉情境推理的查询中。</p>
<hr />
<h2 id="approaching-human-level-forecasting-with-language-models"><a href="http://arxiv.org/abs/2402.18563v1">Approaching Human-Level Forecasting with Language Models</a></h2>
<p>发布时间：2024-02-28</p>
<p>作者：Danny Halawi, Fred Zhang, Chen Yueh-Han, Jacob Steinhardt</p>
<h4 id="_9">中文摘要：</h4>
<p>预测未来事件对于政策和决策至关重要。在这项工作中，我们研究了语言模型（LMs）是否能够在与竞争性人类预测者相同的水平上进行预测。为了实现这一目标，我们开发了一个检索增强型LM系统，该系统旨在自动搜索相关信息、生成预测并汇总预测。为了便于我们的研究，我们收集了来自竞争性预测平台的大量问题数据集。在测试集发布后，该测试集超出了我们LM的知识截止点，我们评估了我们的系统在端到端性能上与人类预测汇总的对比。平均而言，该系统接近竞争性预测者的群体汇总，在某些情况下甚至超过了它。我们的工作表明，使用LM进行未来预测可以提供大规模的准确预测，并有助于为机构决策提供信息。</p>
<h4 id="_10">一句话总结：</h4>
<p>本研究表明，语言模型在预测未来事件方面可以接近甚至超越人类预测者的水平，为大规模准确预测和机构决策提供支持。</p>
<hr />
<h2 id="rnns-are-not-transformers-yet-the-key-bottleneck-on-in-context-retrieval"><a href="http://arxiv.org/abs/2402.18510v3">RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval</a></h2>
<p>发布时间：2024-02-28</p>
<p>作者：Kaiyue Wen, Xingyu Dang, Kaifeng Lyu</p>
<h4 id="_11">中文摘要：</h4>
<p>本文研究了循环神经网络（RNNs）和Transformer在解决算法问题时的表示能力差距。我们关注于理解RNNs，以其在处理长序列时的内存效率而闻名，是否能够匹配Transformer的性能，尤其是在增强思维链（CoT）提示的情况下。我们的理论分析表明，CoT可以提高RNNs的性能，但不足以缩小与Transformer的差距。一个关键瓶颈在于RNNs无法完美地从上下文中检索信息，即使有CoT：对于需要这种能力的一些任务，例如联想回忆和确定一个图是否为树，我们证明RNNs的表达能力不足以解决这些任务，而Transformer可以轻松解决。相反，我们证明采用增强RNNs上下文检索能力的技巧，包括检索增强生成（RAG）和添加一个Transformer层，可以将RNNs提升到能够使用CoT解决所有多项式时间可解问题，从而缩小与Transformer的表示差距。</p>
<h4 id="_12">一句话总结：</h4>
<p>本文通过理论分析和实验验证，揭示了RNNs与Transformer在解决算法问题时的表示能力差距，并提出了一种通过增强RNNs上下文检索能力的方法来缩小这一差距。</p>
<hr />
<h2 id="cutting-off-the-head-ends-the-conflict-a-mechanism-for-interpreting-and-mitigating-knowledge-conflicts-in-language-models"><a href="http://arxiv.org/abs/2402.18154v1">Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models</a></h2>
<p>发布时间：2024-02-28</p>
<p>作者：Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, Jun Zhao</p>
<h4 id="_13">中文摘要：</h4>
<p>最近，检索增强和工具增强通过提供外部上下文，展示了显著的能力来扩展语言模型（LMs）的内部记忆边界。然而，内部记忆和外部上下文不可避免地会发生冲突，导致LMs内部的知识冲突。在本文中，我们旨在通过信息流的视角来解释知识冲突的机制，并通过在关键点进行精确干预来缓解冲突。我们发现，在后期层中存在一些具有相反效果的注意力头，其中记忆头可以从内部记忆中回忆知识，而上下文头可以从外部上下文中检索知识。此外，我们揭示在LMs中知识冲突出现的临界点是记忆头和上下文头整合不一致信息流的地方。受这些洞察的启发，我们提出了一种名为Pruning Head via PatH PatcHing（PH3）的新方法，该方法可以通过剪枝冲突的注意力头来有效地缓解知识冲突，而无需更新模型参数。PH3可以灵活地控制八个LMs使用内部记忆（提升44.0%）或外部上下文（提升38.5%）。此外，PH3还可以提高LMs在开放域问答任务上的性能。我们还进行了广泛的实验来展示我们方法在跨模型、跨关系和跨格式上的泛化能力。</p>
<h4 id="_14">一句话总结：</h4>
<p>本文提出了一种名为PH3的新方法，通过剪枝冲突的注意力头来缓解语言模型中的知识冲突，从而提高模型在内部记忆和外部上下文使用上的灵活性和性能。</p>
<hr />
<h2 id="unsupervised-information-refinement-training-of-large-language-models-for-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2402.18150v2">Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-02-28</p>
<p>作者：Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, Jie Zhou</p>
<h4 id="_15">中文摘要：</h4>
<p>检索增强生成（RAG）通过结合检索到的额外信息来增强大型语言模型（LLMs）。然而，研究表明，LLMs在有效利用检索信息方面仍然面临挑战，即使忽略它们或被它们误导。关键原因在于LLMs的训练并没有明确教会LLMs如何利用不同质量的输入检索文本。在本文中，我们提出了一种新颖的视角，将LLMs在RAG中的作用视为“信息精炼器”，这意味着无论检索文本的正确性、完整性或有用性如何，LLMs都可以持续地将检索文本中的知识和模型参数整合起来，生成比检索文本更简洁、准确和完整的文本。为此，我们提出了一种名为InFO-RAG的信息精炼训练方法，以无监督的方式优化LLMs进行RAG。InFO-RAG成本低且适用于各种任务。在包括问答、槽填充、语言建模、对话和代码生成在内的多种任务中，对11个数据集的零样本预测的大量实验表明，InFO-RAG将LLaMA2的性能平均提高了9.39%的相对点。InFO-RAG在上下文学习和RAG的鲁棒性方面也显示出优势。</p>
<h4 id="_16">一句话总结：</h4>
<p>InFO-RAG通过将LLMs作为“信息精炼器”来优化，显著提升了LLMs在检索增强生成任务中的性能。</p>
<hr />
<h2 id="jmlr-joint-medical-llm-and-retrieval-training-for-enhancing-reasoning-and-professional-question-answering-capability"><a href="http://arxiv.org/abs/2402.17887v4">JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability</a></h2>
<p>发布时间：2024-02-27</p>
<p>作者：Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu</p>
<h4 id="_17">中文摘要：</h4>
<p>大型语言模型（LLMs）在医学知识获取和问答方面展现出惊人的潜力。然而，即使经过特定领域的预训练，LLMs仍然可能产生幻觉并给出事实错误的结果。先前，检索增强生成（RAG）在解决幻觉问题上取得了一定的成功。与之前RAG方法中检索模型独立于LLM训练不同，我们在微调阶段引入了JMLR（联合训练LLM和信息检索）。同步训练机制增强了JMLR检索临床指南和利用医学知识进行推理和回答问题的能力，并减少了计算资源的需求。我们在重要的医学问答应用上评估了JMLR。我们的实验结果表明，JMLR-13B（70.5%）在医学问答数据集上优于使用传统预训练和微调的Meditron-70B（68.9%）和Llama2-13B（67.7%）等先前最先进的开源模型。全面评估显示，JMLR-13B在提高推理质量和减少幻觉方面优于Claude3-Opus。此外，JMLR-13B（148 GPU小时）的训练速度也比Meditron-70B（42630 GPU小时）快得多。通过这项工作，我们为医疗保健提供了一种新的高效的知识增强方法，展示了检索与LLM训练相结合用于医学问答系统的潜力。</p>
<h4 id="_18">一句话总结：</h4>
<p>本研究提出了一种名为JMLR的联合训练模型，有效提升了医学问答系统的准确性和效率，为医疗保健领域提供了新的知识增强方法。</p>
<hr />
<h2 id="follow-my-instruction-and-spill-the-beans-scalable-data-extraction-from-retrieval-augmented-generation-systems"><a href="http://arxiv.org/abs/2402.17840v2">Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems</a></h2>
<p>发布时间：2024-02-27</p>
<p>作者：Zhenting Qi, Hanlin Zhang, Eric Xing, Sham Kakade, Himabindu Lakkaraju</p>
<h4 id="_19">中文摘要：</h4>
<p>检索增强生成（RAG）通过在测试时结合外部知识来改进预训练模型，以实现定制化适应。我们研究了检索上下文RAG语言模型（LMs）中的数据存储泄露风险。我们表明，攻击者可以利用LMs的指令遵循能力，通过提示注入轻松地从使用指令调整的LMs构建的RAG系统数据存储中提取文本数据。这种漏洞存在于包括Llama2、Mistral/Mixtral、Vicuna、SOLAR、WizardLM、Qwen1.5和Platypus2在内的广泛现代LMs中，且随着模型规模的扩大，可利用性加剧。将我们的研究扩展到生产级RAG模型GPTs，我们设计了一种攻击，在25个随机选择的最多2次查询的定制GPTs上实现了100%的成功率，并通过仅使用GPTs自身生成的100个查询提示，从77,000字的书中提取了41%的文本数据，从1,569,000字的语料库中提取了3%的文本数据。</p>
<h4 id="_20">一句话总结：</h4>
<p>该研究揭示了检索增强生成模型中存在数据泄露风险，攻击者可以通过指令注入轻松提取文本数据。</p>
<hr />
<h2 id="evaluating-very-long-term-conversational-memory-of-llm-agents"><a href="http://arxiv.org/abs/2402.17753v1">Evaluating Very Long-Term Conversational Memory of LLM Agents</a></h2>
<p>发布时间：2024-02-27</p>
<p>作者：Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang</p>
<h4 id="_21">中文摘要：</h4>
<p>现有的关于长期开放域对话的研究主要关注评估模型在不超过五轮对话的上下文中的响应。尽管长期上下文大型语言模型（LLMs）和检索增强生成（RAG）技术在长期对话中的有效性有所进步，但其在非常长期对话中的效果仍未得到探索。为了解决这一研究差距，我们引入了一种机器-人工管道，通过利用基于LLM的代理架构，并将对话基于角色和时序事件图来生成高质量的非常长期对话。此外，我们为每个代理配备了共享和反应图像的能力。生成的对话由人工标注员验证和编辑，以确保长期一致性和对事件图的扎根。使用此管道，我们收集了LoCoMo，一个包含非常长期对话的数据集，每个数据集包含平均300个回合和9K个token，超过35个回合。基于LoCoMo，我们提出一个全面的评估基准，以衡量模型中的长期记忆，包括问答、事件摘要和多模态对话生成任务。我们的实验结果表明，LLMs在理解长篇对话和对话中的长期时间和因果动态方面存在挑战。采用长上下文LLM或RAG等策略可以提供改进，但这些模型仍然在性能上与人类有显著差距。</p>
<h4 id="_22">一句话总结：</h4>
<p>本研究提出了一种机器-人工管道，通过利用LLMs和RAG技术，生成高质量的非常长期对话，并评估了LLMs在长期记忆和对话理解方面的挑战。</p>
<hr />
<h2 id="retrieval-is-accurate-generation"><a href="http://arxiv.org/abs/2402.17532v3">Retrieval is Accurate Generation</a></h2>
<p>发布时间：2024-02-27</p>
<p>作者：Bowen Cao, Deng Cai, Leyang Cui, Xuxin Cheng, Wei Bi, Yuexian Zou, Shuming Shi</p>
<h4 id="_23">中文摘要：</h4>
<p>标准语言模型通过从固定的、有限的、独立的词汇表中选择标记来生成文本。我们提出了一种新颖的方法，该方法从一组支持文档中选择上下文感知的短语。这种范式转变的一个重大挑战是确定训练或acles，因为文本字符串可以以多种方式分割，并且每个片段都可以从许多可能的文档中检索到。为了解决这个问题，我们建议使用语言启发式方法初始化训练oracles，更重要的是，通过迭代自我强化来启动oracles。广泛的实验表明，我们的模型不仅在各种知识密集型任务上优于标准语言模型，而且在开放式文本生成中也表现出改进的生成质量。例如，与标准语言模型相对应，我们的模型在OpenbookQA上的准确率从23.47%提高到36.27%，在开放式文本生成中将MAUVE分数从42.61%提高到81.58%。值得注意的是，我们的模型在几个检索增强基线中实现了最佳性能和最低延迟。总之，我们断言检索比生成更准确，并希望我们的工作将鼓励进一步研究这一新的范式转变。</p>
<h4 id="_24">一句话总结：</h4>
<p>本研究提出了一种基于上下文感知短语选择的新方法，显著提升了知识密集型任务和开放式文本生成的性能。</p>
<hr />
<h2 id="rear-a-relevance-aware-retrieval-augmented-framework-for-open-domain-question-answering"><a href="http://arxiv.org/abs/2402.17497v1">REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering</a></h2>
<p>发布时间：2024-02-27</p>
<p>作者：Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, Ji-Rong Wen</p>
<h4 id="_25">中文摘要：</h4>
<p>考虑到内部参数知识的有限性，检索增强生成（RAG）已被广泛用于扩展大型语言模型（LLMs）的知识范围。尽管在RAG研究上投入了大量努力，但在现有方法中，LLMs无法精确评估检索文档的相关性，因此可能导致外部知识（即检索文档）的误导性甚至错误使用。为了解决这一问题，在本文中，我们提出了REAR，一种针对开放域问答（QA）的相关性感知检索增强方法。作为关键动机，我们旨在增强LLMs对源相关性的自我意识，以便在RAG系统中自适应地利用外部知识。特别地，我们为基于LLM的RAG系统开发了一种新的架构，通过整合一个专门设计的排名头，该排名头可以精确评估检索文档的相关性。此外，我们提出了一种基于双粒度相关性融合和抗噪声训练的改进训练方法。通过结合架构和训练方面的改进，我们提出的REAR能够通过有效地感知检索文档的相关性，更好地利用外部知识。在四个开放域QA任务上的实验表明，REAR显著优于先前的一些竞争性RAG方法。我们的代码和数据可以在https://github.com/RUCAIBox/REAR上访问。</p>
<h4 id="_26">一句话总结：</h4>
<p>REAR通过增强大型语言模型对检索文档相关性的感知，有效提升了检索增强生成在开放域问答中的性能。</p>
<hr />
<h2 id="mathsensei-a-tool-augmented-large-language-model-for-mathematical-reasoning"><a href="http://arxiv.org/abs/2402.17231v3">MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning</a></h2>
<p>发布时间：2024-02-27</p>
<p>作者：Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni</p>
<h4 id="_27">中文摘要：</h4>
<p>工具增强的大型语言模型（TALMs）已被证明可以增强大型语言模型（LLMs）的技能集，从而在许多任务中提高了它们的推理能力。尽管TALMs在各个问答基准测试中得到了成功的应用，但它们在复杂数学推理基准测试中的有效性，以及工具在知识检索和数学方程求解方面的潜在互补效益，仍然是开放的研究问题。在本工作中，我们提出了MathSensei，这是一种用于数学推理的工具增强大型语言模型。我们通过在数学推理数据集上的评估，研究了工具（知识检索器[Bing Web Search]、程序生成器+执行器[Python]和符号方程求解器[Wolfram-Alpha API]）的互补效益。我们在MATH（一个用于评估不同数学学科数学推理的流行数据集）上进行了彻底的消融实验。我们还进行了涉及知名工具规划者的实验，以研究工具序列对模型性能的影响。MathSensei在MATH数据集上比gpt-3.5-turbo的Chain-of-Thought实现了13.5%的更高准确率。我们进一步观察到，TALMs对于简单的数学文字问题（在GSM-8K中）并不那么有效，并且随着复杂性和所需知识的增加（逐渐在AQuA、MMLU-Math和MATH中更高层次的复杂问题），其效益也会增加。代码和数据可在https://github.com/Debrup-61/MathSensei上获取。</p>
<h4 id="_28">一句话总结：</h4>
<p>MathSensei通过结合工具增强，显著提升了大型语言模型在数学推理任务上的表现。</p>
<hr />
<h2 id="fact-and-reflection-far-improves-confidence-calibration-of-large-language-models"><a href="http://arxiv.org/abs/2402.17124v1">Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models</a></h2>
<p>发布时间：2024-02-27</p>
<p>作者：Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, Tongshuang Wu, Jianshu Chen</p>
<h4 id="_29">中文摘要：</h4>
<p>为了使大型语言模型（LLM）值得信赖，其置信水平应与其实际性能良好校准。尽管现在普遍认为LLM的表现受到提示的影响很大，但针对LLM的提示置信度校准尚未得到充分研究。在本文中，我们探讨了不同的提示策略如何影响LLM的置信度校准，以及如何改进它。我们在问答场景中广泛实验了六种提示方法，并观察到，尽管这些方法有助于提高预期的LLM校准，但它们也会导致LLM在回答某些实例时过于自信。受人类认知的启发，我们提出了事实与反思（Fact-and-Reflection，简称FaR）提示，它通过两个步骤来提高LLM的校准。首先，FaR从LLM中诱发出与输入提示相关的已知“事实”。然后，它要求模型对这些事实进行“反思”以生成最终答案。实验表明，FaR提示实现了显著更好的校准；在我们的多用途问答任务中，它将预期校准误差降低了23.5%。值得注意的是，FaR提示甚至能够在不太自信的场景中诱发出口头表达担忧的能力，这有助于触发检索增强来解决这些更难实例。</p>
<h4 id="_30">一句话总结：</h4>
<p>本文提出了一种名为“事实与反思”的提示方法，有效提高了大型语言模型的置信度校准，降低了预期校准误差，并增强了模型在处理复杂实例时的检索能力。</p>
<hr />
<h2 id="gistembed-guided-in-sample-selection-of-training-negatives-for-text-embedding-fine-tuning"><a href="http://arxiv.org/abs/2402.16829v1">GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning</a></h2>
<p>发布时间：2024-02-26</p>
<p>作者：Aivin V. Solatorio</p>
<h4 id="_31">中文摘要：</h4>
<p>嵌入模型对于人工智能应用如语义搜索、个性化推荐以及LLMs的检索增强生成至关重要，这需要高质量的训练数据。然而，手动数据整理的有限可扩展性促使需要自动化方法来确保数据完整性。传统的无监督三元组挖掘自动化了训练数据生成，这对于嵌入模型训练至关重要，但无意中引入了偏差和噪声，从而降低了模型性能。针对这一问题，我们引入了GISTEmbed，这是一种新颖的策略，通过引导模型在对比训练中增强了批内负样本的选择。这种方法不同于依赖于随机采样和批次负样本的等效用假设，显著减少了数据质量问题带来的噪声，并提高了模型的微调。与大规模文本嵌入基准（MTEB）进行基准测试，GISTEmbed在各种模型大小上展示了持续的性能提升，并在某些类别中实现了最先进的成果。该框架通过利用强大但资源密集型的大模型的能力，为小型模型带来了显著的提升。GISTEmbed有可能彻底改变高效、小型模型的创建，使高级人工智能技术更加民主化。使这些技术更加易于获取和成本效益，特别是对于受资源限制的应用，显著扩大了最先进人工智能解决方案在各个领域的应用范围和可及性。</p>
<h4 id="_32">一句话总结：</h4>
<p>GISTEmbed通过改进负样本选择策略，显著提升了嵌入模型训练的质量和效率，推动了小型高效模型的发展，并促进了人工智能技术的普及。</p>
<hr />
<h2 id="long-context-language-modeling-with-parallel-context-encoding"><a href="http://arxiv.org/abs/2402.16617v2">Long-Context Language Modeling with Parallel Context Encoding</a></h2>
<p>发布时间：2024-02-26</p>
<p>作者：Howard Yen, Tianyu Gao, Danqi Chen</p>
<h4 id="_33">中文摘要：</h4>
<p>扩展大型语言模型（LLMs）以处理更长的输入对于广泛的应用至关重要。然而，转换器（transformers）的巨大计算成本和位置编码的有限泛化能力限制了其上下文窗口的大小。我们提出了一个名为“并行编码的上下文扩展”（Context Expansion with Parallel Encoding，简称CEPE）的框架，该框架可以应用于任何现有的仅解码器（decoder-only）LLMs，以扩展其上下文窗口。CEPE使用一个小型编码器分块处理长输入，使得冻结的解码器可以通过交叉注意力利用额外的上下文。CEPE高效、可泛化和灵活：使用8K令牌的文档进行训练，它将LLAMA-2的上下文窗口扩展到128K令牌，提供了10倍的吞吐量，同时只占用1/6的内存。CEPE在语言建模和上下文学习中表现出色。在检索增强应用中，CEPE也表现出色，而现有的长上下文模型在检索到的上下文中会退化。我们进一步介绍了一种CEPE变体，它仅使用未标记数据即可扩展指令调整模型的上下文窗口，并在LLAMA-2-CHAT上展示了其有效性，从而产生了一个能够利用非常长上下文的强大指令遵循模型。</p>
<h4 id="_34">一句话总结：</h4>
<p>CEPE是一种高效、可泛化和灵活的框架，可以扩展大型语言模型的上下文窗口，从而提高其处理长输入的能力。</p>
<hr />
<h2 id="retrieval-augmented-generation-systems-automatic-dataset-creation-evaluation-and-boolean-agent-setup"><a href="http://arxiv.org/abs/2403.00820v1">Retrieval Augmented Generation Systems: Automatic Dataset Creation, Evaluation and Boolean Agent Setup</a></h2>
<p>发布时间：2024-02-26</p>
<p>作者：Tristan Kenneweg, Philip Kenneweg, Barbara Hammer</p>
<h4 id="_35">中文摘要：</h4>
<p>检索增强生成（RAG）系统在增强大型语言模型（LLM）输出以包含特定领域和时间敏感数据方面获得了巨大的人气。然而，最近正发生从简单的RAG设置（每次用户输入都查询向量数据库以获取额外信息）向更复杂的RAG形式的转变。然而，目前不同的具体方法主要基于轶事证据进行竞争。在本文中，我们提出了一种严格的创建和评估工作流程，以定量比较不同的RAG策略。我们使用这种方法创建的数据集来开发和评估布尔代理RAG设置：一个LLM可以决定是否查询向量数据库的系统，从而在可以用内部知识回答的问题上节省token。我们将我们的代码和生成的数据集在线发布。</p>
<h4 id="_36">一句话总结：</h4>
<p>本文提出了一种严格的RAG策略评估方法，并开发了一种基于布尔代理的RAG系统，以优化LLM的输出。</p>
<hr />
<h2 id="retrievalqa-assessing-adaptive-retrieval-augmented-generation-for-short-form-open-domain-question-answering"><a href="http://arxiv.org/abs/2402.16457v2">RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering</a></h2>
<p>发布时间：2024-02-26</p>
<p>作者：Zihan Zhang, Meng Fang, Ling Chen</p>
<h4 id="_37">中文摘要：</h4>
<p>自适应检索增强生成（ARAG）旨在动态确定查询检索的必要性，而不是无差别地检索，以提高源信息的效率和相关性。然而，先前的研究在很大程度上忽略了ARAG方法的评估，导致其有效性研究不足。本研究提出一个基准，RetrievalQA，包含1,271个涵盖新世界和长尾知识的短形式问题。回答这些问题所需的知识在大型语言模型（LLMs）中不存在；因此，必须检索外部信息才能正确回答。这使得RetrievalQA成为一个适合评估现有ARAG方法的测试平台。我们观察到基于校准的方法高度依赖于阈值调整，而传统的提示方法不足以指导LLMs做出可靠的检索决策。基于我们的发现，我们提出了时间感知自适应检索（TA-ARE），这是一种简单而有效的方法，帮助LLMs评估检索的必要性，而无需校准或额外的训练。数据集和代码将在https://github.com/hyintell/RetrievalQA上提供。</p>
<h4 id="_38">一句话总结：</h4>
<p>本研究提出了一种名为TA-ARE的时间感知自适应检索方法，旨在提高LLMs在ARAG任务中的检索决策可靠性。</p>
<hr />
<h2 id="from-rags-to-riches-using-large-language-models-to-write-documents-for-clinical-trials"><a href="http://arxiv.org/abs/2402.16406v1">From RAGs to riches: Using large language models to write documents for clinical trials</a></h2>
<p>发布时间：2024-02-26</p>
<p>作者：Nigel Markey, Ilyass El-Mansouri, Gaetan Rensonnet, Casper van Langen, Christoph Meier</p>
<h4 id="_39">中文摘要：</h4>
<p>临床试验需要撰写大量文件，包括方案、知情同意书、临床研究报告等。大型语言模型（LLMs）具有快速生成这些文件初稿的潜力，但其输出质量仍存在担忧。本研究评估了LLMs在生成临床试验方案等文件部分的能力。我们发现，现成的LLMs能够提供合理的结果，尤其是在评估内容相关性和术语使用正确性方面。然而，仍存在不足：具体包括临床思维和逻辑，以及参考文献的适当使用。为了提高性能，我们使用了检索增强生成（RAG）技术，以向LLM提供准确、最新的信息。使用RAG后，LLM的写作质量显著提高，这对LLMs在临床试验相关写作中的实际可用性具有重要意义。</p>
<h4 id="_40">一句话总结：</h4>
<p>本研究评估了大型语言模型在生成临床试验方案方面的能力，并发现通过检索增强生成技术可以显著提高其写作质量。</p>
<hr />
<h2 id="a-fine-tuning-enhanced-rag-system-with-quantized-influence-measure-as-ai-judge"><a href="http://arxiv.org/abs/2402.17081v1">A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge</a></h2>
<p>发布时间：2024-02-26</p>
<p>作者：Keshav Rangan, Yiqiao Yin</p>
<h4 id="_41">中文摘要：</h4>
<p>本研究提出了一种对检索增强生成（RAG）系统的创新增强方法，通过无缝集成微调的大型语言模型（LLMs）和向量数据库，充分利用了结构化数据检索和高级LLMs提供的细微理解能力。我们的方法的核心是LoRA和QLoRA方法，它们在通过参数高效的微调和内存优化进行模型精炼方面处于前沿。我们研究的独特之处在于将用户反馈直接纳入训练过程，确保模型持续适应用户期望，从而提高其性能和适用性。此外，我们引入了一种量化影响度量（QIM）作为创新的“AI裁判”机制，以增强结果选择的精确性，进一步提高了系统的准确性。伴随着执行图和详细的QLoRA微调算法，我们的工作为在聊天机器人技术中实现这些进步提供了一个全面的框架。这项研究为特定用途的LLM优化提供了重要的见解，并为检索增强模型进一步发展的新方向发出了信号。通过广泛的实验和分析，我们的发现为聊天机器人和检索系统未来的进步奠定了坚实的基础，标志着在创建更复杂、精确和以用户为中心的对话人工智能系统方面迈出了重要一步。</p>
<h4 id="_42">一句话总结：</h4>
<p>本研究通过集成微调的大型语言模型和向量数据库，结合用户反馈和量化影响度量，显著提升了检索增强生成系统的性能和精确度。</p>
<hr />
<h2 id="cfret-dvqa-coarse-to-fine-retrieval-and-efficient-tuning-for-document-visual-question-answering"><a href="http://arxiv.org/abs/2403.00816v1">CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering</a></h2>
<p>发布时间：2024-02-26</p>
<p>作者：Jinxu Zhang, Yongqi Yu, Yu Zhang</p>
<h4 id="_43">中文摘要：</h4>
<p>文档视觉问答（Document Visual Question Answering，简称DVQA）是一项基于图像内容进行查询响应的任务。现有工作局限于在单页内定位信息，且不便于跨页问答交互。此外，对模型输入施加的令牌长度限制可能导致与答案相关的片段被截断。在本研究中，我们提出了一种简单但有效的名为CFRet-DVQA的方法，该方法专注于检索和高效调整以有效解决这一关键问题。为此，我们首先从文档中检索与当前问题相关的多个片段。随后，我们利用大型语言模型（Large Language Model，简称LLM）的高级推理能力，并通过指令调整进一步增强了其性能。这种方法能够生成与文档标签风格一致的答案。实验表明，我们的方法在各个领域的单页和多页文档中都实现了最先进或具有竞争力的结果。</p>
<h4 id="_44">一句话总结：</h4>
<p>本研究提出的CFRet-DVQA方法通过检索和高效调整，有效解决了文档视觉问答中跨页交互和信息截断的问题，实现了在单页和多页文档上的高性能。</p>
<hr />
<h2 id="citation-enhanced-generation-for-llm-based-chatbots"><a href="http://arxiv.org/abs/2402.16063v3">Citation-Enhanced Generation for LLM-based Chatbots</a></h2>
<p>发布时间：2024-02-25</p>
<p>作者：Weitao Li, Junkai Li, Weizhi Ma, Yang Liu</p>
<h4 id="_45">中文摘要：</h4>
<p>大型语言模型（LLMs）在多种场景下展现出强大的通用智能，包括它们与聊天机器人的集成。然而，基于LLMs的聊天机器人面临的一个关键挑战是它们可能在回复中产生幻觉内容，这显著限制了它们的适用性。为了减轻幻觉，已经做出了各种努力，例如检索增强生成和基于人类反馈的强化学习，但大多数方法都需要额外的训练和数据标注。在本文中，我们提出了一种新颖的后处理引文增强生成（CEG）方法，结合了检索论证。与之前专注于生成过程中防止幻觉的研究不同，我们的方法以事后处理的方式解决这个问题。它包含一个检索模块来搜索与生成内容相关的支持文档，并采用基于自然语言推理的引文生成模块。一旦生成内容中的陈述缺乏参考文献，我们的模型可以重新生成回复，直到所有陈述都得到引文的支撑。请注意，我们的方法是一个无需训练的即插即用插件，适用于各种LLMs。在多个与幻觉相关的数据集上的实验表明，我们的框架在三个基准测试中在幻觉检测和回复再生方面均优于最先进的方法。我们的代码和数据集将公开可用。</p>
<h4 id="_46">一句话总结：</h4>
<p>本文提出了一种基于检索论证的后处理引文增强生成方法，有效减轻了基于LLMs的聊天机器人中产生的幻觉内容。</p>
<hr />
<h2 id="evaluating-robustness-of-generative-search-engine-on-adversarial-factual-questions"><a href="http://arxiv.org/abs/2403.12077v1">Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions</a></h2>
<p>发布时间：2024-02-25</p>
<p>作者：Xuming Hu, Xiaochuan Li, Junzhe Chen, Yinghui Li, Yangning Li, Xiaoguang Li, Yasheng Wang, Qun Liu, Lijie Wen, Philip S. Yu, Zhijiang Guo</p>
<h4 id="_47">中文摘要：</h4>
<p>生成式搜索引擎有潜力改变人们在线搜索信息的方式，但由现有大型语言模型（LLMs）支持的生成式搜索引擎生成的响应可能并不总是准确的。尽管如此，检索增强生成加剧了安全担忧，因为对手可能通过微妙地操纵主张中最脆弱的部分来成功规避整个系统。为此，我们提出在现实且高风险的设置中评估生成式搜索引擎的鲁棒性，在这种设置中，对手只有黑盒系统访问权限，并试图欺骗模型返回错误的响应。通过全面的人类评估，我们展示了各种生成式搜索引擎，如Bing Chat、PerplexityAI和YouChat在多样化查询中的有效性，证明了对抗性事实问题在诱导错误响应方面的效果。此外，与没有检索的LLMs相比，检索增强生成对事实错误的敏感性更高。这些发现突显了这些系统的潜在安全风险，并强调了在部署前进行严格评估的必要性。</p>
<h4 id="_48">一句话总结：</h4>
<p>本研究揭示了生成式搜索引擎在现实场景中的鲁棒性问题，强调了其潜在安全风险，并呼吁在部署前进行严格评估。</p>
<hr />
<h2 id="query-augmentation-by-decoding-semantics-from-brain-signals"><a href="http://arxiv.org/abs/2402.15708v2">Query Augmentation by Decoding Semantics from Brain Signals</a></h2>
<p>发布时间：2024-02-24</p>
<p>作者：Ziyi Ye, Jingtao Zhan, Qingyao Ai, Yiqun Liu, Maarten de Rijke, Christina Lioma, Tuukka Ruotsalo</p>
<h4 id="_49">中文摘要：</h4>
<p>查询增强是一种用于改进语义不精确查询的关键技术。传统上，查询增强依赖于从最初检索到的、可能相关的文档中提取信息。如果最初检索到的文档质量较低，那么查询增强的有效性也会受到限制。我们提出了Brain-Aug，它通过结合从脑信号中解码的语义信息来增强查询。Brain-Aug通过使用包含脑信号信息的提示和以排名为导向的推理方法生成原始查询的延续。在fMRI（功能性磁共振成像）数据集上的实验结果表明，Brain-Aug生成的查询在语义上更加准确，从而提高了文档排名性能。这种由脑信号带来的改进对于模糊查询尤其显著。</p>
<h4 id="_50">一句话总结：</h4>
<p>Brain-Aug通过结合脑信号解码的语义信息，显著提高了查询的语义准确性，从而优化了文档排名性能。</p>
<hr />
<h2 id="the-good-and-the-bad-exploring-privacy-issues-in-retrieval-augmented-generation-rag"><a href="http://arxiv.org/abs/2402.16893v1">The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)</a></h2>
<p>发布时间：2024-02-23</p>
<p>作者：Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, Jiliang Tang</p>
<h4 id="_51">中文摘要：</h4>
<p>检索增强生成（RAG）是一种强大的技术，用于帮助语言模型利用专有和私有数据，其中数据隐私是一个关键问题。尽管大量研究已经证明了大型语言模型（LLMs）的隐私风险，但RAG技术可能会重塑LLM生成的固有行为，从而引发新的隐私问题，这些问题目前尚未得到充分探索。在这项工作中，我们通过新颖的攻击方法进行了广泛的实证研究，这些研究证明了RAG系统在泄露私有检索数据库方面的脆弱性。尽管RAG对检索数据带来了新的风险，但我们进一步揭示RAG可以减轻LLMs训练数据的泄露。总的来说，本文为检索增强LLMs的隐私保护提供了新的见解，这对LLMs和RAG系统构建者都有益。我们的代码可在https://github.com/phycholosogy/RAG-privacy上找到。</p>
<h4 id="_52">一句话总结：</h4>
<p>本文揭示了检索增强生成技术中存在的隐私风险，并提出了解决方案，以保护大型语言模型的训练数据隐私。</p>
<hr />
<h2 id="ese-espresso-sentence-embeddings"><a href="http://arxiv.org/abs/2402.14776v2">ESE: Espresso Sentence Embeddings</a></h2>
<p>发布时间：2024-02-22</p>
<p>作者：Xianming Li, Zongxi Li, Jing Li, Haoran Xie, Qing Li</p>
<h4 id="_53">中文摘要：</h4>
<p>高质量的句子嵌入在许多自然语言处理（NLP）任务中至关重要，例如语义文本相似度（STS）和检索增强生成（RAG）。然而，大多数现有方法利用全层语言模型的固定长度嵌入，这缺乏可扩展性，无法适应各种应用中可用的不同资源。针对这一差距，我们提出了一种新的句子嵌入模型 $\mathrm{Espresso}$ $\mathrm{Sentence}$ $\mathrm{Embeddings}$（ESE），它包含两个学习过程。首先，表达学习过程将更显著的表现形式编码到较低的层。其次，压缩学习过程使用主成分分析（PCA）将关键特征压缩到初始维度。这样，ESE可以通过前者过程扩展模型深度，通过后者过程扩展嵌入大小。在STS和RAG上的大量实验表明，ESE可以有效地以较少的模型深度和嵌入大小生成高质量的嵌入，从而提高嵌入推理效率。</p>
<h4 id="_54">一句话总结：</h4>
<p>ESE通过两个学习过程，实现了可扩展的句子嵌入，有效降低了模型深度和嵌入大小，提高了嵌入推理效率。</p>
<hr />
<h2 id="large-language-models-as-urban-residents-an-llm-agent-framework-for-personal-mobility-generation"><a href="http://arxiv.org/abs/2402.14744v2">Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation</a></h2>
<p>发布时间：2024-02-22</p>
<p>作者：Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Noboru Koshizuka, Chuan Xiao</p>
<h4 id="_55">中文摘要：</h4>
<p>本文介绍了一种新颖的方法，该方法将大型语言模型（LLMs）集成到代理框架中，以实现灵活有效的个人移动性生成。LLMs通过有效处理语义数据并提供在建模各种任务中的多功能性，克服了先前模型的局限性。我们的方法针对三个研究问题：将LLMs与真实世界的城市移动性数据对齐、开发可靠的活动生成策略以及探索LLMs在城市移动性中的应用。关键技术贡献是一个新的LLM代理框架，该框架考虑了个人活动模式和动机，包括一种自洽的方法来将LLMs与真实世界的活动数据对齐，以及一种用于可解释活动生成的检索增强策略。我们评估了我们的LLM代理框架，并将其与最先进的个人移动性生成方法进行了比较，证明了我们方法的有效性和其在城市移动性中的潜在应用。总体而言，这项研究标志着基于真实世界人类活动数据设计LLM代理框架进行活动生成的开创性工作，为城市移动性分析提供了一种有前景的工具。</p>
<h4 id="_56">一句话总结：</h4>
<p>本文提出了一种基于大型语言模型的个人移动性生成方法，通过整合真实世界活动数据，为城市移动性分析提供了一种创新工具。</p>
<hr />
<h2 id="ufo-a-unified-and-flexible-framework-for-evaluating-factuality-of-large-language-models"><a href="http://arxiv.org/abs/2402.14690v1">UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models</a></h2>
<p>发布时间：2024-02-22</p>
<p>作者：Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, Ji-rong Wen</p>
<h4 id="_57">中文摘要：</h4>
<p>大型语言模型（LLMs）可能生成与人类知识不一致的文本，导致事实不准确或（hallucination）（幻觉）。现有的LLMs事实性评估研究涉及使用LLM提取事实主张，并对其与预定义的事实来源进行验证。然而，这些评估指标是特定于任务的，并且不可扩展，不同任务中事实来源的可替代性研究不足。为了解决这些挑战，我们将四种可用的事实来源进行分类：人类编写的证据、参考文档、搜索引擎结果和LLM知识，以及包含六个代表性数据集的五个文本生成任务。然后，我们提出了基于LLM的统一且灵活的评估框架UFO，用于验证事实与即插即用的事实来源。我们基于此框架实现了五种评估场景。实验结果表明，对于大多数问答任务，人类编写的证据和参考文档至关重要，它们可以在检索增强的问答任务中相互替代。在新闻事实生成任务中，搜索引擎结果和LLM知识是必不可少的。我们的数据集和代码可在\url{https://github.com/WaldenRUC/UFO}获取。</p>
<h4 id="_58">一句话总结：</h4>
<p>本研究提出了一种基于LLM的统一评估框架UFO，用于评估LLMs生成文本的事实性，并通过实验验证了不同事实来源在各类文本生成任务中的重要性。</p>
<hr />
<h2 id="assessing-generalization-capability-of-text-ranking-models-in-polish"><a href="http://arxiv.org/abs/2402.14318v1">Assessing generalization capability of text ranking models in Polish</a></h2>
<p>发布时间：2024-02-22</p>
<p>作者：Sławomir Dadas, Małgorzata Grębowiec</p>
<h4 id="_59">中文摘要：</h4>
<p>检索增强生成（RAG）正成为将内部知识库与大型语言模型集成的一种越来越流行的技术。在一个典型的RAG流程中，使用三个模型，分别负责检索、重排序和生成阶段。在本文中，我们专注于波兰语的重排序问题，检查了重排序器的性能，并将它们的成果与现有的检索模型进行了比较。我们对我们现有的模型和通过我们训练的模型进行了全面的评估，使用了针对波兰语的41个不同信息检索任务的基准。我们的实验结果表明，大多数模型在跨领域泛化方面存在困难。然而，有效的优化方法和大型训练数据集的结合使得构建既紧凑又具有泛化能力的重排序器成为可能。我们最好的模型在波兰语重排序方面建立了新的最先进水平，其参数数量比现有模型多出30倍。</p>
<h4 id="_60">一句话总结：</h4>
<p>本文提出了一种针对波兰语的重排序方法，通过结合有效的优化方法和大型训练数据集，显著提升了重排序的性能，并建立了新的最先进水平。</p>
<hr />
<h2 id="assisting-in-writing-wikipedia-like-articles-from-scratch-with-large-language-models"><a href="http://arxiv.org/abs/2402.14207v2">Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</a></h2>
<p>发布时间：2024-02-22</p>
<p>作者：Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, Monica S. Lam</p>
<h4 id="_61">中文摘要：</h4>
<p>本文研究了如何将大型语言模型应用于从头开始撰写具有与维基百科页面相当广度和深度的有根据和组织性的长篇文章。这个问题在写作前的阶段提出了新的挑战，包括如何研究主题并在写作前准备大纲。我们提出了STORM，一个通过检索和多角度提问来综合主题大纲的写作系统。STORM通过以下方式模拟写作前的阶段：(1)在研究给定主题时发现不同的观点，(2)模拟不同观点的作者向基于可信互联网资源的主题专家提出问题的对话，(3)整理收集到的信息以创建大纲。为了评估，我们整理了FreshWiki，一个包含最新高质量维基百科文章的数据集，并制定了大纲评估标准以评估写作前的阶段。我们还从经验丰富的维基百科编辑那里收集反馈。与由大纲驱动的检索增强基线生成的文章相比，STORM生成的文章被认为更有组织（绝对增加25%）和覆盖面更广（增加10%）。专家反馈还有助于识别生成有根据的长文章的新挑战，例如来源偏差转移和无关事实的过度关联。</p>
<h4 id="_62">一句话总结：</h4>
<p>本文提出了一种名为STORM的写作系统，通过检索和多角度提问来综合主题大纲，从而提高长篇文章的广度和组织性。</p>
<hr />
<h2 id="metmap-metamorphic-testing-for-detecting-false-vector-matching-problems-in-llm-augmented-generation"><a href="http://arxiv.org/abs/2402.14480v1">MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation</a></h2>
<p>发布时间：2024-02-22</p>
<p>作者：Guanyu Wang, Yuekang Li, Yi Liu, Gelei Deng, Tianlin Li, Guosheng Xu, Yang Liu, Haoyu Wang, Kailong Wang</p>
<h4 id="_63">中文摘要：</h4>
<p>增强生成技术，如检索增强生成（RAG）和缓存增强生成（CAG），通过增强大型语言模型（LLM）的输出并引入外部知识和缓存信息，已经彻底改变了该领域。然而，这些增强技术的核心——向量数据库的集成，引入了关键挑战，特别是在确保准确向量匹配方面。在这些数据库中，错误的向量匹配会严重损害LLM输出的完整性和可靠性，导致错误信息或错误响应。尽管这些问题具有关键影响，但在有效检测和解决LLM增强生成中的错误向量匹配的方法上，仍存在显著的研究空白。本文提出了MeTMaP，这是一个用于识别LLM增强生成系统中错误向量匹配的变形测试框架。我们从六个NLP数据集中推导出八个变形关系（MRs），这些关系构成了我们方法的核心，基于语义相似文本应匹配而语义不相似文本不应匹配的理念。MeTMaP使用这些MRs创建用于测试的句子三元组，模拟现实世界中的LLM场景。我们对MeTMaP在203个向量匹配配置上的评估，涉及29个嵌入模型和7个距离度量，揭示了显著的不准确性。结果显示，与原始数据集相比，我们的测试中最大准确率仅为41.51%，强调了向量匹配方法中广泛存在的错误匹配问题，以及在LLM增强应用中有效检测和缓解的迫切需要。</p>
<h4 id="_64">一句话总结：</h4>
<p>本文提出了一种名为MeTMaP的变形测试框架，用于检测LLM增强生成系统中的错误向量匹配，揭示了向量匹配方法中广泛存在的错误匹配问题。</p>
<hr />
<h2 id="more-multi-modal-retrieval-augmented-generative-commonsense-reasoning"><a href="http://arxiv.org/abs/2402.13625v2">MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning</a></h2>
<p>发布时间：2024-02-21</p>
<p>作者：Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng</p>
<h4 id="_65">中文摘要：</h4>
<p>由于常识信息被记录的频率远低于其存在，通过文本生成预训练的语言模型难以学习到足够的常识知识。一些研究利用文本检索来增强模型的常识能力。与文本不同，图像天生能够捕捉常识信息，但很少有人努力有效地利用它们。在这项工作中，我们提出了一种新颖的多模态检索（MORE）增强框架，旨在利用文本和图像来增强语言模型的常识能力。在Common-Gen任务上的大量实验证明了基于单模态和多模态预训练模型的MORE的有效性。</p>
<h4 id="_66">一句话总结：</h4>
<p>本研究提出了一种利用文本和图像增强语言模型常识能力的新框架，并通过实验验证了其有效性。</p>
<hr />
<h2 id="activerag-revealing-the-treasures-of-knowledge-via-active-learning"><a href="http://arxiv.org/abs/2402.13547v1">ActiveRAG: Revealing the Treasures of Knowledge via Active Learning</a></h2>
<p>发布时间：2024-02-21</p>
<p>作者：Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu</p>
<h4 id="_67">中文摘要：</h4>
<p>检索增强生成（Retrieval Augmented Generation，RAG）为大型语言模型（Large Language Models，LLMs）引入了一种新的范式，有助于解决知识密集型任务。然而，当前的RAG模型将LLMs定位为被动的知识接收者，从而限制了它们学习和理解外部知识的能力。在本文中，我们提出了ActiveRAG，这是一种创新的RAG框架，它从被动的知识获取转变为主动学习机制。这种方法利用知识构建机制，通过将其与先前获取或记忆的知识相关联，来发展对外部知识的更深入理解。随后，它设计了认知枢纽机制（Cognitive Nexus mechanism），将思维链和知识构建的结果结合起来，从而校准LLMs的内生认知。我们的实验结果表明，ActiveRAG优于之前的RAG模型，在问答数据集上实现了5%的性能提升。所有数据和代码均可在https://github.com/OpenMatch/ActiveRAG上获取。</p>
<h4 id="_68">一句话总结：</h4>
<p>ActiveRAG通过引入主动学习机制，提高了大型语言模型在知识密集型任务中的问答性能。</p>
<hr />
<h2 id="arl2-aligning-retrievers-for-black-box-large-language-models-via-self-guided-adaptive-relevance-labeling"><a href="http://arxiv.org/abs/2402.13542v2">ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling</a></h2>
<p>发布时间：2024-02-21</p>
<p>作者：Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang</p>
<h4 id="_69">中文摘要：</h4>
<p>检索增强生成通过整合外部知识源的相关信息来增强大型语言模型（LLMs）。这使得LLMs能够适应特定领域并减轻知识密集型任务中的幻觉。然而，由于现有检索器与LLMs的训练过程分离以及LLMs的黑盒性质，它们往往与LLMs不匹配。为了解决这一挑战，我们提出了ARL2，这是一种利用LLMs作为标签器的检索学习技术。ARL2利用LLMs来标注和评分相关证据，从而从鲁棒的LLM监督中学习检索器。此外，ARL2采用自适应自训练策略来整理高质量和多样化的相关性数据，这可以有效降低标注成本。大量实验证明了ARL2的有效性，在NQ上比最先进的方法提高了5.4%的准确率，在MMLU上提高了4.6%。此外，ARL2展现出强大的迁移学习能力和零样本泛化能力。我们的代码将在\url{https://github.com/zhanglingxi-cs/ARL2}发布。</p>
<h4 id="_70">一句话总结：</h4>
<p>ARL2通过利用LLMs作为标签器，有效提升了检索器的性能，并降低了标注成本。</p>
<hr />
<h2 id="retrieval-augmented-data-augmentation-for-low-resource-domain-tasks"><a href="http://arxiv.org/abs/2402.13482v1">Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks</a></h2>
<p>发布时间：2024-02-21</p>
<p>作者：Minju Seo, Jinheon Baek, James Thorne, Sung Ju Hwang</p>
<h4 id="_71">中文摘要：</h4>
<p>尽管最近的语言模型在多种任务上取得了巨大成功，但在资源有限且可用训练数据有限的环境中，它们的性能会严重退化。许多现有工作通过从训练数据生成合成数据并在其上训练模型来解决这一问题，最近还使用了大型语言模型（LLMs）。然而，在资源有限的环境中，可用于数据增强的种子数据样本量非常小，这导致生成的样本次优且多样性不足。为了应对这一挑战，我们提出了一种新颖的方法，通过结合来自其他数据集的大量示例以及给定的训练数据来增强训练数据。具体来说，我们首先根据与给定种子数据的相似性，从其他数据集中检索相关实例，例如它们的输入-输出对或上下文，然后提示LLMs在原始和检索到的样本的上下文中生成新的样本。这种方法可以确保生成的数据不仅相关，而且比仅使用有限的种子数据所能实现的更加多样化。我们在多个数据集上验证了所提出的检索增强数据增强（RADA）框架，在训练和测试时数据增强的资源有限环境中，它在性能上优于现有的LLM驱动的数据增强基线。</p>
<h4 id="_72">一句话总结：</h4>
<p>提出了一种基于检索增强的数据增强方法，通过结合多个数据集的丰富示例来提高低资源环境下语言模型的性能。</p>
<hr />
<h2 id="benchmarking-retrieval-augmented-generation-for-medicine"><a href="http://arxiv.org/abs/2402.13178v2">Benchmarking Retrieval-Augmented Generation for Medicine</a></h2>
<p>发布时间：2024-02-20</p>
<p>作者：Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang</p>
<h4 id="_73">中文摘要：</h4>
<p>尽管大型语言模型（LLMs）在广泛的医学问答（QA）任务上取得了最先进的性能，但它们仍然面临着幻觉和过时知识等挑战。检索增强生成（RAG）是一种有前景的解决方案，并且已被广泛采用。然而，一个RAG系统可能包含多个灵活的组件，而对于各种医学目的的最佳RAG设置，目前缺乏最佳实践。为了系统地评估此类系统，我们提出了医学信息检索增强生成评估（MIRAGE），这是一个首创的基准，包括来自五个医学QA数据集的7,663个问题。使用MIRAGE，我们通过本工作中引入的MedRAG工具包，在41种不同语料库、检索器和骨干LLMs的组合上进行了超过1.8万亿个提示标记的大规模实验。总体而言，MedRAG将六个不同LLMs的准确率提高了高达18%，将GPT-3.5和Mixtral的性能提升到了GPT-4的水平。我们的结果表明，各种医学语料库和检索器的组合实现了最佳性能。此外，我们还发现了医学RAG中的对数线性缩放属性和“中间丢失”效应。我们相信，我们的全面评估可以作为实施医学RAG系统的实用指南。</p>
<h4 id="_74">一句话总结：</h4>
<p>MedRAG通过引入一个包含大量医学问答数据的基准，显著提高了LLMs在医学问答任务上的性能，并揭示了医学RAG中的关键特性。</p>
<hr />
<h2 id="formulareasoning-a-dataset-for-formula-based-numerical-reasoning"><a href="http://arxiv.org/abs/2402.12692v3">FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning</a></h2>
<p>发布时间：2024-02-20</p>
<p>作者：Xiao Li, Bolin Zhu, Sichen Liu, Yin Zhu, Yiwei Liu, Gong Cheng</p>
<h4 id="_75">中文摘要：</h4>
<p>在解决数值推理问题时，公式的应用是人类的一项基本能力。然而，现有的数值推理数据集很少明确指出推理步骤中使用的公式。为了填补这一空白，我们构建了一个基于公式的数值推理数据集，名为FormulaReasoning，其中包含5,420个基于推理的问题。我们利用它对从70亿到超过1000亿参数规模的LLMs进行了评估，并采用了零样本和少样本思维链方法。我们还进一步探索了使用检索增强的LLMs，这些LLMs与我们数据集相关联的外部公式数据库相结合。此外，我们还尝试了监督方法，将推理过程分为公式生成、参数提取和数值计算，并进行了数据增强。我们的实证发现强调了现有模型在应用于我们复杂的、公式驱动的FormulaReasoning数据集时具有显著的改进潜力。</p>
<h4 id="_76">一句话总结：</h4>
<p>本研究构建了FormulaReasoning数据集，评估了LLMs在数值推理任务中的表现，并探索了改进现有模型的方法。</p>
<hr />
<h2 id="finben-a-holistic-financial-benchmark-for-large-language-models"><a href="http://arxiv.org/abs/2402.12659v2">FinBen: A Holistic Financial Benchmark for Large Language Models</a></h2>
<p>发布时间：2024-02-20</p>
<p>作者：Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang Li, Yangyang Yu, Gang Hu, Jiajia Huang, Xiao-Yang Liu, Alejandro Lopez-Lira, Benyou Wang, Yanzhao Lai, Hao Wang, Min Peng, Sophia Ananiadou, Jimin Huang</p>
<h4 id="_77">中文摘要：</h4>
<p>本文介绍了FinBen，这是首个广泛的开源评估基准，涵盖了36个数据集，涉及24个金融任务，包括信息提取（IE）、文本分析、问答（QA）、文本生成、风险管理、预测和决策等七个关键方面。FinBen具有多项创新：更广泛的任务和数据集范围、首次对股票交易的评估、新颖的代理和检索增强生成（RAG）评估，以及三个新的开源评估数据集，分别针对文本摘要、问答和股票交易。我们对包括GPT-4、ChatGPT和最新Gemini在内的15个代表性LLMs进行了评估，发现LLMs在IE和文本分析方面表现出色，但在高级推理和复杂任务如文本生成和预测方面存在困难。GPT-4在IE和股票交易方面表现优异，而Gemini在文本生成和预测方面更为出色。指令微调的LLMs在文本分析方面有所提升，但在QA等复杂任务上带来的益处有限。FinBen已被用于在IJCAI-2024期间的FinNLP-AgentScen研讨会中举办首个金融LLMs共享任务，吸引了12个团队参与。他们的创新解决方案超越了GPT-4，展示了FinBen在推动金融LLMs创新方面的潜力。所有数据集、结果和代码都已向研究社区发布：https://github.com/The-FinAI/PIXIU。</p>
<h4 id="_78">一句话总结：</h4>
<p>FinBen作为首个金融LLMs评估基准，揭示了LLMs在金融领域的潜力与挑战，并推动了金融LLMs的创新。</p>
<hr />
<h2 id="graph-based-retriever-captures-the-long-tail-of-biomedical-knowledge"><a href="http://arxiv.org/abs/2402.12352v1">Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge</a></h2>
<p>发布时间：2024-02-19</p>
<p>作者：Julien Delile, Srayanta Mukherjee, Anton Van Pamel, Leonid Zhukov</p>
<h4 id="_79">中文摘要：</h4>
<p>大型语言模型（LLMs）正在改变信息检索的方式，通过自然语言对话将大量知识进行总结和呈现。然而，LLMs容易突出训练集中最常出现的信息片段，而忽视那些罕见的片段。在生物医学研究领域，最新的发现对于学术和工业参与者至关重要，但它们被不断增长的文献库（信息过载问题）所掩盖。利用LLMs揭示生物医学实体之间（如药物、基因、疾病）的新关联，成为捕捉生物医学科学产出长尾知识的挑战。为了克服这一挑战，检索增强生成（RAG）被提出，通过从外部数据集中检索上下文来增强提示，以减轻LLMs的一些不足。RAG方法通常通过文本嵌入的最大相似性搜索来选择上下文。在本研究中，我们发现RAG方法由于生物医学文献中过度代表的概念簇而遗漏了相当比例的相关信息。我们介绍了一种新颖的信息检索方法，该方法利用知识图来下采样这些簇，减轻信息过载问题。其检索性能在精确度和召回率方面都大约比嵌入相似性替代方案好两倍。最后，我们证明了嵌入相似性和知识图检索方法可以有利地结合成一个混合模型，该模型优于两者，从而为生物医学问答模型的潜在改进提供了可能性。</p>
<h4 id="_80">一句话总结：</h4>
<p>本研究提出了一种利用知识图减轻生物医学文献信息过载问题的新信息检索方法，显著提高了LLMs在生物医学问答中的性能。</p>
<hr />
<h2 id="arks-active-retrieval-in-knowledge-soup-for-code-generation"><a href="http://arxiv.org/abs/2402.12317v1">ARKS: Active Retrieval in Knowledge Soup for Code Generation</a></h2>
<p>发布时间：2024-02-19</p>
<p>作者：Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu</p>
<h4 id="_81">中文摘要：</h4>
<p>最近，检索增强生成（RAG）范式因其能够在不进一步训练的情况下将外部知识融入大型语言模型（LLMs）的潜力而备受关注。尽管在自然语言应用中得到了广泛探索，但在代码生成方面的应用仍处于探索阶段。在本文中，我们介绍了知识汤中的主动检索（ARKS），这是一种用于代码的通用大型语言模型的高级策略。与依赖于单一来源不同，我们构建了一个知识汤，它集成了网络搜索、文档、执行反馈和演化的代码片段。我们采用了一种主动检索策略，该策略迭代地细化查询并更新知识汤。为了评估ARKS的性能，我们编制了一个新的基准，其中包括与频繁更新的库和长尾编程语言相关的现实编码问题。在ChatGPT和CodeLlama上的实验结果表明，ARKS在LLMs上的平均执行精度有显著提高。分析证实了我们提出的知识汤和主动检索策略的有效性，为有效检索增强代码生成（RACG）管道的构建提供了丰富的见解。我们的模型、代码和数据可在https://arks-codegen.github.io获取。</p>
<h4 id="_82">一句话总结：</h4>
<p>本文提出了一种名为ARKS的主动检索策略，通过整合多种知识源，显著提高了大型语言模型在代码生成任务上的执行精度。</p>
<hr />
<h2 id="mafin-enhancing-black-box-embeddings-with-model-augmented-fine-tuning"><a href="http://arxiv.org/abs/2402.12177v4">Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning</a></h2>
<p>发布时间：2024-02-19</p>
<p>作者：Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber</p>
<h4 id="_83">中文摘要：</h4>
<p>检索增强生成（RAG）已成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。在RAG的检索阶段，通常涉及一个预训练的嵌入模型，该模型将查询和段落转换为向量以捕获其语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要微调。本文针对仅从黑盒模型中可获取嵌入的情况进行研究。我们引入了模型增强微调（Mafin）——一种通过添加可训练嵌入模型来微调黑盒嵌入模型的新方法。我们的结果表明，Mafin通过仅需要训练一个小型增强模型，显著提高了黑盒嵌入的性能。我们在标记和无标记数据集上验证了我们方法的有效性，展示了其广泛的应用性和效率。</p>
<h4 id="_84">一句话总结：</h4>
<p>本文提出了一种名为Mafin的新方法，通过添加可训练嵌入模型来微调黑盒嵌入模型，显著提高了大型语言模型中检索增强生成的性能。</p>
<hr />
<h2 id="feb4rag-evaluating-federated-search-in-the-context-of-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2402.11891v1">FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-02-19</p>
<p>作者：Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, Guido Zuccon</p>
<h4 id="_85">中文摘要：</h4>
<p>联邦搜索系统通过聚合多个搜索引擎的结果，选择合适的来源以提升结果质量和与用户意图保持一致。随着检索增强生成（RAG）管道的广泛应用，联邦搜索在跨异构数据源获取相关信息以生成有见地回应方面可以发挥关键作用。然而，现有的数据集，如过去TREC FedWeb轨迹中开发的数据集，都早于RAG范式转变，且缺乏对现代信息检索挑战的代表性。为了填补这一差距，我们提出了FeB4RAG，这是一个专门为RAG框架内的联邦搜索设计的创新数据集。该数据集来源于广泛使用的BEIR基准集合的16个子集合，包括790个信息请求（类似于对话查询），适用于聊天机器人应用，以及每个资源返回的顶级结果和相关的LLM（大型语言模型）导出的相关性判断。此外，为了支持这一集合的需求，我们展示了与联邦搜索的朴素方法相比，高质量联邦搜索系统对RAG响应生成的影响。我们通过定性对比分析RAG管道生成的答案来实现这一点。我们的集合促进了新联邦搜索方法的发展与评估，尤其是在RAG管道的背景下。</p>
<h4 id="_86">一句话总结：</h4>
<p>FeB4RAG是一个针对RAG框架内联邦搜索的创新数据集，旨在解决现有数据集无法代表现代信息检索挑战的问题，并通过定性对比分析证明了其在提升RAG响应生成质量方面的作用。</p>
<hr />
<h2 id="unveiling-the-magic-investigating-attention-distillation-in-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2402.11794v1">Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation</a></h2>
<p>发布时间：2024-02-19</p>
<p>作者：Zizhong Li, Haopeng Zhang, Jiawei Zhang</p>
<p>：</p>
<h4 id="_87">中文摘要：</h4>
<p>检索增强生成框架可以通过实现实时知识更新来解决大型语言模型的局限性，从而提供更准确的答案。在检索增强模型的训练阶段，注意力蒸馏是一种有效的方法，它使用注意力分数作为监督信号，而不是手动标注的查询-文档对。尽管注意力蒸馏越来越受欢迎，但其成功背后的详细机制仍未被充分探索，特别是它利用的具体模式以促进训练。在本文中，我们通过全面审查注意力蒸馏工作流程并确定影响检索增强语言模型学习质量的关键因素来填补这一空白。我们进一步提出了优化模型训练方法和避免无效训练的指标。</p>
<h4 id="_88">一句话总结：</h4>
<p>本文通过分析注意力蒸馏机制，为检索增强语言模型的训练提供了优化策略和评价指标。</p>
<hr />
<h2 id="metacognitive-retrieval-augmented-large-language-models"><a href="http://arxiv.org/abs/2402.11626v1">Metacognitive Retrieval-Augmented Large Language Models</a></h2>
<p>发布时间：2024-02-18</p>
<p>作者：Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, Zhicheng Dou</p>
<h4 id="_89">中文摘要：</h4>
<p>检索增强生成在自然语言处理中变得至关重要，因为它们在生成事实性内容方面非常有效。尽管传统方法采用单次检索，但更近期的 approaches（方法）已经转向多时间检索以进行多跳推理任务。然而，这些策略受限于预定义的推理步骤，可能导致响应生成中的不准确。本文介绍了MetaRAG，这是一种将检索增强生成过程与元认知相结合的方法。借鉴认知心理学，元认知允许实体进行自我反思和批判性地评估其认知过程。通过整合这一机制，MetaRAG使模型能够监控、评估和规划其响应策略，增强了其内省推理能力。通过一个三步元的认知调节管道，模型可以识别初始认知响应中的不足并加以修正。实证评估表明，MetaRAG显著优于现有方法。</p>
<h4 id="_90">一句话总结：</h4>
<p>MetaRAG通过结合元认知与检索增强生成，显著提升了自然语言处理模型在生成事实性内容时的准确性和内省推理能力。</p>
<hr />
<h2 id="blendfilter-advancing-retrieval-augmented-large-language-models-via-query-generation-blending-and-knowledge-filtering"><a href="http://arxiv.org/abs/2402.11129v2">BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering</a></h2>
<p>发布时间：2024-02-16</p>
<p>作者：Haoyu Wang, Ruirui Li, Haoming Jiang, Jinjin Tian, Zhengyang Wang, Chen Luo, Xianfeng Tang, Monica Cheng, Tuo Zhao, Jing Gao</p>
<h4 id="_91">中文摘要：</h4>
<p>检索增强的大型语言模型（LLMs）在提升知识密集型场景的性能方面提供了显著的好处。然而，这些方法通常面临复杂输入的挑战，并且由于噪声知识检索而遇到困难，这显著阻碍了模型的有效性。为了解决这个问题，我们引入了BlendFilter，这是一种新颖的方法，通过整合查询生成混合与知识过滤来提升检索增强的LLMs。BlendFilter通过其查询生成方法提出了混合过程，该方法将外部和内部知识增强与原始查询相结合，确保全面的信息收集。此外，我们独特的知识过滤模块利用了LLM的内在能力，有效地消除了无关数据。我们在三个开放域问答基准上进行了广泛的实验，结果表明我们的创新性BlendFilter在性能上显著超越了最先进的基线。</p>
<h4 id="_92">一句话总结：</h4>
<p>BlendFilter通过结合查询生成混合和知识过滤，显著提升了检索增强的大型语言模型在知识密集型场景中的性能。</p>
<hr />
<h2 id="persona-db-efficient-large-language-model-personalization-for-response-prediction-with-collaborative-data-refinement"><a href="http://arxiv.org/abs/2402.11060v1">Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement</a></h2>
<p>发布时间：2024-02-16</p>
<p>作者：Chenkai Sun, Ke Yang, Revanth Gangi Reddy, Yi R. Fung, Hou Pong Chan, ChengXiang Zhai, Heng Ji</p>
<h4 id="_93">中文摘要：</h4>
<p>随着对大型语言模型（LLMs）进行个性化交互需求的增加，迫切需要开发能够准确且高效地识别用户意见和偏好的方法。检索增强作为一种有效的策略，能够在不产生微调成本的情况下满足大量用户的需求。然而，现有研究主要集中于增强检索阶段，而对优化数据库表示的探索有限，这对于个性化等任务至关重要。在本研究中，我们从一个新的角度审视了这个问题，重点关注如何在LLM定制背景下更好地表示数据以提高检索效率。为了应对这一挑战，我们引入了Persona-DB，这是一个简单而有效的框架，包括一个分层构建过程，以提高跨任务上下文的一般化能力，以及协作细化，以有效地弥合用户之间的知识差距。在响应预测任务中，Persona-DB在保持准确性的同时显著减少了检索规模，这在历史记录广泛或上下文窗口有限的情况下是一个关键优势。我们的实验还表明，在冷启动场景下，当用户数据非常稀疏时，性能提高了超过15%。此外，我们的分析还揭示了随着检索能力的增强，协作知识的重要性日益增加。</p>
<h4 id="_94">一句话总结：</h4>
<p>本研究提出了一种名为Persona-DB的框架，通过优化数据库表示和协作知识，提高了大型语言模型在个性化交互中的检索效率和准确性。</p>
<hr />
<h2 id="retrieval-augmented-generation-is-dense-passage-retrieval-retrieving"><a href="http://arxiv.org/abs/2402.11035v2">Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?</a></h2>
<p>发布时间：2024-02-16</p>
<p>作者：Benjamin Reichman, Larry Heck</p>
<h4 id="_95">中文摘要：</h4>
<p>密集文本检索（DPR）是检索增强生成（RAG）范式中的第一步，旨在提高大型语言模型（LLM）的性能。DPR通过微调预训练网络来增强查询与相关文本数据之间的嵌入对齐。为了从根本上释放这一方法的全部潜力，需要更深入地理解DPR的微调过程。在本研究中，我们通过结合探针、层激活分析和模型编辑等方法，对DPR训练模型进行机制性探索。我们的实验表明，DPR训练分散了网络中知识的存储方式，为相同信息创建了多个访问路径。我们还发现这种训练方式的一个局限性：预训练模型内部的知识限制了检索模型可以检索的内容。这些发现为密集检索提出了一些可能的改进方向：（1）将DPR训练过程暴露给更多知识，以便更多知识可以分散；（2）将事实作为分散表示注入；（3）在检索过程中对模型和知识不确定性进行建模和整合；（4）直接将内部模型知识映射到知识库。</p>
<h4 id="_96">一句话总结：</h4>
<p>本研究通过机制性探索发现，密集文本检索（DPR）训练分散了网络中知识的存储，但受限于预训练模型内部知识，提出了改进密集检索的几个可能方向。</p>
<hr />
<h2 id="pat-questions-a-self-updating-benchmark-for-present-anchored-temporal-question-answering"><a href="http://arxiv.org/abs/2402.11034v2">PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering</a></h2>
<p>发布时间：2024-02-16</p>
<p>作者：Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong, Vagelis Hristidis</p>
<h4 id="_97">中文摘要：</h4>
<p>现有的时间问答（TQA）研究主要集中在锚定于特定时间戳或事件的问答上（例如，“1970年美国总统是谁？”）。很少有研究关注时间背景相对于当前时间的问答（例如，“谁是上一任美国总统？”）。我们将这个问题称为基于当前锚点的时间问答（PATQA）。PATQA提出了独特的挑战：（1）大型语言模型（LLMs）可能存在过时知识，（2）复杂的时序关系（例如，“之前”，“上一任”）难以推理，（3）可能需要多跳推理，以及（4）基准测试的金答案必须持续更新。为了解决这些挑战，我们引入了PAT-Questions基准，它包括单跳和多跳时序问答。如果可用，PAT-Questions中的答案可以通过在知识图上重新运行SPARQL查询来自动刷新。我们通过直接提示和检索增强生成（RAG）评估了几个最先进的LLMs和一个最先进的时序推理模型（TEMPREASON-T5）在PAT-Questions上的表现。结果表明，现有解决方案在PATQA中的局限性，并激励了需要新方法来提高PATQA推理能力。</p>
<h4 id="_98">一句话总结：</h4>
<p>本研究提出了基于当前锚点的时间问答（PATQA）问题，并引入了PAT-Questions基准，以评估和改进现有LLMs在时序问答中的推理能力。</p>
<hr />
<h2 id="retrieve-only-when-it-needs-adaptive-retrieval-augmentation-for-hallucination-mitigation-in-large-language-models"><a href="http://arxiv.org/abs/2402.10612v1">Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models</a></h2>
<p>发布时间：2024-02-16</p>
<p>作者：Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng</p>
<h4 id="_99">中文摘要：</h4>
<p>幻觉对于大型语言模型（LLMs）的实际应用构成了重大挑战。在生成事实性内容时，LLMs对参数化知识的利用受到其知识局限性的限制，可能导致内部幻觉的产生。虽然引入外部信息可以帮助填补知识空白，但也引入了无关信息的风险，从而增加了外部幻觉的可能性。在LLMs中谨慎且平衡地整合参数化知识与外部信息对于减轻幻觉至关重要。在本研究中，我们提出了Rowen，这是一种新颖的方法，通过针对解决幻觉输出而设计的选择性检索增强过程来增强LLMs。该过程由一个多语言语义感知检测模块控制，该模块评估针对相同查询的扰动响应在不同语言中的一致性。一旦检测到表明幻觉的不一致性，Rowen就会激活外部信息的检索以纠正模型输出。Rowen巧妙地协调LLMs中的内在参数与外部知识源，通过确保内部推理与外部证据的平衡整合，有效地减轻了幻觉。通过全面的实证分析，我们证明了Rowen在检测和减轻LLMs输出中的幻觉内容方面超越了当前最先进的技术。</p>
<h4 id="_100">一句话总结：</h4>
<p>Rowen通过多语言语义感知检测和外部信息检索，有效减轻了大型语言模型中的幻觉问题。</p>
<hr />
<h2 id="generative-ai-in-the-construction-industry-a-state-of-the-art-analysis"><a href="http://arxiv.org/abs/2402.09939v1">Generative AI in the Construction Industry: A State-of-the-art Analysis</a></h2>
<p>发布时间：2024-02-15</p>
<p>作者：Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Tarek Zayed</p>
<h4 id="_101">中文摘要：</h4>
<p>建筑行业是全球经济的支柱产业，但在设计、规划、采购、检查和维护等各个流程中面临着许多生产率挑战。基于输入或先验知识生成新颖和现实的数据或内容（如文本、图像、视频或代码）的生成式人工智能（AI），为解决这些挑战提供了创新和颠覆性的解决方案。然而，关于生成式人工智能在建筑行业当前状态、机遇和挑战方面的文献中存在差距。本研究旨在填补这一空白，通过对建筑行业生成式人工智能的最新分析，实现以下三个目标：（1）回顾和分类建筑行业中现有和新兴的生成式人工智能机遇和挑战；（2）为建筑公司提出一个框架，以利用自己的数据构建定制的生成式人工智能解决方案，包括数据收集、数据集整理、训练定制的巨型语言模型（LLM）、模型评估和部署等步骤；（3）通过开发查询合同文档的生成模型的案例研究来展示该框架。结果表明，检索增强生成（RAG）在质量、相关性和可重复性方面将基线LLM分别提高了5.2%、9.4%和4.8%。本研究为学术界和建筑专业人士提供了一个全面的分析和实用框架，以指导采用生成式人工智能技术，以提高建筑行业的生产率、质量、安全性和可持续性。</p>
<h4 id="_102">一句话总结：</h4>
<p>本研究通过分析生成式人工智能在建筑行业的应用，为提升行业整体效率和质量提供了创新框架和实用工具。</p>
<hr />
<h2 id="generative-representational-instruction-tuning"><a href="http://arxiv.org/abs/2402.09906v2">Generative Representational Instruction Tuning</a></h2>
<p>发布时间：2024-02-15</p>
<p>作者：Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela</p>
<h4 id="_103">中文摘要：</h4>
<p>所有基于文本的语言问题都可以归结为生成或嵌入。当前的模型要么在生成任务上表现良好，要么在嵌入任务上表现良好。我们引入了生成表示指令微调（GRIT），通过指令区分生成和嵌入任务，训练大型语言模型同时处理这两种任务。与其它开源模型相比，我们得到的GritLM 7B在大量文本嵌入基准（MTEB）上达到了新的水平，并在一系列生成任务上优于所有同等规模的模型。通过进一步扩展，GritLM 8x7B在所有我们尝试的开源生成语言模型中表现最佳，同时仍然是最好的嵌入模型之一。值得注意的是，我们发现GRIT与仅生成或嵌入数据训练的效果相当，因此我们可以统一两者而不会损失性能。除此之外，通过GRIT的统一使得检索增强生成（RAG）在长文档上的速度提高了超过60%，不再需要单独的检索和生成模型。模型、代码等资源可在https://github.com/ContextualAI/gritlm上免费获取。</p>
<h4 id="_104">一句话总结：</h4>
<p>GRIT通过指令区分生成和嵌入任务，实现了大型语言模型在生成和嵌入任务上的统一，显著提升了文本处理能力。</p>
<hr />
<h2 id="grounding-language-model-with-chunking-free-in-context-retrieval"><a href="http://arxiv.org/abs/2402.09760v1">Grounding Language Model with Chunking-Free In-Context Retrieval</a></h2>
<p>发布时间：2024-02-15</p>
<p>作者：Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, Zhicheng Dou</p>
<h4 id="_105">中文摘要：</h4>
<p>本文提出了一种新颖的无分块上下文检索（CFIC）方法，该方法专门针对检索增强生成（RAG）系统。传统的RAG系统在利用精确的证据文本进行响应归因时往往遇到困难，这是因为处理长文档和过滤无关内容的挑战。常用的解决方案，如文档分块和调整语言模型以处理更长的上下文，都有其局限性。这些方法要么破坏了文本的语义连贯性，要么未能有效解决证据检索中的噪声和准确性问题。CFIC通过绕过传统的分块过程来应对这些挑战。它利用文档的编码隐藏状态进行上下文检索，采用自动激进的解码来准确识别用户查询所需的具体证据文本，从而消除了分块的需求。CFIC通过引入两种解码策略进一步得到增强，即约束句子前缀解码和跳过解码。这些策略不仅提高了检索过程的效率，还确保了生成的归因文本证据的准确性。我们在一系列公开的问答数据集上对CFIC的评估表明，它在检索相关和准确证据方面具有优越性，与传统方法相比有显著改进。通过消除文档分块的需求，CFIC提供了一种更简洁、有效和高效的检索解决方案，这是RAG系统领域的一项重要进步。</p>
<h4 id="_106">一句话总结：</h4>
<p>本文提出了一种无分块上下文检索方法，有效提高了检索增强生成系统在证据检索中的准确性和效率。</p>
<hr />
<h2 id="advancing-building-energy-modeling-with-large-language-models-exploration-and-case-studies"><a href="http://arxiv.org/abs/2402.09579v1">Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies</a></h2>
<p>发布时间：2024-02-14</p>
<p>作者：Liang Zhang, Zhelun Chen, Vitaly Ford</p>
<h4 id="_107">中文摘要：</h4>
<p>本文探讨了大型语言模型与建筑能耗模拟软件的创新集成，特别是ChatGPT与EnergyPlus的结合。首先，通过文献综述揭示了在工程建模中融入大型语言模型的趋势正在增长，尽管在建筑能耗模拟方面的研究有限。本文强调了大型语言模型在解决建筑能耗模拟挑战方面的潜力，并概述了包括1）模拟输入生成，2）模拟输出分析和可视化，3）进行错误分析，4）协同模拟，5）模拟知识提取和训练，以及6）模拟优化在内的潜在应用。三个案例研究揭示了大型语言模型在自动化和优化建筑能耗模拟任务中的变革潜力，强调了人工智能在推进可持续建筑实践和能源效率中的关键作用。案例研究表明，选择合适的大型语言模型技术对于提高性能和减少工程工作量至关重要。除了直接使用大型语言模型外，还利用了三种具体技术：1）提示工程，2）检索增强生成，以及3）多智能体大型语言模型。研究结果倡导未来人工智能研究采取跨学科的方法，其影响不仅限于建筑能耗模拟，还扩展到其他专业工程建模。</p>
<h4 id="_108">一句话总结：</h4>
<p>本文通过将大型语言模型与建筑能耗模拟软件相结合，展示了人工智能在提高建筑能耗模拟效率和可持续性方面的巨大潜力。</p>
<hr />
<h2 id="pandora-jailbreak-gpts-by-retrieval-augmented-generation-poisoning"><a href="http://arxiv.org/abs/2402.08416v1">Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning</a></h2>
<p>发布时间：2024-02-13</p>
<p>作者：Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu</p>
<h4 id="_109">中文摘要：</h4>
<p>大型语言模型（LLMs）在各个领域得到了广泛的应用，因此确保这些模型的安全性至关重要。越狱攻击，即通过操纵LLMs生成恶意内容，被视为一个重要的漏洞。尽管现有研究主要关注对LLMs的直接越狱攻击，但对间接方法的探索却有限。将各种插件集成到LLMs中，特别是检索增强生成（RAG），这使得LLMs能够将外部知识库（如GPTs）纳入其响应生成过程，为间接越狱攻击开辟了新的途径。为了填补这一空白，我们研究了LLMs，尤其是GPTs的间接越狱攻击，引入了一种名为检索增强生成中毒（Retrieval Augmented Generation Poisoning）的新攻击向量。这种方法，Pandora，通过提示操纵利用LLMs和RAG之间的协同作用来生成意外的响应。Pandora使用恶意制作的内容来影响RAG过程，有效地启动越狱攻击。我们的初步测试表明，Pandora在四种不同的场景中成功进行了越狱攻击，其成功率高于直接攻击，GPT-3.5的成功率为64.3%，GPT-4的成功率为34.8%。</p>
<h4 id="_110">一句话总结：</h4>
<p>本研究提出了一种名为Pandora的新攻击方法，通过检索增强生成中毒来对大型语言模型进行间接越狱攻击，有效提升了攻击成功率。</p>
<hr />
<h2 id="retrieval-augmented-thought-process-as-sequential-decision-making"><a href="http://arxiv.org/abs/2402.07812v1">Retrieval-Augmented Thought Process as Sequential Decision Making</a></h2>
<p>发布时间：2024-02-12</p>
<p>作者：Thomas Pouplin, Hao Sun, Samuel Holt, Mihaela van der Schaar</p>
<h4 id="_111">中文摘要：</h4>
<p>大型语言模型（LLMs）已经展示了它们在协助人们和展现“智慧火花”方面的强大能力。然而，一些开放性的挑战阻碍了它们更广泛的应用，例如对隐私的担忧、产生幻觉的倾向以及处理长上下文的困难。在这项工作中，我们通过引入检索增强思维过程（RATP）来解决这些挑战。RATP通过访问外部知识，将LLMs的思维生成过程视为一个多步骤决策过程。为了优化这种思维过程，RATP利用蒙特卡洛树搜索，并学习了一个Q值估计器，允许高效推理。在处理涉及隐私数据的问答任务时，其中伦理和安全问题限制了LLMs的训练方法，RATP相较于现有的上下文检索增强语言模型实现了50%的性能提升。</p>
<h4 id="_112">一句话总结：</h4>
<p>该研究通过引入检索增强思维过程（RATP）解决了大型语言模型在隐私、幻觉和长上下文处理方面的挑战，实现了问答任务中性能的显著提升。</p>
<hr />
<h2 id="t-rag-lessons-from-the-llm-trenches"><a href="http://arxiv.org/abs/2402.07483v2">T-RAG: Lessons from the LLM Trenches</a></h2>
<p>发布时间：2024-02-12</p>
<p>作者：Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla</p>
<h4 id="_113">中文摘要：</h4>
<p>大型语言模型（LLM）展现出卓越的语言能力，推动了将它们集成到广泛领域应用中的尝试。一个重要的应用领域是在私有企业文档上进行问答，其中主要考虑因素是数据安全，这需要可以部署在本地、有限的计算资源和能够正确响应查询的稳健应用。检索增强生成（RAG）已成为构建基于LLM应用的最突出框架。虽然构建RAG相对简单，但要使其稳健且可靠，则需要大量的定制和相对深入的应用领域知识。我们分享了构建和部署用于在私有组织文档上进行问答的LLM应用的经验。我们的应用结合了RAG的使用和微调的开源LLM。此外，我们的系统，我们称之为树-RAG（T-RAG），使用树结构来表示组织内部的实体层次结构。这被用来在响应涉及组织层次结构中实体的用户查询时，生成文本描述以增强上下文。我们的评估，包括“针在草堆中”测试，表明这种组合比简单的RAG或微调实现表现更好。最后，我们根据构建用于实际应用的LLM应用的经验，分享了一些经验教训。</p>
<h4 id="_114">一句话总结：</h4>
<p>本研究通过结合RAG和微调的开源LLM，并使用树结构来增强上下文，构建了一个稳健且可靠的私有企业文档问答系统。</p>
<hr />
<h2 id="poisonedrag-knowledge-poisoning-attacks-to-retrieval-augmented-generation-of-large-language-models"><a href="http://arxiv.org/abs/2402.07867v1">PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models</a></h2>
<p>发布时间：2024-02-12</p>
<p>作者：Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia</p>
<h4 id="_115">中文摘要：</h4>
<p>大型语言模型（LLMs）因其卓越的生成能力而取得了显著的成功。尽管如此，它们也存在固有的局限性，如缺乏最新知识和幻觉。检索增强生成（RAG）是一种最先进的缓解这些局限性的技术。具体来说，给定一个问题，RAG从知识数据库中检索相关知识以增强LLM的输入。例如，检索到的知识可能是一组与给定问题最语义相似的top-k文本，当知识数据库包含从维基百科收集的数百万文本时。因此，LLM可以利用检索到的知识作为上下文来为给定问题生成答案。现有研究主要关注提高RAG的准确度或效率，而对其安全性探讨较少。我们旨在填补这一空白。特别是，我们提出了PoisonedRAG，一套针对RAG的知识中毒攻击，其中攻击者可以向知识数据库中注入少量中毒文本，使得LLM为攻击者选择的目标问题生成攻击者选择的目标答案。我们将知识中毒攻击建模为一个优化问题，其解是一组中毒文本。根据攻击者对RAG的背景知识（例如，黑盒和白盒设置），我们分别提出了两种解决方案来解决优化问题。我们在多个基准数据集和LLMs上的结果表明，当向包含数百万文本的数据库中注入每个目标问题的5个中毒文本时，我们的攻击可以实现90%的攻击成功率。我们还评估了最近的防御措施，我们的结果表明，它们不足以防御我们的攻击，突显了新防御措施的需求。</p>
<h4 id="_116">一句话总结：</h4>
<p>本研究提出了一种针对检索增强生成（RAG）的知识中毒攻击方法，揭示了RAG在安全性方面的不足，并强调了开发新防御措施的必要性。</p>
<hr />
<h2 id="cybermetric-a-benchmark-dataset-based-on-retrieval-augmented-generation-for-evaluating-llms-in-cybersecurity-knowledge"><a href="http://arxiv.org/abs/2402.07688v2">CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs in Cybersecurity Knowledge</a></h2>
<p>发布时间：2024-02-12</p>
<p>作者：Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, Tamas Bisztray, Merouane Debbah</p>
<h4 id="_117">中文摘要：</h4>
<p>大型语言模型（LLMs）在各个领域得到越来越广泛的应用，从软件开发到网络威胁情报。理解网络安全的所有不同领域，包括密码学、逆向工程和风险评估等主题，即使是对于人类专家来说也是一个挑战。为了准确测试LLMs在网络安全领域的通用知识，研究界需要一个多样化、准确且最新的数据集。为了解决这一差距，我们提出了CyberMetric-80、CyberMetric-500、CyberMetric-2000和CyberMetric-10000，这些是多选题型问答基准数据集，分别包含80、500、2000和10000个问题。通过利用GPT-3.5和检索增强生成（RAG），我们收集了包括NIST标准、研究论文、公开可访问的书籍、RFC和其他网络安全领域的出版物，以生成每个问题都有四个可能答案的问题。结果经过了几轮的错误检查和精炼。人类专家投入了超过200小时来验证问题和解决方案，以确保其准确性和相关性，并筛选出任何与网络安全无关的问题。我们在CyberMetric数据集上评估和比较了25个最先进的LLM模型。除了评估LLMs的主要目标外，我们还让30名人类参与者在一个闭卷场景下解决CyberMetric-80。这些结果可以作为比较人类和LLMs通用网络安全知识的参考。研究发现，GPT-4o、GPT-4-turbo、Mixtral-8x7B-Instruct、Falcon-180B-Chat和GEMINI-pro 1.0是表现最好的LLMs。此外，在CyberMetric-80上，顶级LLMs的准确率高于人类，尽管经验丰富的专家仍然优于Llama-3-8B、Phi-2或Gemma-7b等小型模型。</p>
<h4 id="_118">一句话总结：</h4>
<p>本研究通过构建CyberMetric数据集，评估了多种LLMs在网络安全领域的通用知识，发现GPT-4系列和Mixtral等模型表现优异。</p>
<hr />
<h2 id="g-retriever-retrieval-augmented-generation-for-textual-graph-understanding-and-question-answering"><a href="http://arxiv.org/abs/2402.07630v3">G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering</a></h2>
<p>发布时间：2024-02-12</p>
<p>作者：Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi</p>
<h4 id="_119">中文摘要：</h4>
<p>给定具有文本属性的图，我们使用户能够“与他们的图进行聊天”：即使用对话界面来提问关于图的问题。针对用户的问题，我们的方法提供文本回复并突出显示图中的相关部分。尽管现有工作以各种方式整合了大型语言模型（LLMs）和图神经网络（GNNs），但它们大多侧重于传统的图任务（如节点、边和图分类），或者在小或合成图上回答简单的图查询。相比之下，我们开发了一个灵活的问答框架，针对现实世界的文本图，适用于包括场景图理解、常识推理和知识图谱推理在内的多个应用。为此，我们首先开发了一个图问答（GraphQA）基准，该基准收集了来自不同任务的数据。然后，我们提出了我们的G-Retriever方法，引入了第一个针对通用文本图的检索增强生成（RAG）方法，该方法可以通过软提示进行微调以增强图理解。为了抵抗幻觉并允许文本图远大于LLM的上下文窗口大小，G-Retriever通过将此任务表述为奖励收集斯坦纳树优化问题，在图上执行RAG。实证评估表明，我们的方法在多个领域的文本图任务上优于基线，随着图大小的增加具有良好的扩展性，并减轻了幻觉。~\footnote{我们的代码和数据集可在以下网址获取：\url{https://github.com/XiaoxinHe/G-Retriever}}</p>
<h4 id="_120">一句话总结：</h4>
<p>该研究提出了一种名为G-Retriever的图问答方法，通过检索增强生成技术，实现了对大型文本图的问答，有效提升了图理解能力。</p>
<hr />
<h2 id="prompt-perturbation-in-retrieval-augmented-generation-based-large-language-models"><a href="http://arxiv.org/abs/2402.07179v3">Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models</a></h2>
<p>发布时间：2024-02-11</p>
<p>作者：Zhibo Hu, Chen Wang, Yanfeng Shu, Helen, Paik, Liming Zhu</p>
<h4 id="_121">中文摘要：</h4>
<p>随着大型语言模型（LLMs）在各个领域的应用迅速增长，其鲁棒性变得越来越重要。检索增强生成（RAG）被视为提高LLMs文本生成可信度的手段。然而，基于RAG的LLMs的输出如何受到略微不同的输入的影响尚未得到充分研究。在本工作中，我们发现，即使在提示中插入一个很短的前缀，也会导致生成远离事实正确答案的输出。我们通过引入一种名为梯度引导提示扰动（GGPP）的新颖优化技术，系统地评估了此类前缀对RAG的影响。GGPP在引导基于RAG的LLMs的输出指向目标错误答案方面取得了高成功率。它还可以应对提示中的指令，要求忽略无关的上下文。我们还利用LLMs在带有和没有GGPP扰动的提示之间的神经元激活差异，提供了一种通过在由GGPP生成的提示触发的神经元激活上训练的高度有效的检测器来提高基于RAG的LLMs鲁棒性的方法。我们在开源LLMs上的评估证明了我们方法的有效性。</p>
<h4 id="_122">一句话总结：</h4>
<p>本研究通过引入GGPP技术，揭示了基于RAG的LLMs输出对微小输入变化的敏感性，并提出了提高其鲁棒性的方法。</p>
<hr />
<h2 id="generalizing-conversational-dense-retrieval-via-llm-cognition-data-augmentation"><a href="http://arxiv.org/abs/2402.07092v3">Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation</a></h2>
<p>发布时间：2024-02-11</p>
<p>作者：Haonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan Liu, Ziliang Zhao</p>
<h4 id="_123">中文摘要：</h4>
<p>本文提出了一种通过LLM-cognition数据增强（ConvAug）来泛化对话密集检索框架。该框架首先生成多级增强对话以捕捉对话上下文的多样性。受人类认知的启发，我们设计了一种认知感知过程以减少假阳性、假阴性和幻觉的产生。此外，我们还开发了一种难度自适应样本过滤器，用于选择复杂对话中的挑战性样本，从而为模型提供更大的学习空间。然后，采用对比学习目标来训练更好的对话上下文编码器。在四个公开数据集上进行的广泛实验，在正常和零样本设置下，证明了ConvAug的有效性、泛化能力和适用性。代码已发布在https://github.com/haon-chen/ConvAug。</p>
<h4 id="_124">一句话总结：</h4>
<p>本文提出的ConvAug框架通过LLM-cognition数据增强，有效提升了对话密集检索的泛化能力和适用性。</p>
<hr />
<h2 id="realm-rag-driven-enhancement-of-multimodal-electronic-health-records-analysis-via-large-language-models"><a href="http://arxiv.org/abs/2402.07016v1">REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models</a></h2>
<p>发布时间：2024-02-10</p>
<p>作者：Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, Chengwei Pan</p>
<h4 id="_125">中文摘要：</h4>
<p>本研究提出了一种名为REALM的检索增强生成框架，旨在通过整合多模态电子健康记录（EHR）数据，显著提升临床预测能力。该框架利用大型语言模型（LLM）对长文本临床笔记进行编码，并使用GRU模型对时间序列EHR数据进行编码。此外，LLM被用于提取与任务相关的医学实体，并将这些实体与专业标注的外部知识图谱（PrimeKG）中的对应医学知识进行匹配。通过匹配和与临床标准的对齐，该框架消除了幻觉并确保了一致性。最后，我们提出了一种自适应的多模态融合网络，以整合提取的知识和多模态EHR数据。在MIMIC-III死亡和再入院任务上的广泛实验表明，REALM框架在性能上优于基线模型，强调了每个模块的有效性。REALM框架有助于优化多模态EHR数据在医疗保健中的应用，并弥合了对于临床预测至关重要的细微医学背景之间的差距。</p>
<h4 id="_126">一句话总结：</h4>
<p>REALM框架通过整合多模态EHR数据和外部知识图谱，显著提升了临床预测能力，并优化了医疗保健中多模态数据的应用。</p>
<hr />
<h2 id="exaranker-open-synthetic-explanation-for-ir-using-open-source-llms"><a href="http://arxiv.org/abs/2402.06334v1">ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs</a></h2>
<p>发布时间：2024-02-09</p>
<p>作者：Fernando Ferraretto, Thiago Laitz, Roberto Lotufo, Rodrigo Nogueira</p>
<h4 id="_127">中文摘要：</h4>
<p>ExaRanker最近提出了一种训练信息检索（IR）模型的方法，该方法将自然语言解释作为额外的标签纳入其中。该方法解决了标签示例有限的问题，从而提高了IR模型的有效性。然而，最初的结果是基于如GPT-3.5等专有语言模型，由于成本和数据隐私问题，这对其数据集大小造成了限制。在本文中，我们介绍了ExaRanker-Open，其中我们调整并探索了开源语言模型在生成解释中的应用。该方法已经通过不同的LLMs和数据集大小进行了测试，以更好地理解数据增强的有效贡献。我们的发现表明，将解释纳入其中可以持续提升神经排名器的性能，随着LLM规模的增加，其益处也相应增加。值得注意的是，数据增强方法即使在大型数据集上也证明是有益的，正如我们的研究中ExaRanker超过目标基线0.6 nDCG@10点所证明的那样。为了鼓励研究社区的进一步发展，我们在https://github.com/unicamp-dl/ExaRanker上开源了代码和数据集。</p>
<h4 id="_128">一句话总结：</h4>
<p>ExaRanker-Open通过使用开源语言模型生成解释，显著提升了信息检索模型的效果，尤其是在大型数据集上。</p>
<hr />
<h2 id="glam-fine-tuning-large-language-models-for-domain-knowledge-graph-alignment-via-neighborhood-partitioning-and-generative-subgraph-encoding"><a href="http://arxiv.org/abs/2402.06764v3">GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding</a></h2>
<p>发布时间：2024-02-09</p>
<p>作者：Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, Sutanay Choudhury</p>
<h4 id="_129">中文摘要：</h4>
<p>将大型语言模型（LLMs）与从特定领域数据中提取的知识图谱相结合，是朝着更强大和更真实的推理能力的重要进步。随着这些模型能力的增强，使它们能够在现实世界知识图谱上执行多步推理，同时最大限度地减少幻觉，变得至关重要。虽然大型语言模型在对话和文本生成方面表现出色，但它们在处理相互关联实体的领域特定图上的推理能力仍然有限。例如，我们能否查询一个LLM来根据私人数据库中的关系和属性，为特定目标识别专业网络中的最佳联系人？答案是否定的——这样的能力超出了当前方法的能力范围。然而，这个问题突显了一个必须解决的关键技术差距。许多在科学、安全和电子商务等领域的有价值应用，依赖于编码独特结构、关系和逻辑约束的专有知识图谱。我们介绍了一个微调框架，用于开发图对齐语言模型（GLaM），该框架将知识图谱转换为一个带有标记问答对的替代文本表示。我们证明，将模型基于特定图的知识进行归一化，可以扩展模型基于结构的推理能力。我们的方法利用大型语言模型的生成能力来创建数据集，并提出了一种高效的检索增强生成风格的替代方法。</p>
<h4 id="_130">一句话总结：</h4>
<p>本研究提出了一种基于图的知识图谱语言模型（GLaM），通过将知识图谱转换为文本表示，增强了大型语言模型在结构化推理方面的能力。</p>
<hr />
<h2 id="merging-facts-crafting-fallacies-evaluating-the-contradictory-nature-of-aggregated-factual-claims-in-long-form-generations"><a href="http://arxiv.org/abs/2402.05629v4">Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations</a></h2>
<p>发布时间：2024-02-08</p>
<p>作者：Cheng-Han Chiang, Hung-yi Lee</p>
<h4 id="_131">中文摘要：</h4>
<p>长文本生成的大型语言模型（LLMs）包含事实和非事实的陈述，这使得评估其真实性变得困难。先前的研究通过将长段落分解成多个事实，独立验证这些事实，并汇总结果来评估长段落的事实性。这些方法假设将事实陈述组合在一起形成一个事实段落。然而，这一假设可能被违反：我们发现像Llama-chat这样的强大开源模型可以生成包含可验证事实的段落，但由于实体歧义，这些事实被组合成一个非事实段落。我们进一步揭示，现有的真实性度量指标，包括FActScore和引用召回率，无法正确评估这些非事实段落，并高估了它们的事实性。为了解决这个问题，我们引入了一个增强的指标，D-FActScore，专门设计用于具有模糊实体的内容。我们评估了检索增强型LLMs生成的人物传记的D-FActScores。我们发现D-FActScore比FActScore能更好地评估具有实体歧义的段落的事实性。我们还发现，四种广泛使用的开源LLMs倾向于将不同实体的信息混合在一起形成非事实段落，这使得它们的D-FActScore比FActScore低10%以上。</p>
<h4 id="_132">一句话总结：</h4>
<p>本研究提出了一种新的真实性度量指标D-FActScore，用于评估大型语言模型生成文本中实体歧义导致的事实性，并发现现有指标高估了这些文本的真实性。</p>
<hr />
<h2 id="dfa-rag-conversational-semantic-router-for-large-language-model-with-definite-finite-automaton"><a href="http://arxiv.org/abs/2402.04411v2">DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton</a></h2>
<p>发布时间：2024-02-06</p>
<p>作者：Yiyou Sun, Junjie Hu, Wei Cheng, Haifeng Chen</p>
<h4 id="_133">中文摘要：</h4>
<p>本文介绍了一种名为检索增强有限自动机（DFA-RAG）的大语言模型检索框架，这是一种旨在增强对话代理能力的新颖框架。传统的LLM在需要遵循预定义响应指南的特殊场景中，如情感支持和客户服务，面临生成受控和合规响应的挑战。我们的框架通过在LLM中嵌入从训练对话中学习到的有限自动机（DFA），来解决这些挑战。这种结构化方法充当语义路由器，使LLM能够遵循确定性响应路径。路由是通过检索增强生成（RAG）策略实现的，该策略仔细选择与当前对话上下文一致的对话示例。DFA-RAG的优点包括通过可读的DFA实现可解释的结构、对话中响应的上下文感知检索以及与现有LLM的即插即用兼容性。广泛的基准测试验证了DFA-RAG的有效性，表明其作为对话代理的有价值贡献的潜力。</p>
<h4 id="_134">一句话总结：</h4>
<p>本文提出了一种名为DFA-RAG的检索增强大语言模型框架，旨在提升对话代理在特定场景下的响应生成能力。</p>
<hr />
<h2 id="enhancing-retrieval-processes-for-language-generation-with-augmented-queries"><a href="http://arxiv.org/abs/2402.16874v1">Enhancing Retrieval Processes for Language Generation with Augmented Queries</a></h2>
<p>发布时间：2024-02-06</p>
<p>作者：Julien Pierre Edmond Ghali, Kosuke Shima, Koichi Moriyama, Atsuko Mutoh, Nobuhiro Inuzuka</p>
<h4 id="_135">中文摘要：</h4>
<p>在智能技术快速发展的世界中，由于高级语言模型的兴起，搜索文档变得更加具有挑战性。这些模型有时会面临困难，如提供不准确的信息，通常被称为“幻觉”。本研究通过检索增强生成（RAG）技术来解决这一问题，该技术指导模型根据真实事实给出准确的响应。为了克服可扩展性问题，研究探索了使用创新的查询优化过程将用户查询与复杂的语言模型（如BERT和Orca2）相连接。研究分为三个场景：首先，没有RAG；其次，没有额外帮助；最后，有额外帮助。选择紧凑而高效的Orca2 7B模型展示了智能使用计算资源。实证结果表明，在RAG下，初始语言模型的性能有显著提高，尤其是在提示增强器的辅助下。在不同编码间的一致性突出了使用语言模型生成的查询的有效性。BERT引入UMAP进一步简化了文档检索，同时保持了强大的结果。</p>
<h4 id="_136">一句话总结：</h4>
<p>本研究通过检索增强生成技术，结合高效的Orca2模型和UMAP，显著提升了高级语言模型在文档检索中的准确性和效率。</p>
<hr />
<h2 id="cadren-contextual-anchor-driven-relational-network-for-controllable-cross-graphs-node-importance-estimation"><a href="http://arxiv.org/abs/2402.05135v1">CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation</a></h2>
<p>发布时间：2024-02-06</p>
<p>作者：Zijie Zhong, Yunhui Zhang, Ziyi Chang, Zengchang Qin</p>
<h4 id="_137">中文摘要：</h4>
<p>节点重要性估计（NIE）对于通过检索增强生成将外部信息整合到大型语言模型中至关重要。传统方法侧重于静态、单图特征，缺乏对新图和用户特定需求的适应性。CADReN，我们提出的方法，通过引入上下文锚点（CA）机制来解决这些局限性。这种方法使网络能够相对于CA评估节点重要性，同时考虑知识图谱（KGs）中的结构和语义特征。大量实验表明，CADReN在跨图NIE任务中实现了更好的性能，并具有零样本预测能力。CADReN在单图NIE任务上的性能也得到了验证。此外，我们还引入并开源了两个新的数据集，RIC200和WK1K，专门为跨图NIE研究设计，为该领域未来的发展提供了宝贵的资源。</p>
<h4 id="_138">一句话总结：</h4>
<p>CADReN通过引入上下文锚点机制，实现了对节点重要性的有效估计，并在跨图NIE任务中取得了显著性能提升。</p>
<hr />
<h2 id="explaining-autonomy-enhancing-human-robot-interaction-through-explanation-generation-with-large-language-models"><a href="http://arxiv.org/abs/2402.04206v1">Explaining Autonomy: Enhancing Human-Robot Interaction through Explanation Generation with Large Language Models</a></h2>
<p>发布时间：2024-02-06</p>
<p>作者：David Sobrín-Hidalgo, Miguel A. González-Santamarta, Ángel M. Guerrero-Higueras, Francisco J. Rodríguez-Lera, Vicente Matellán-Olivera</p>
<h4 id="_139">中文摘要：</h4>
<p>本文介绍了一个旨在为在人机交互（HRI）中执行动作的自主机器人生成解释的系统。在机器人领域，可解释性（封装在可解释自主机器人（XAR）的概念中）是一个日益增长的研究领域。本文描述的工作旨在利用大型语言模型（LLMs）在执行自然语言处理任务方面的能力。本研究重点关注使用此类模型结合检索增强生成（RAG）方法来解释从自主系统日志中收集的数据的可能性。此外，这项工作还提出了所提议的解释系统的形式化。它通过欧洲机器人联盟（ERL）的导航测试进行了评估，这是一个欧洲范围内的社交机器人竞赛。关于获得的结果，进行了一项验证问卷来衡量技术用户对解释质量的看法。实验期间获得的结果突出了LLMs在实现机器人解释能力方面的潜在效用。</p>
<h4 id="_140">一句话总结：</h4>
<p>本文提出了一种利用大型语言模型和检索增强生成方法为自主机器人动作生成解释的系统，并通过实验验证了其在提高机器人解释能力方面的潜力。</p>
<hr />
<h2 id="financial-report-chunking-for-effective-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2402.05131v3">Financial Report Chunking for Effective Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-02-05</p>
<p>作者：Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, Renyu Li</p>
<h4 id="_141">中文摘要：</h4>
<p>信息分块是检索增强生成（RAG）的关键步骤。当前研究主要集中于段落级别的分块。这种方法将所有文本视为平等，忽略了文档结构中包含的信息。我们提出了一种扩展的方法，通过超越单纯的段落级别分块，以文档的结构元素组件为主要分块依据来分块文档。将文档分解成这些构成元素创造了一种新的文档分块方式，无需调整即可获得最佳分块大小。我们引入了一个新的框架，该框架评估了基于文档理解模型标注的元素类型分块如何对检索到的信息的整体上下文和准确性做出贡献。我们还展示了这种方法如何影响RAG辅助的问答任务性能。我们的研究包括对各种元素类型、它们在有效信息检索中的作用以及它们对RAG输出质量的影响的全面分析。研究发现，基于元素类型的分块在很大程度上改善了财务报告上的RAG结果。通过这项研究，我们还能够回答如何揭示高度准确的RAG。</p>
<h4 id="_142">一句话总结：</h4>
<p>本研究提出了一种基于文档结构元素的分块方法，显著提升了检索增强生成（RAG）在财务报告等领域的性能。</p>
<hr />
<h2 id="c-rag-certified-generation-risks-for-retrieval-augmented-language-models"><a href="http://arxiv.org/abs/2402.03181v5">C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models</a></h2>
<p>发布时间：2024-02-05</p>
<p>作者：Mintong Kang, Nezihe Merve Gürel, Ning Yu, Dawn Song, Bo Li</p>
<h4 id="_143">中文摘要：</h4>
<p>尽管大型语言模型（LLMs）在众多应用中展现出令人印象深刻的性能，但它们仍存在可信度问题，如幻觉和偏差。检索增强语言模型（RAG）被提出以通过固化外部知识来提高生成内容的可信度，但其生成风险的理论理解尚未得到充分探索。在本文中，我们回答了以下问题：1）RAG是否确实能降低生成风险；2）如何为RAG和传统LLMs的生成风险提供可证明的保证；3）什么充分条件使得RAG模型能够降低生成风险。我们提出了C-RAG，这是第一个为RAG模型认证生成风险的框架。具体来说，我们为RAG模型提供了形式化风险分析，并认证了生成风险的置信上限，我们称之为形式化生成风险。我们还为测试分布转移下的通用有界风险函数提供了形式化生成风险的理论保证。我们证明，当检索模型和transformer的质量非平凡时，RAG比单个LLM实现了更低的置信生成风险。我们密集的实证结果证明了我们形式化生成风险保证在四个广泛使用的NLP数据集和四个最先进的检索模型上的稳健性和紧密性。</p>
<h4 id="_144">一句话总结：</h4>
<p>本文提出了C-RAG框架，为检索增强语言模型提供生成风险认证，证明了RAG在降低生成风险方面的有效性。</p>
<hr />
<h2 id="multi-lingual-malaysian-embedding-leveraging-large-language-models-for-semantic-representations"><a href="http://arxiv.org/abs/2402.03053v1">Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations</a></h2>
<p>发布时间：2024-02-05</p>
<p>作者：Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan</p>
<h4 id="_145">中文摘要：</h4>
<p>在这项工作中，我们对马来西亚语言模型（特别是Llama2和Mistral）在涉及正负对嵌入任务上的微调进行了全面探索。我们发布了两个针对语义相似性和检索增强生成（RAG）任务定制的模型。对于语义相似性，我们的600百万参数Llama2模型在b.cari.com.my、c.cari.com.my、马来新闻和马来西亚Twitter测试集的所有recall@k指标上均优于OpenAI的text-embedding-ada-002模型。在RAG模型领域，我们的方法在马来西亚环境下与OpenAI的text-embedding-ada-002模型具有竞争力。值得注意的是，我们的20亿参数Llama2模型在“Melayu”关键词研究论文数据集上实现了优越的Recall@5和Recall@10，并在lom.agc.gov.my数据集上表现出Recall@3、Recall@5和Recall@10的优异表现。这些发现强调了我们的微调策略的有效性，并突出了在语义相似性和RAG任务中的性能提升。所有发布的模型可在https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99找到。</p>
<h4 id="_146">一句话总结：</h4>
<p>本研究通过微调Llama2和Mistral模型，在语义相似性和检索增强生成任务上实现了对马来西亚语言模型的性能提升。</p>
<hr />
<h2 id="enhancing-textbook-question-answering-task-with-large-language-models-and-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2402.05128v2">Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-02-05</p>
<p>作者：Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal</p>
<h4 id="_147">中文摘要：</h4>
<p>教科书问答（TQA）是人工智能领域的一项具有挑战性的任务，因为其涉及到的上下文和多媒体数据的复杂性。尽管先前的研究显著提高了这一任务，但仍存在一些局限性，包括模型推理能力较弱以及无法捕捉到长上下文中的上下文信息。大型语言模型（LLMs）的引入彻底改变了人工智能领域，然而，直接应用LLMs往往会导致答案不准确。本文提出了一种方法，通过结合检索增强生成（RAG）技术和利用迁移学习来处理TQA中的跨领域场景，其中概念分布在不同的课程中，并增强推理能力。通过对LLM模型Llama-2进行监督微调和结合RAG，我们的架构优于基线，在验证集上实现了4.12%的准确率提升，在测试集上实现了9.84%的提升，针对非图表选择题。</p>
<h4 id="_148">一句话总结：</h4>
<p>本文提出了一种结合RAG技术和迁移学习的方法，显著提高了教科书问答任务中的模型推理能力和准确率。</p>
<hr />
<h2 id="list-aware-reranking-truncation-joint-model-for-search-and-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2402.02764v1">List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation</a></h2>
<p>发布时间：2024-02-05</p>
<p>作者：Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, Xueqi Cheng</p>
<h4 id="_149">中文摘要：</h4>
<p>信息检索（IR）的结果通常以候选文档的排序列表形式呈现，例如人类使用的网络搜索和用于大型语言模型（LLMs）的检索增强生成。列表感知检索旨在捕捉列表级别的上下文特征，以返回更好的列表，主要包括重新排序和截断。重新排序精细地重新评分列表中的文档。截断动态地确定排序列表的截止点，以在整体相关性和避免无关文档中的错误信息之间达到平衡。先前的研究将它们视为两个独立任务并分别建模。然而，这种分离并不理想。首先，很难在两个任务之间共享排名列表的上下文信息。其次，独立的管道通常会遇到错误累积问题，其中重新排序阶段的小错误可以极大地影响截断阶段。为了解决这些问题，我们提出了一种重新排序-截断联合模型（GenRT），它可以同时执行这两个任务。GenRT通过基于编码器-解码器架构的生成范式整合重新排序和截断。我们还设计了新颖的损失函数，用于联合优化，使模型学习这两个任务。通过联合模型共享参数有助于充分利用两个任务的共同建模信息。此外，这两个任务同时执行并协同优化，以解决独立阶段之间的错误累积问题。在公共学习排序基准和开放域问答任务上的实验表明，我们的方法在重新排序和截断任务上均达到了SOTA性能，适用于网络搜索和检索增强的LLMs。</p>
<h4 id="_150">一句话总结：</h4>
<p>提出了一种名为GenRT的联合模型，通过同时优化重新排序和截断任务，显著提升了信息检索的列表质量。</p>
<hr />
<h2 id="retrieval-augmented-score-distillation-for-text-to-3d-generation"><a href="http://arxiv.org/abs/2402.02972v2">Retrieval-Augmented Score Distillation for Text-to-3D Generation</a></h2>
<p>发布时间：2024-02-05</p>
<p>作者：Junyoung Seo, Susung Hong, Wooseok Jang, Inès Hyeonsu Kim, Minseop Kwak, Doyup Lee, Seungryong Kim</p>
<h4 id="_151">中文摘要：</h4>
<p>文本到3D生成技术通过结合强大的2D扩散模型取得了显著的成功，但不足的3D先验知识也导致了3D几何的不一致性。近年来，随着大规模多视角数据集的发布，在多视角数据集上微调扩散模型成为解决3D不一致性问题的主要方法。然而，与2D数据相比，3D数据的有限质量和多样性给研究带来了根本性的困难。为了规避这些权衡，我们探索了一种针对评分蒸馏的检索增强方法，称为ReDream。我们假设通过在优化过程中直接使用语义相关的资产，可以充分利用2D扩散模型的表达能力和3D资产几何一致性。为此，我们引入了一种基于检索的文本到3D生成质量增强的新框架。我们利用检索到的资产将其几何先验纳入变分目标，并调整扩散模型的2D先验以实现视角一致性，从而在生成的场景的几何和保真度方面取得了显著的提升。我们进行了广泛的实验来证明ReDream在几何一致性提高的同时展现出更优的质量。项目页面可在https://ku-cvlab.github.io/ReDream/找到。</p>
<h4 id="_152">一句话总结：</h4>
<p>ReDream通过引入检索增强方法，有效提升了文本到3D生成中几何一致性和生成质量。</p>
<hr />
<h2 id="improving-assessment-of-tutoring-practices-using-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2402.14594v1">Improving Assessment of Tutoring Practices using Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-02-04</p>
<p>作者：Zifei FeiFei Han, Jionghao Lin, Ashish Gurung, Danielle R. Thomas, Eason Chen, Conrad Borchers, Shivang Gupta, Kenneth R. Koedinger</p>
<h4 id="_153">中文摘要：</h4>
<p>一对一辅导是一种有效的教学方法，可以提升学习效果，但其有效性依赖于辅导教师的技能。新晋的数学辅导教师往往更重视内容指导，而忽视了如社会情感学习等方面。社会情感学习能够促进公平和包容，并帮助学生建立良好的人际关系，这对学生的全面发展至关重要。准确且高效地评估辅导教师的技能可以推动定制化辅导培训项目的发展。然而，在实时辅导过程中评估新晋辅导教师的技能仍然具有挑战性，因为这通常需要专家的参与。为了应对这一挑战，本研究旨在利用生成预训练转换器（GPT），如GPT-3.5和GPT-4模型，自动评估辅导教师使用社会情感辅导策略的能力。此外，本研究还报告了在实时和大规模应用这些模型进行自动评估的财务维度和考虑因素。当前研究考察了四种提示策略：两种基本的零样本提示策略、思维树提示和基于检索增强生成器（RAG）的提示。结果表明，RAG提示在准确性（通过生成评估文本中的幻觉程度和正确性来评估）和财务成本方面优于其他评估策略。这些发现为开发个性化的辅导培训干预措施，以增强辅导学习的教育效果提供了信息。</p>
<h4 id="_154">一句话总结：</h4>
<p>本研究利用GPT模型自动评估辅导教师的社会情感辅导能力，并发现基于RAG的提示策略在准确性和成本效益方面表现最佳。</p>
<hr />
<h2 id="how-well-do-llms-cite-relevant-medical-references-an-evaluation-framework-and-analyses"><a href="http://arxiv.org/abs/2402.02008v1">How well do LLMs cite relevant medical references? An evaluation framework and analyses</a></h2>
<p>发布时间：2024-02-03</p>
<p>作者：Kevin Wu, Eric Wu, Ally Cassasola, Angela Zhang, Kevin Wei, Teresa Nguyen, Sith Riantawan, Patricia Shi Riantawan, Daniel E. Ho, James Zou</p>
<h4 id="_155">中文摘要：</h4>
<p>大型语言模型（LLMs）目前被用于回答各种临床领域的医学问题。特别是，最近表现优异的商业LLMs也能够引用来源来支持其回答。在本文中，我们提出一个问题：LLMs生成的来源是否真正支持它们所提出的论断？为了回答这个问题，我们提出了三个贡献。首先，由于专家医学注释是可扩展评估中的昂贵且耗时的瓶颈，我们证明了GPT-4在验证来源相关性方面非常准确，88%的时间与一组医学医生的意见一致。其次，我们开发了一个端到端、自动化的流程，称为“SourceCheckup”，并使用它在一个包含1200个生成问题的数据集上评估了五个表现优异的LLMs，总共有超过40K对陈述和来源。有趣的是，我们发现大约50%到90%的LLM回答没有得到它们提供的来源的充分支持。我们还评估了带有检索增强生成（RAG）的GPT-4，发现即使如此，大约30%的个别陈述没有得到支持，而其回答中近一半没有得到充分支持。第三，我们开源了我们的医学问题精选数据集和专家注释，以供未来的评估使用。鉴于LLM发展的快速步伐和错误或过时医学信息可能带来的潜在危害，了解和量化它们生成相关、可信医学参考文献的能力同样至关重要。</p>
<h4 id="_156">一句话总结：</h4>
<p>本文通过验证大型语言模型在医学领域生成回答的来源支持度，揭示了其生成医学参考文献的能力存在局限，并强调了评估和改进这一能力的重要性。</p>
<hr />
<h2 id="locally-adaptive-quantization-for-streaming-vector-search"><a href="http://arxiv.org/abs/2402.02044v1">Locally-Adaptive Quantization for Streaming Vector Search</a></h2>
<p>发布时间：2024-02-03</p>
<p>作者：Cecilia Aguerrebere, Mark Hildebrand, Ishwar Singh Bhati, Theodore Willke, Mariano Tepper</p>
<h4 id="_157">中文摘要：</h4>
<p>检索与给定查询最相似的向量嵌入在大量向量集合中一直是无数现实应用的关键组成部分。最近提出的检索增强生成是其中最突出的例子之一。对于许多这些应用，数据库随着时间的推移通过插入新数据并删除过时数据而演变。在这些情况下，检索问题被称为流式相似性搜索。虽然局部自适应向量量化（LVQ），一种高效的向量压缩方法，对于非演化的数据库提供了最先进的搜索性能，但其在线性设置中的有用性尚未得到证实。在这项工作中，我们研究了LVQ在流式相似性搜索中的应用。为了支持我们的评估，我们引入了LVQ的两个改进：Turbo LVQ和multi-means LVQ，分别通过28%和27%的比例提高了其搜索性能。我们的研究表明，LVQ及其新变体能够实现快速的向量搜索，对于相同分布的数据，其性能优于最接近的竞争对手高达9.4倍，在数据分布变化（即数据的统计分布随时间变化）的挑战性场景下，性能优于8.8倍。我们将我们的贡献作为可扩展向量搜索的一部分发布，这是一个用于高性能相似性搜索的开源库。</p>
<h4 id="_158">一句话总结：</h4>
<p>本研究通过引入Turbo LVQ和multi-means LVQ改进，显著提升了局部自适应向量量化在流式相似性搜索中的性能，实现了快速且高效的向量嵌入检索。</p>
<hr />
<h2 id="retrieval-augmented-end-to-end-spoken-dialog-models"><a href="http://arxiv.org/abs/2402.01828v1">Retrieval Augmented End-to-End Spoken Dialog Models</a></h2>
<p>发布时间：2024-02-02</p>
<p>作者：Mingqiu Wang, Izhak Shafran, Hagen Soltau, Wei Han, Yuan Cao, Dian Yu, Laurent El Shafey</p>
<h4 id="_159">中文摘要：</h4>
<p>我们最近开发了一种联合语音和语言模型SLM，该模型融合了一个预训练的基础语音模型和一个大型语言模型（LLM），同时保留了预训练LLM固有的上下文学习能力。在本文中，我们将SLM应用于语音对话应用，其中对话状态直接从音频信号中推断出来。面向任务的对话通常包含领域特定的实体，例如餐厅、酒店、火车站和城市名称，这些实体难以识别，但对于下游应用却至关重要。受RAG（检索增强生成）范式启发，我们提出了一种检索增强SLM（ReSLM），以克服这一弱点。我们首先训练一个语音检索器来检索音频中提到的文本实体。然后，检索到的实体被添加为文本输入到底层的SLM中，以偏置模型预测。我们在语音MultiWoz任务（DSTC-11挑战）上评估了ReSLM，发现这种检索增强提高了模型性能，实现了联合目标准确率（38.6% vs 32.7%）、槽位错误率（20.6% vs 24.8%）和ASR词错误率（5.5% vs 6.7%）。虽然我们的方法在对话状态跟踪中得到了演示，但它广泛适用于需要上下文信息或领域特定实体的其他语音任务，例如具有偏置能力的上下文ASR。</p>
<h4 id="_160">一句话总结：</h4>
<p>我们提出了一种检索增强的SLM模型，通过检索音频中的文本实体来提高语音对话应用中的模型性能。</p>
<hr />
<h2 id="continual-learning-for-large-language-models-a-survey"><a href="http://arxiv.org/abs/2402.01364v2">Continual Learning for Large Language Models: A Survey</a></h2>
<p>发布时间：2024-02-02</p>
<p>作者：Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, Gholamreza Haffari</p>
<h4 id="_161">中文摘要：</h4>
<p>大型语言模型（LLMs）由于规模庞大，导致训练成本高昂，因此不适宜频繁重新训练。然而，为了赋予LLMs新技能并使其与快速发展的人类知识保持同步，更新是必要的。本文综述了关于LLMs持续学习的最新研究。由于LLMs的独特性质，我们采用了一种新颖的多阶段分类方案来整理持续学习技术，包括持续预训练、指令调整和对齐。我们将LLMs的持续学习与较小模型中使用的简单适应方法，以及其他增强策略（如检索增强生成和模型编辑）进行了对比。此外，基于对基准和评估的讨论，我们确定了这一关键任务所面临的几个挑战和未来的研究方向。</p>
<h4 id="_162">一句话总结：</h4>
<p>本文综述了LLMs持续学习的最新研究，并探讨了其面临的挑战和未来的研究方向。</p>
<hr />
<h2 id="corpuslm-towards-a-unified-language-model-on-corpus-for-knowledge-intensive-tasks"><a href="http://arxiv.org/abs/2402.01176v2">CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks</a></h2>
<p>发布时间：2024-02-02</p>
<p>作者：Xiaoxi Li, Zhicheng Dou, Yujia Zhou, Fangchao Liu</p>
<h4 id="_163">中文摘要：</h4>
<p>大型语言模型（LLMs）在各个领域受到了广泛关注，但它们在知识密集型（KI）任务中容易出现幻觉。为了解决这个问题，检索增强生成（RAG）已成为提高事实准确性的流行解决方案。然而，传统的检索模块通常依赖于大型文档索引，并且与生成任务脱节。随着生成检索（GR）的出现，语言模型可以通过直接生成文档标识符（DocIDs）进行检索，从而在检索任务中提供更优的性能。然而，GR与下游任务之间的潜在关系尚未得到探索。在本文中，我们提出了CorpusLM，这是一个统一的语言模型，它通过整合生成检索、闭卷生成和RAG，利用外部语料库来处理各种知识密集型任务，并通过统一的贪婪解码过程实现。我们设计了以下机制以促进有效的检索和生成，并提高KI任务的端到端有效性：（1）我们开发了一种以排名为导向的DocID列表生成策略，通过直接从DocID排名列表中学习来优化GR，从而提高检索质量。（2）我们设计了一种连续的DocIDs-References-Answer生成策略，以促进有效且高效的RAG。（3）我们采用精心设计的无监督DocID理解任务，以理解DocID语义及其与下游任务的相关性。我们使用广泛使用的KILT基准测试评估了我们的方法，并使用了两种骨干模型变体，即T5和Llama2。实验结果表明，我们的模型在检索和下游任务中都表现出优异的性能。</p>
<h4 id="_164">一句话总结：</h4>
<p>本文提出的CorpusLM通过整合生成检索、闭卷生成和RAG，显著提高了知识密集型任务中的检索和生成效果。</p>
<hr />
<h2 id="litllm-a-toolkit-for-scientific-literature-review"><a href="http://arxiv.org/abs/2402.01788v1">LitLLM: A Toolkit for Scientific Literature Review</a></h2>
<p>发布时间：2024-02-02</p>
<p>作者：Shubham Agarwal, Issam H. Laradji, Laurent Charlin, Christopher Pal</p>
<h4 id="_165">中文摘要：</h4>
<p>进行科学论文的文献综述对于理解研究、其局限性以及在此基础上构建现有工作至关重要。这是一项繁琐的任务，使得自动文献综述生成器颇具吸引力。不幸的是，许多使用大型语言模型（LLMs）生成此类综述的现有作品存在重大局限性。它们倾向于产生幻觉——生成非实际信息——并忽略它们未接受过训练的最新研究。为了解决这些局限性，我们提出了一套基于检索增强生成（RAG）原则的工具包，该工具包利用LLMs进行专门的提示和指导技术。我们的系统首先通过使用现成的LLMs将用户提供的摘要总结成关键词来启动网络搜索以检索相关论文。作者可以通过补充相关论文或关键词来增强搜索，从而贡献于定制化的检索过程。其次，系统根据用户提供的摘要重新排序检索到的论文。最后，基于重新排序的结果和摘要生成相关工作部分。与传统方法相比，文献综述的时间和精力有显著减少，确立了我们的工具包作为高效替代品的地位。我们的开源工具包可在https://github.com/shubhamagarwal92/LitLLM和Huggingface space（https://huggingface.co/spaces/shubhamagarwal92/LitLLM）获取，视频演示在https://youtu.be/E2ggOZBAFw0。</p>
<h4 id="_166">一句话总结：</h4>
<p>该工具包利用LLMs和RAG原理，通过自动检索和重新排序相关文献，显著提高了文献综述的效率和准确性。</p>
<hr />
<h2 id="health-llm-personalized-retrieval-augmented-disease-prediction-system"><a href="http://arxiv.org/abs/2402.00746v6">Health-LLM: Personalized Retrieval-Augmented Disease Prediction System</a></h2>
<p>发布时间：2024-02-01</p>
<p>作者：Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua, Suiyuan Zhu, Yanda Meng, Zhenting Wang, Mengnan Du, Yongfeng Zhang</p>
<h4 id="_167">中文摘要：</h4>
<p>近年来，人工智能（AI）的快速发展，尤其是大型语言模型（LLMs）的进步，显著推动了医疗保健应用的发展，并展示了在智能医疗治疗中的潜力。然而，存在诸如大量数据量和不一致的症状表征标准等明显挑战，阻碍了医疗保健AI系统与个体患者需求的充分整合。为了促进专业化和个性化的医疗保健，我们提出了一种创新框架，Health-LLM，该框架结合了大规模特征提取和医学知识权衡评分。与传统健康管理系统相比，我们的系统具有三个主要优势：（1）它将健康报告和医学知识整合到一个大型模型中，向大型语言模型提出相关问题以进行疾病预测；（2）它利用检索增强生成（RAG）机制来增强特征提取；（3）它包含一个半自动化的特征更新框架，可以合并和删除特征以提高疾病预测的准确性。我们在大量健康报告中进行了实验，以评估Health-LLM系统的有效性。结果表明，所提出的系统优于现有系统，并有可能显著推进疾病预测和个性化健康管理。</p>
<h4 id="_168">一句话总结：</h4>
<p>该研究提出了一种名为Health-LLM的创新框架，通过结合大规模特征提取和医学知识权衡评分，有效提升了疾病预测和个性化健康管理的能力。</p>
<hr />
<h2 id="dont-hallucinate-abstain-identifying-llm-knowledge-gaps-via-multi-llm-collaboration"><a href="http://arxiv.org/abs/2402.00367v2">Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration</a></h2>
<p>发布时间：2024-02-01</p>
<p>作者：Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, Yulia Tsvetkov</p>
<h4 id="_169">中文摘要：</h4>
<p>尽管人们努力扩展大型语言模型（LLMs）的知识，但由于知识的不断演变，知识差距——即LLMs中缺失或过时的信息——可能始终存在。在这项工作中，我们研究了识别LLMs知识差距的方法，并在存在知识差距时避免回答问题。我们首先将现有的通过微调/提示进行模型校准或适应的方法进行改编，并分析其避免生成低置信度输出的能力。受限于自我反思不足和对保留集过度依赖的失败，我们提出了两种基于模型协作的新方法，即LLMs通过合作或竞争的方式探测其他LLMs中的知识差距。在四个具有不同知识领域的QA任务上，使用三个LLMs进行的广泛实验表明，揭示LLMs知识差距的合作和竞争方法在避免回答的准确率上比最强的基线提高了高达19.3%。进一步的分析揭示，我们提出的机制可以帮助识别检索增强中的失败案例，并定位多跳推理中的知识差距。</p>
<h4 id="_170">一句话总结：</h4>
<p>本研究提出了一种基于模型协作的方法，通过合作或竞争的方式揭示大型语言模型中的知识差距，显著提高了避免回答问题的准确率。</p>
<hr />
<h2 id="hiqa-a-hierarchical-contextual-augmentation-rag-for-massive-documents-qa"><a href="http://arxiv.org/abs/2402.01767v1">HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA</a></h2>
<p>发布时间：2024-02-01</p>
<p>作者：Xinyue Chen, Pengyu Gao, Jiangjiang Song, Xiaoyang Tan</p>
<h4 id="_171">中文摘要：</h4>
<p>随着利用外部工具的语言模型代理的快速进化，在利用补充文档和检索增强生成（RAG）方法进行问答（QA）的方法上取得了显著进展。这一进步提高了语言模型的响应质量，并减轻了幻觉的出现。然而，当面对大量难以区分的文档时，这些方法在检索准确性方面表现出局限性，这在其实际应用中带来了显著的挑战。为了应对这些新兴的挑战，我们提出了HiQA，这是一个集成了级联元数据到内容以及多路径检索机制的先进的多文档问答（MDQA）框架。我们还发布了一个名为MasQA的基准，用于评估和研究MDQA。最后，HiQA在多文档环境中展示了最先进的性能。</p>
<h4 id="_172">一句话总结：</h4>
<p>HiQA是一个集成了级联元数据和多路径检索机制的多文档问答框架，显著提升了语言模型在多文档环境中的问答性能。</p>
<hr />
<h2 id="towards-ai-assisted-synthesis-of-verified-dafny-methods"><a href="http://arxiv.org/abs/2402.00247v2">Towards AI-Assisted Synthesis of Verified Dafny Methods</a></h2>
<p>发布时间：2024-02-01</p>
<p>作者：Md Rakib Hossain Misu, Cristina V. Lopes, Iris Ma, James Noble</p>
<h4 id="_173">中文摘要：</h4>
<p>大型语言模型在许多领域，包括编程领域，展现出巨大的潜力。然而，承诺容易做出，但难以实现，语言模型往往无法履行承诺，生成错误的代码。确保模型诚实的一个有前景的方法是结合形式化验证：生成程序的规范以及代码，以便代码可以根据规范被证明是正确的。不幸的是，现有的大型语言模型在验证编程方面表现出严重的不足。在本文中，我们展示了如何提高两个预训练模型在Dafny验证感知语言中的熟练度。使用MBPP数据集中的178个问题，我们提示两个当代模型（GPT-4和PaLM-2）合成Dafny方法。我们使用了三种不同类型的提示：一个直接的无关上下文提示；一个包含方法签名和测试用例的签名提示，以及一个思维链（CoT）提示，它将问题分解为步骤，并包括检索增强生成的示例问题和解决方案。我们的结果表明，GPT-4在这些任务上的表现优于PaLM-2，并且两种模型在使用检索增强生成的CoT提示时表现最佳。GPT-4能够为58%的问题生成经过验证、人工评估的Dafny方法，然而，GPT-4在无上下文提示下仅能处理19%的问题，而对于签名提示，则更少（10%）。因此，我们能够为MBPP问题贡献153个验证的Dafny解决方案，其中50个是我们手动编写的，103个是由GPT-4合成的。我们的结果表明，形式化程序验证的好处现在已在大规模语言模型生成代码的范畴内触手可及。</p>
<h4 id="_174">一句话总结：</h4>
<p>本文通过结合形式化验证和大型语言模型，显著提高了模型在验证编程方面的能力，为代码生成大型语言模型带来了形式化验证的好处。</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
    
  </body>
</html>