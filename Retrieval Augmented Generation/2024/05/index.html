
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../06/">
      
      
        <link rel="next" href="../04/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.31">
    
    
      
        <title>2024-05(123) - ArXiv Tracker</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="ArXiv Tracker" class="md-header__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ArXiv Tracker
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2024-05(123)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Emission%20Trading%20System/2024/06/" class="md-tabs__link">
          
  
  Emission Trading System

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../08/" class="md-tabs__link">
          
  
  Retrieval Augmented Generation

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="ArXiv Tracker" class="md-nav__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ArXiv Tracker
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Emission Trading System
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Emission Trading System
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2016/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-02(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Retrieval Augmented Generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Retrieval Augmented Generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-08(17)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(130)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(158)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2024-05(123)
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-04(91)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-03(72)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(89)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(53)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(36)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-11(42)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(53)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(23)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-08(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-07(14)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(15)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(38)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-04(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-02(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-01(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(9)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(20)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-09(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-08(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-07(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-05(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-04(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-03(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-01(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-12(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-09(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-08(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-07(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-06(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-09(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-06(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-04(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-12(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-05(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-04(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-12(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-04(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2015/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2012/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2012-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2009/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2009-03(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="retrieval-augmented-generation-202405">Retrieval Augmented Generation - 2024年05月</h1>
<h2 id="rag-does-not-work-for-enterprises"><a href="http://arxiv.org/abs/2406.04369v1">RAG Does Not Work for Enterprises</a></h2>
<p>发布时间：2024-05-31</p>
<p>作者：Tilmann Bruckhaus</p>
<h4 id="_1">中文摘要：</h4>
<p>检索增强生成（RAG）通过结合知识检索来提高大型语言模型输出的准确性和相关性。然而，在企业中实施RAG面临着数据安全、准确性、可扩展性和集成等方面的挑战。本文探讨了企业RAG的独特需求，调查了当前的方法和局限性，并讨论了语义搜索、混合查询和优化检索方面的潜在进展。它提出了一种评估框架来验证企业RAG解决方案，包括定量测试、定性分析、消融研究和行业案例研究。该框架旨在帮助证明专门设计的RAG架构能够在企业级安全、合规性和集成方面提供准确性和相关性的提升。文章最后讨论了企业部署的启示、局限性和未来的研究方向。研究人员与行业合作伙伴之间的紧密合作可能加速检索增强生成技术的发展和部署。</p>
<h4 id="_2">一句话总结：</h4>
<p>本文探讨了企业级检索增强生成（RAG）的实施挑战、现有方法及其在语义搜索、混合查询和优化检索方面的潜在进展。</p>
<hr />
<h2 id="enhancing-noise-robustness-of-retrieval-augmented-language-models-with-adaptive-adversarial-training"><a href="http://arxiv.org/abs/2405.20978v1">Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training</a></h2>
<p>发布时间：2024-05-31</p>
<p>作者：Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, Ruifeng Xu</p>
<h4 id="_3">中文摘要：</h4>
<p>大型语言模型（LLMs）展现出强大的能力，但同时也面临着幻觉、过时知识和不可追踪的推理过程等挑战。检索增强生成（RAG）作为一种有前景的解决方案，通过整合外部数据库中的知识来缓解这些挑战。然而，不当检索到的段落可能会阻碍LLMs生成全面和高质量的响应。先前关于检索噪声鲁棒性的RAG研究通常局限于有限种类的噪声类型，这与现实世界的检索环境不符，限制了其实际应用。在本研究中，我们首先调查了检索噪声，并将它们分为三种不同的类型，反映了现实世界的环境。我们分析了这些各种检索噪声对LLMs鲁棒性的影响。随后，我们提出了一种名为检索增强自适应对抗训练（RAAT）的新颖RAG方法。RAAT利用自适应对抗训练来动态调整模型训练过程以应对检索噪声。同时，它采用多任务学习以确保模型能够内部识别噪声环境。大量实验表明，使用RAAT训练的LLaMA-2 7B模型在多种噪声条件下在F1和EM分数上都有显著提升。为了确保可重复性，我们在以下网址发布了我们的代码和数据：https://github.com/calubkk/RAAT。</p>
<h4 id="_4">一句话总结：</h4>
<p>本研究提出了一种名为RAAT的检索增强自适应对抗训练方法，有效提升了大型语言模型在检索噪声环境下的鲁棒性和生成质量。</p>
<hr />
<h2 id="retrieval-meets-reasoning-even-high-school-textbook-knowledge-benefits-multimodal-reasoning"><a href="http://arxiv.org/abs/2405.20834v1">Retrieval Meets Reasoning: Even High-school Textbook Knowledge Benefits Multimodal Reasoning</a></h2>
<p>发布时间：2024-05-31</p>
<p>作者：Cheng Tan, Jingxuan Wei, Linzhuang Sun, Zhangyang Gao, Siyuan Li, Bihui Yu, Ruifeng Guo, Stan Z. Li</p>
<h4 id="_5">中文摘要：</h4>
<p>大型语言模型配备检索增强生成（RAG）技术，是一个旨在通过利用外部知识库来增强回答能力的快速发展的领域。尽管RAG与仅使用语言模型的结合已被广泛研究，但其适应多模态视觉-语言模型仍处于起步阶段。超越单纯的答案生成，多模态RAG的主要目标是培养模型对相关查询进行推理的能力。为此，我们引入了一个名为RMR（检索遇推理）的新型多模态RAG框架。RMR框架采用双模态检索模块来识别最相关的问答对，这些问答对随后作为多模态推理过程的支架。这种无需训练的方法不仅鼓励模型深入参与检索内容内固有的推理过程，而且促进了精确且易于解释的答案的生成。令人惊讶的是，仅使用从小学和中学科学课程中收集的ScienceQA数据集，RMR显著提高了各种视觉-语言模型在包括A-OKVQA、MMBench和SEED在内的多个基准数据集上的性能。这些结果突显了我们多模态检索和推理机制在提高视觉-语言模型推理能力方面的巨大潜力。</p>
<h4 id="_6">一句话总结：</h4>
<p>RMR框架通过结合检索和推理，显著提升了视觉-语言模型的推理能力。</p>
<hr />
<h2 id="phantom-general-trigger-attacks-on-retrieval-augmented-language-generation"><a href="http://arxiv.org/abs/2405.20485v1">Phantom: General Trigger Attacks on Retrieval Augmented Language Generation</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Harsh Chaudhari, Giorgio Severi, John Abascal, Matthew Jagielski, Christopher A. Choquette-Choo, Milad Nasr, Cristina Nita-Rotaru, Alina Oprea</p>
<h4 id="_7">中文摘要：</h4>
<p>检索增强生成（RAG）扩展了现代大型语言模型（LLMs）在聊天机器人应用中的功能，使开发者能够在不进行昂贵训练或微调的情况下，适应和个性化LLM的输出。RAG系统使用外部知识数据库检索与给定查询最相关的文档，并将这些上下文提供给LLM生成器。虽然RAG在许多应用中实现了令人印象深刻的效用，但其用于实现个性化生成模型的应用引入了新的安全风险。在这项工作中，我们提出了一种针对RAG系统的新攻击面，攻击者通过在其知识数据库中注入单个恶意文档来破坏受害者的RAG系统。我们设计了针对RAG增强的LLMs的通用两步攻击框架Phantom。第一步涉及制作一个毒化文档，该文档仅在存在对抗触发器（作为后门的特定单词序列）时，才会被RAG系统在top-k结果中检索到。第二步，毒化文档中的特别设计的对抗字符串在LLM生成器中触发各种对抗攻击，包括拒绝服务、声誉损害、隐私侵犯和有害行为。我们在多个LLM架构上展示了我们的攻击，包括Gemma、Vicuna和Llama。</p>
<h4 id="_8">一句话总结：</h4>
<p>本文提出了一种针对RAG增强的LLMs的新型攻击框架，通过注入恶意文档实现对个性化生成模型的安全威胁。</p>
<hr />
<h2 id="hallucination-free-assessing-the-reliability-of-leading-ai-legal-research-tools"><a href="http://arxiv.org/abs/2405.20362v1">Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Varun Magesh, Faiz Surani, Matthew Dahl, Mirac Suzgun, Christopher D. Manning, Daniel E. Ho</p>
<h4 id="_9">中文摘要：</h4>
<p>法律实践中，包含人工智能（AI）的产品数量急剧增加。这些工具旨在协助完成各种核心法律任务，从案例法的搜索和摘要到文件起草。然而，这些工具中使用的的大型语言模型容易“幻觉”，即制造虚假信息，这使得它们在高风险领域中的使用存在风险。最近，某些法律研究提供商宣称，如检索增强生成（RAG）等方法可以“消除”（Casetext, 2023）或“避免”（Thomson Reuters, 2023）幻觉，或保证“无幻觉”的法律引用（LexisNexis, 2023）。由于这些系统的封闭性，系统地评估这些说法具有挑战性。在本文中，我们设计和报告了第一个基于AI驱动的法律研究工具的预注册实证评估。我们发现，提供商的说法被夸大了。虽然与通用聊天机器人（GPT-4）相比，幻觉减少了，但我们发现LexisNexis（Lexis+ AI）和Thomson Reuters（Westlaw AI-Assisted Research and Ask Practical Law AI）开发的AI研究工具各有17%至33%的时间出现幻觉。我们还记录了系统在响应性和准确性方面的显著差异。我们的文章做出了四个主要贡献。首先，它是第一个评估和报告基于RAG的专有法律AI工具性能的研究。其次，它引入了一个全面的、预注册的数据集，用于识别和理解这些系统的漏洞。第三，它提出了一个清晰的类型学，用于区分幻觉和准确的法律回应。最后，它提供了证据，以告知法律专业人士在监督和验证AI输出时的责任，这对于负责任地将AI融入法律仍是一个核心的开放性问题。</p>
<h4 id="_10">一句话总结：</h4>
<p>本文对基于RAG的法律AI工具进行了实证评估，发现其“无幻觉”的宣称被夸大，并提出了对法律专业人士监督AI输出的责任建议。</p>
<hr />
<h2 id="retrieval-augmented-structured-generation-business-document-information-extraction-as-tool-use"><a href="http://arxiv.org/abs/2405.20245v1">Retrieval Augmented Structured Generation: Business Document Information Extraction As Tool Use</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Franz Louis Cesista, Rui Aguiar, Jason Kim, Paolo Acilo</p>
<h4 id="_11">中文摘要：</h4>
<p>商业文档信息提取（BDIE）是将一团非结构化信息（如原始文本、扫描文档等）转换为下游系统可以解析和使用结构化格式的过程。它主要包括两个任务：关键信息提取（KIE）和行项目识别（LIR）。在本文中，我们提出BDIE最好被建模为一个工具使用问题，其中工具就是这些下游系统。然后，我们提出了检索增强结构化生成（RASG），这是一个针对BDIE的全新通用框架，在BDIE基准测试中，该框架在KIE和LIR任务上均达到了最先进（SOTA）的结果。本文的贡献有三点：（1）通过消融基准测试，我们展示了带有RASG的大型语言模型（LLMs）在BDIE基准测试中已经与或超过了不带RASG的当前最先进的（SOTA）大型多模态模型（LMMs）。（2）我们提出了一种新的行项目识别指标类别，即通用行项目识别指标（GLIRM），与现有的如ANLS*、DocILE和GriTS等指标相比，更符合实际BDIE应用场景。（3）我们提供了一种启发式算法，用于在不使用视觉编码器的情况下回算预测行项目和表格的边界框。最后，我们声称，尽管LMMs有时可能提供微小的性能提升，但在实际应用和BDIE的限制下，LLMs + RASG通常更优越。</p>
<h4 id="_12">一句话总结：</h4>
<p>本文提出了一种基于检索增强结构化生成的商业文档信息提取框架，显著提升了关键信息提取和行项目识别的性能。</p>
<hr />
<h2 id="gnn-rag-graph-neural-retrieval-for-large-language-model-reasoning"><a href="http://arxiv.org/abs/2405.20139v1">GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Costas Mavromatis, George Karypis</p>
<h4 id="_13">中文摘要：</h4>
<p>知识图谱（KGs）以三元组（头，关系，尾）的形式表示人类编造的事实知识，这些三元组共同构成一个图。基于知识图谱的问答（KGQA）是回答自然问题的任务，其推理过程基于KG提供的信息。大型语言模型（LLMs）是问答任务中的最先进模型，因为它们具有理解自然语言的非凡能力。另一方面，图神经网络（GNNs）在KGQA中得到了广泛的应用，因为它们可以处理存储在KG中的复杂图信息。在这项工作中，我们引入了GNN-RAG，这是一种结合LLMs的语言理解能力和GNNs的推理能力的新方法，采用检索增强生成（RAG）风格。首先，一个GNN在密集的KG子图上进行推理，以检索给定问题的答案候选。其次，提取KG中连接问题实体和答案候选的最短路径，以表示KG推理路径。提取的路径被转化为语言描述，并作为RAG中LLM推理的输入。在我们的GNN-RAG框架中，GNN作为密集子图推理器来提取有用的图信息，而LLM利用其自然语言处理能力进行最终的KGQA。此外，我们开发了一种检索增强（RA）技术，以进一步通过GNN-RAG提升KGQA性能。实验结果表明，GNN-RAG在两个广泛使用的KGQA基准（WebQSP和CWQ）中实现了最先进的性能，其性能超过了或与具有70亿参数的调优LLM GPT-4相当。此外，GNN-RAG在多跳和多实体问题上表现出色，在答案F1上比竞争方法高出8.9-15.5个百分点。</p>
<h4 id="_14">一句话总结：</h4>
<p>GNN-RAG通过结合图神经网络和大型语言模型的能力，实现了在知识图谱问答任务上的最先进性能。</p>
<hr />
<h2 id="similarity-is-not-all-you-need-endowing-retrieval-augmented-generation-with-multi-layered-thoughts"><a href="http://arxiv.org/abs/2405.19893v1">Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Chunjing Gan, Dan Yang, Binbin Hu, Hanxiao Zhang, Siyuan Li, Ziqi Liu, Yue Shen, Lin Ju, Zhiqiang Zhang, Jinjie Gu, Lei Liang, Jun Zhou</p>
<h4 id="_15">中文摘要：</h4>
<p>近年来，大型语言模型（LLMs）在各个领域取得了显著成就。然而，知识更新的不及时性和成本，以及LLMs的幻觉问题，限制了它们在知识密集型任务中的应用，而检索增强生成（RAG）可以提供帮助。尽管如此，现有的检索增强模型通常使用相似性作为查询和文档之间的桥梁，并遵循检索后阅读的程序。在这项工作中，我们认为相似性并非总是万能的，完全依赖相似性有时会降低检索增强生成的性能。为此，我们提出了MetRag，一个多层思想增强的检索增强生成框架。首先，除了现有的以相似性为导向的思想之外，我们采纳了一个小规模效用模型，该模型从LLM中获取监督以进行以效用为导向的思想，并通过全面结合相似性和以效用为导向的思想来提出一个更智能的模型。此外，鉴于检索到的文档集往往很大，单独使用它们难以捕捉它们之间的共性和特征，我们提出将LLM作为任务自适应的摘要器，以赋予检索增强生成以紧凑性为导向的思想。最后，通过前阶段的多层思想，需要LLM进行知识增强生成。在知识密集型任务上的大量实验证明了MetRag的优越性。</p>
<h4 id="_16">一句话总结：</h4>
<p>MetRag通过结合相似性和效用导向的思想，并利用LLM进行任务自适应摘要和知识增强生成，显著提升了检索增强生成的性能。</p>
<hr />
<h2 id="dataflow-guided-retrieval-augmentation-for-repository-level-code-completion"><a href="http://arxiv.org/abs/2405.19782v1">Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Wei Cheng, Yuhan Wu, Wei Hu</p>
<h4 id="_17">中文摘要：</h4>
<p>近年来，代码语言模型（LMs）在各种代码智能任务，如代码补全中的应用得到了部署。然而，在私有仓库中，预训练的LMs生成正确的补全内容仍然具有挑战性。先前的研究基于导入关系或文本相似性检索跨文件上下文，但这与补全目标的相关性不足。在本文中，我们提出了一种数据流引导的检索增强方法，称为DraCo，用于仓库级别的代码补全。DraCo通过扩展的数据流分析将私有仓库解析为代码实体，并建立它们之间的关系，形成一个特定于仓库的上下文图。每当触发代码补全时，DraCo会从特定于仓库的上下文图中精确检索相关背景知识，并生成良好的提示以查询代码LMs。此外，我们构建了一个包含更多样化补全目标的庞大Python数据集，ReccEval。我们的实验表明，DraCo在准确性和适用效率方面优于现有方法，与最先进的方法相比，平均提高了代码精确匹配3.43%和标识符F1分数3.27%。</p>
<h4 id="_18">一句话总结：</h4>
<p>DraCo通过构建特定于仓库的上下文图和精确检索相关背景知识，显著提高了代码补全的准确性和效率。</p>
<hr />
<h2 id="one-token-can-help-learning-scalable-and-pluggable-virtual-tokens-for-retrieval-augmented-large-language-models"><a href="http://arxiv.org/abs/2405.19670v3">One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Yutao Zhu, Zhaoheng Huang, Zhicheng Dou, Ji-Rong Wen</p>
<h4 id="_19">中文摘要：</h4>
<p>检索增强生成（RAG）是一种有前景的方法，可以提高大型语言模型（LLMs）生成更事实性、准确和更新的内容。现有方法要么优化提示来引导LLMs利用检索到的信息，要么直接微调LLMs以适应RAG场景。尽管微调可以带来更好的性能，但它通常通过修改参数来牺牲LLMs的通用生成能力。这种限制在实用应用中提出了挑战，尤其是在LLMs已经部署的情况下，因为参数调整可能会影响它们原有的功能。为了解决这个问题，我们提出了一种新的方法，该方法涉及学习可扩展和可插拔的虚拟标记以用于RAG。通过保持LLMs的原有参数，仅微调这些可插拔标记的嵌入，我们的方法不仅提高了LLMs的性能，还保留了它们的通用生成能力。此外，我们还设计了几个训练策略来提高我们方法的可扩展性、灵活性和泛化能力。在九个问答任务上的全面实验证明了我们方法的优势。</p>
<h4 id="_20">一句话总结：</h4>
<p>提出了一种基于可插拔虚拟标记的RAG方法，通过微调嵌入而非参数，既提升了大型语言模型性能又保留了其通用生成能力。</p>
<hr />
<h2 id="depsrag-towards-managing-software-dependencies-using-large-language-models"><a href="http://arxiv.org/abs/2405.20455v3">DepsRAG: Towards Managing Software Dependencies using Large Language Models</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Mohannad Alhanahnah, Yazan Boshmaf, Benoit Baudry</p>
<h4 id="_21">中文摘要：</h4>
<p>管理软件依赖是软件开发中的关键维护任务，并且正迅速成为一个快速增长的研究领域，尤其是在软件供应链攻击显著增加的背景下。要全面理解依赖关系并揭示关于依赖关系的隐藏属性（例如，依赖项数量、依赖链、依赖深度）需要专门的专家知识和大量的开发者努力。大型语言模型（LLMs）的最近进展使得从各种数据源检索信息以生成响应成为可能，从而为独特地管理软件依赖提供了一个新的机会。为了突出这一技术的潜力，我们提出了~\tool，这是一种概念验证的检索增强生成（RAG）方法，它将软件包的直接和传递依赖关系构建为四个流行软件生态系统中的知识图谱（KG）。DepsRAG可以通过自动生成必要的查询从KG中检索信息来回答用户关于软件依赖的问题，并使用检索到的信息增强LLMs的输入。DepsRAG还可以执行网络搜索以回答LLMs无法直接通过KG回答的问题。我们确定了DepsRAG可以提供的实际好处，并讨论了其局限性。</p>
<h4 id="_22">一句话总结：</h4>
<p>DepsRAG通过构建知识图谱并利用LLMs，为软件依赖管理提供了一种创新的检索增强生成方法。</p>
<hr />
<h2 id="is-my-data-in-your-retrieval-database-membership-inference-attacks-against-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2405.20446v2">Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Maya Anderson, Guy Amit, Abigail Goldsteen</p>
<h4 id="_23">中文摘要：</h4>
<p>检索增强生成（RAG）系统在自然语言处理领域展现出巨大的潜力。然而，它们依赖于存储在检索数据库中的数据，这些数据可能包含专有或敏感信息，从而引入了新的隐私问题。具体来说，攻击者可能通过观察RAG系统的输出，推断出某个文本段落是否出现在检索数据库中，这种攻击被称为成员身份推断攻击（Membership Inference Attack，MIA）。尽管这种威胁具有重要意义，但针对RAG系统的MIA攻击研究尚未得到充分探索。本研究通过引入一种高效且易于使用的MIA方法来填补这一空白。我们使用两个基准数据集和多种生成模型来展示我们攻击的有效性，表明在黑盒和灰盒设置中，通过创建适当的提示可以有效地确定文档在检索数据库中的成员身份。此外，我们引入了一种基于向RAG模板添加指令的初步防御策略，这在某些数据集和模型中显示出很高的有效性。我们的发现强调了在部署的RAG系统中实施安全对策以及开发更高级防御措施以保护检索数据库的隐私和安全的重要性。</p>
<h4 id="_24">一句话总结：</h4>
<p>本研究提出了一种针对RAG系统的成员身份推断攻击方法，并探讨了相应的防御策略，强调了保护检索数据库隐私和安全的重要性。</p>
<hr />
<h2 id="designing-an-evaluation-framework-for-large-language-models-in-astronomy-research"><a href="http://arxiv.org/abs/2405.20389v1">Designing an Evaluation Framework for Large Language Models in Astronomy Research</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：John F. Wu, Alina Hyk, Kiera McCormick, Christine Ye, Simone Astarita, Elina Baral, Jo Ciuca, Jesse Cranney, Anjalie Field, Kartheik Iyer, Philipp Koehn, Jenn Kotler, Sandor Kruk, Michelle Ntampaka, Charles O'Neill, Joshua E. G. Peek, Sanjib Sharma, Mikaeel Yunus</p>
<h4 id="_25">中文摘要：</h4>
<p>大型语言模型（LLMs）正在改变科学研究的方式。了解研究人员如何与这些模型互动以及科学子社区（如天文学）如何从中受益至关重要。然而，目前尚无标准来评估LLMs在天文学中的应用。因此，我们提出了一个评估研究的设计方案，用于研究天文学研究人员如何与LLMs互动。我们部署了一个Slack聊天机器人，该机器人可以通过检索增强生成（RAG）来回答用户的查询；这些回答基于arXiv上的天文学论文。我们记录并匿名化用户问题、聊天机器人回答、用户对LLM回答的点赞和踩、用户对LLM的反馈以及检索到的文档与查询的相似度分数。我们的数据收集方法将使未来能够动态评估LLMs在天文学工具中的应用。</p>
<h4 id="_26">一句话总结：</h4>
<p>本研究旨在通过实验设计评估天文学研究人员与大型语言模型（LLMs）的互动，以促进LLMs在天文学领域的应用评估。</p>
<hr />
<h2 id="generating-query-recommendations-via-llms"><a href="http://arxiv.org/abs/2405.19749v2">Generating Query Recommendations via LLMs</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Andrea Bacciu, Enrico Palumbo, Andreas Damianou, Nicola Tonellotto, Fabrizio Silvestri</p>
<h4 id="_27">中文摘要：</h4>
<p>查询推荐系统在现代搜索引擎中无处不在，帮助用户生成有效的查询以满足他们的信息需求。然而，这些系统需要大量的数据来生成好的推荐，例如大量的文档进行索引和查询日志。特别是在冷启动场景中，查询日志和用户数据不可用。查询日志的收集和维护成本高昂，并且需要复杂且耗时的级联管道来创建、组合和排序推荐。为了解决这些问题，我们将查询推荐问题框架化为一个生成任务，提出了一种名为生成查询推荐（GQR）的新方法。GQR以大型语言模型（LLM）为基础，无需训练或微调即可解决查询推荐问题。我们设计了一个提示，使LLM能够理解特定的推荐任务，即使使用单个示例。然后，我们通过提出一个利用查询日志的版本来改进我们的系统，称为检索增强GQR（RA-GQR）。RA-GQR通过从查询日志中检索相似查询来动态地组合其提示。GQR方法重用了现有的神经网络架构，从而在冷启动场景中实现了一种更简单、更易于市场化的方法。我们提出的GQR在NDCG@10和清晰度得分方面取得了最先进的性能，与两个商业搜索引擎以及Robust04和ClueWeb09B集合上的先前最先进的方法相比，平均提高了Robust04和ClueWeb09B上的NDCG@10性能高达~4%。RA-GQR进一步提高了NDCG@10，相对于最佳竞争对手，在Robust04和ClueWeb09B上分别提高了~11%、~6%。此外，我们的系统在盲用户研究中获得了~59%的用户偏好，证明我们的方法产生了最具吸引力的查询。</p>
<h4 id="_28">一句话总结：</h4>
<p>提出了一种基于大型语言模型的生成查询推荐系统，有效解决了冷启动场景下的查询推荐问题，并显著提升了推荐效果。</p>
<hr />
<h2 id="keyword-driven-retrieval-augmented-large-language-models-for-cold-start-user-recommendations"><a href="http://arxiv.org/abs/2405.19612v1">Keyword-driven Retrieval-Augmented Large Language Models for Cold-start User Recommendations</a></h2>
<p>发布时间：2024-05-30</p>
<p>作者：Hai-Dang Kieu, Minh Duc Nguyen, Thanh-Son Nguyen, Dung D. Le</p>
<h4 id="_29">中文摘要：</h4>
<p>近年来，大型语言模型（LLMs）在增强推荐系统方面展现出巨大的潜力。然而，解决冷启动推荐问题（即用户缺乏历史数据的情况）仍然是一个重大挑战。在本文中，我们提出了KALM4Rec（基于关键词检索增强的大型语言模型用于冷启动用户餐厅推荐），这是一个专门设计的框架，旨在通过在冷启动用户餐厅推荐的实际场景中仅需要少量用户输入关键词来解决这一问题。KALM4Rec分为两个主要阶段：候选检索和基于LLM的候选重排序。在第一阶段，使用关键词驱动的检索模型来识别潜在候选者，解决LLMs处理大量标记的限制，并降低生成误导性信息的风险。在第二阶段，我们采用具有各种提示策略的LLMs，包括零样本和少样本技术，通过将多个示例直接集成到LLM提示中，对这些候选者进行重排序。我们的评估使用了一个包含来自三个英语城市用户评论的Yelp餐厅数据集，表明我们提出的框架显著提高了推荐质量。具体来说，将上下文指令与LLMs结合用于重排序明显提升了冷启动用户推荐系统的性能。</p>
<h4 id="_30">一句话总结：</h4>
<p>KALM4Rec通过结合关键词检索和大型语言模型，有效解决了冷启动用户餐厅推荐中的推荐质量问题。</p>
<hr />
<h2 id="unlearning-climate-misinformation-in-large-language-models"><a href="http://arxiv.org/abs/2405.19563v1">Unlearning Climate Misinformation in Large Language Models</a></h2>
<p>发布时间：2024-05-29</p>
<p>作者：Michael Fore, Simranjit Singh, Chaehong Lee, Amritanshu Pandey, Antonios Anastasopoulos, Dimitrios Stamoulis</p>
<h4 id="_31">中文摘要：</h4>
<p>气候变化的相关虚假信息是应对人类面临的最严重威胁之一的关键障碍。本文调查了大型语言模型（LLMs）在气候信息方面的事实准确性。利用真实/虚假标签的问答数据对LLMs进行微调和评估，我们比较了开源模型，评估它们生成关于气候变化问题的真实回答的能力。我们研究了故意注入虚假气候信息的模型的可检测性，发现这种中毒可能不会影响模型在其他领域回答的准确性。此外，我们比较了重新学习算法、微调和检索增强生成（RAG）在将LLMs在气候变化主题上事实化方面的有效性。我们的评估表明，尽管之前的研究表明它们在隐私环境中效率低下，但重新学习算法对于细微的概念性主张仍然可能有效。这些见解旨在指导开发更可靠的事实LLMs，并强调需要进一步工作以保护LLMs免受虚假信息攻击。</p>
<h4 id="_32">一句话总结：</h4>
<p>本文通过评估大型语言模型在处理气候变化信息时的准确性，旨在指导开发更可靠的事实LLMs，并强调保护LLMs免受虚假信息攻击的必要性。</p>
<hr />
<h2 id="two-layer-retrieval-augmented-generation-framework-for-low-resource-medical-question-answering-proof-of-concept-using-reddit-data"><a href="http://arxiv.org/abs/2405.19519v1">Two-layer retrieval augmented generation framework for low-resource medical question-answering: proof of concept using Reddit data</a></h2>
<p>发布时间：2024-05-29</p>
<p>作者：Sudeshna Das, Yao Ge, Yuting Guo, Swati Rajwal, JaMor Hairston, Jeanne Powell, Drew Walker, Snigdha Peddireddy, Sahithi Lakamana, Selen Bozkurt, Matthew Reyna, Reza Sameni, Yunyu Xiao, Sangmi Kim, Rasheeta Chandler, Natalie Hernandez, Danielle Mowery, Rachel Wightman, Jennifer Love, Anthony Spadaro, Jeanmarie Perrone, Abeed Sarker</p>
<h4 id="_33">中文摘要：</h4>
<p>检索增强生成（RAG）通过提供相关的上下文文本，能够限制生成模型输出的内容，并减轻幻觉的可能性。由于生成大型语言模型（LLM）可以纳入作为上下文的标记数量是有限的，这限制了生成答案的知识量。我们提出了一种双层RAG框架，用于查询导向的答案生成，并在社交媒体论坛的查询导向摘要生成背景下评估了该框架的概念验证，重点关注新兴的药物相关信息。评估结果表明，在资源受限的环境中，双层框架能够有效地帮助研究人员从用户那里获得近乎实时的数据。</p>
<h4 id="_34">一句话总结：</h4>
<p>本文提出了一种双层检索增强生成框架，用于从社交媒体论坛中生成针对新兴药物信息的查询导向摘要，有效提高了研究人员获取实时数据的能力。</p>
<hr />
<h2 id="toward-conversational-agents-with-context-and-time-sensitive-long-term-memory"><a href="http://arxiv.org/abs/2406.00057v2">Toward Conversational Agents with Context and Time Sensitive Long-term Memory</a></h2>
<p>发布时间：2024-05-29</p>
<p>作者：Nick Alonso, Tomás Figliolia, Anthony Ndirango, Beren Millidge</p>
<h4 id="_35">中文摘要：</h4>
<p>近年来，对具有长期记忆能力的对话代理的兴趣日益增长，这促使语言模型迅速发展，并采用了检索增强生成（RAG）技术。直到最近，大多数关于RAG的研究都集中在从大型文本数据库（如维基百科）中进行信息检索，而不是从长篇对话中获取信息。在本文中，我们提出，与静态数据库检索相比，从长篇对话数据中有效检索面临两个独特问题：1）基于时间/事件查询，这要求模型根据时间或对话事件的顺序（例如，星期二的第三次对话）检索关于先前对话的信息，以及2）需要周围对话上下文来理解的模糊查询。为了更好地开发能够应对这些挑战的基于RAG的代理，我们生成了一个新的数据集，包含模糊和基于时间的问题，这些数据集基于最近的长篇模拟对话数据集，并证明标准RAG方法处理这类问题效果不佳。然后，我们开发了一个新颖的检索模型，该模型结合了链式表搜索方法、标准向量数据库检索以及用于消除查询歧义的建议方法，并证明这种方法在解决这些任务方面显著优于现有方法。我们相信，这个新的数据集和更先进的RAG代理可以作为关键基准和迈向有效记忆增强对话代理的垫脚石，这些代理可以在广泛的AI应用中使用。</p>
<h4 id="_36">一句话总结：</h4>
<p>本文提出了一种新的检索模型，以解决从长篇对话数据中检索信息时遇到的独特挑战，并显著提升了基于RAG的对话代理的性能。</p>
<hr />
<h2 id="nearest-neighbor-speculative-decoding-for-llm-generation-and-attribution"><a href="http://arxiv.org/abs/2405.19325v2">Nearest Neighbor Speculative Decoding for LLM Generation and Attribution</a></h2>
<p>发布时间：2024-05-29</p>
<p>作者：Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, Xi Victoria Lin</p>
<h4 id="_37">中文摘要：</h4>
<p>本文提出了一种名为Nearest Neighbor Speculative Decoding（NEST）的新型半参数语言模型方法。该方法通过在每个推理步骤中进行标记级检索，计算半参数混合分布，并在语料库中识别有希望的文本片段延续，从而将任意长度的真实世界文本片段融入语言模型生成中，并为这些生成内容提供来源归属。NEST采用近似推测解码过程，接受检索到的文本片段的前缀或生成新标记。在多种知识密集型任务中，NEST显著提升了基础语言模型的生成质量和归属率，超越了传统的kNN-LM方法，并与上下文检索增强方法具有竞争力。此外，NEST在应用于Llama-2-Chat 70B时，大幅提高了生成速度，推理时间加快了1.8倍。</p>
<h4 id="_38">一句话总结：</h4>
<p>NEST通过结合半参数语言模型和推测解码，有效提升了语言模型的生成质量和速度，同时提供了文本来源归属。</p>
<hr />
<h2 id="reverse-image-retrieval-cues-parametric-memory-in-multimodal-llms"><a href="http://arxiv.org/abs/2405.18740v1">Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs</a></h2>
<p>发布时间：2024-05-29</p>
<p>作者：Jialiang Xu, Michael Moor, Jure Leskovec</p>
<h4 id="_39">中文摘要：</h4>
<p>尽管近年来多模态大型语言模型（MLLMs）取得了令人印象深刻的进展，但如GPT-4系列中的最先进模型仍然在知识密集型任务上存在困难。为了解决这个问题，我们考虑了逆向图像检索（RIR）增强生成，这是一种简单而有效的策略，通过将网络规模的逆向图像搜索结果增强到MLLMs中。逆向图像检索（RIR）稳健地提高了GPT-4V的知识密集型视觉问答（VQA）性能，使其在开放式VQA评估指标上提高了37-43%，GPT-4 Turbo提高了25-27%，GPT-4o提高了18-20%。令人惊讶的是，我们发现RIR有助于模型更好地访问其自身的世界知识。具体来说，我们的实验表明，RIR增强通过提供额外的视觉和文本线索来帮助，而不一定包含查询的直接答案。此外，我们还阐明了RIR可能损害性能的情况，并进行了人工评估。最后，我们发现使用RIR的整体优势使得能够选择使用RIR的代理难以比默认设置RIR的方法表现得更好。</p>
<h4 id="_40">一句话总结：</h4>
<p>逆向图像检索（RIR）增强显著提升了大型语言模型在知识密集型视觉问答任务上的性能。</p>
<hr />
<h2 id="ctrla-adaptive-retrieval-augmented-generation-via-probe-guided-control"><a href="http://arxiv.org/abs/2405.18727v1">CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control</a></h2>
<p>发布时间：2024-05-29</p>
<p>作者：Huanshuo Liu, Hao Zhang, Zhijiang Guo, Kuicai Dong, Xiangyang Li, Yi Quan Lee, Cong Zhang, Yong Liu</p>
<h4 id="_41">中文摘要：</h4>
<p>检索增强生成（RAG）已成为缓解大型语言模型（LLMs）幻觉的一种有前景的解决方案，通过检索外部知识来实现。自适应RAG通过动态评估检索必要性来增强这种方法，旨在平衡外部和内部知识的使用。然而，现有的自适应RAG方法主要依赖于LLMs的表面语言化或概率反馈，或者通过精心设计的数据集直接微调LLMs，从而导致了不可靠的检索必要性决策、高昂的额外成本和次优的响应生成。我们首次尝试深入LLMs的内部状态，通过引入一个有效的探针引导的自适应RAG框架，称为CtrlA，来缓解这些问题。具体来说，CtrlA使用一个诚实探针通过操纵LLMs的表示来调节其行为，以增加诚实度，并使用一个置信度探针来监控LLMs的内部状态并评估置信水平，在生成过程中确定检索必要性。实验表明，CtrlA在一系列不同的任务上优于现有的自适应RAG方法，诚实控制可以有效地使LLMs更加诚实，置信度监控已被证明是检索触发的一个有希望的指标。我们的代码可在https://github.com/HSLiu-Initial/CtrlA.git上找到。</p>
<h4 id="_42">一句话总结：</h4>
<p>CtrlA通过探针引导的自适应RAG框架，有效缓解了大型语言模型检索增强生成中的幻觉问题。</p>
<hr />
<h2 id="can-gpt-redefine-medical-understanding-evaluating-gpt-on-biomedical-machine-reading-comprehension"><a href="http://arxiv.org/abs/2405.18682v1">Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension</a></h2>
<p>发布时间：2024-05-29</p>
<p>作者：Shubham Vatsal, Ayush Singh</p>
<h4 id="_43">中文摘要：</h4>
<p>大型语言模型（LLMs）在多个领域的许多任务上表现出色。然而，它们在闭卷式生物医学机器阅读理解（MRC）中的性能尚未得到深入评估。在这项工作中，我们对GPT在四个闭卷式生物医学MRC基准上进行评估。我们尝试了不同的传统提示技术，并引入了我们自己的新颖提示方法。为了解决LLMs固有的检索问题，我们提出了一种名为隐式检索增强生成（RAG）的提示策略，该策略减轻了在传统RAG设置中使用向量数据库检索重要片段的需求。此外，我们还对从我们的方法中生成的自然语言生成输出进行了定性评估。结果表明，我们的新提示技术能够在四个数据集中的两个中取得最佳性能，在其他数据集中排名第二。实验表明，即使是现代LLMs如GPT，在零样本设置下也能优于监督模型，在两个基准上取得了新的最先进（SoTA）结果。</p>
<h4 id="_44">一句话总结：</h4>
<p>本研究通过引入隐式检索增强生成（RAG）策略，显著提升了大型语言模型在闭卷式生物医学机器阅读理解任务上的性能。</p>
<hr />
<h2 id="a-multi-source-retrieval-question-answering-framework-based-on-rag"><a href="http://arxiv.org/abs/2405.19207v1">A Multi-Source Retrieval Question Answering Framework Based on RAG</a></h2>
<p>发布时间：2024-05-29</p>
<p>作者：Ridong Wu, Shuhong Chen, Xiangbiao Su, Yuankai Zhu, Yifei Liao, Jianming Wu</p>
<h4 id="_45">中文摘要：</h4>
<p>随着大规模语言模型的快速发展，检索增强生成（RAG）被广泛采用。然而，现有的RAG范式不可避免地会受到错误检索信息的影响，从而降低生成结果的可靠性和正确性。因此，为了提高检索信息的相关性，本研究提出了一种方法，该方法用GPT-3.5替换传统的检索器，利用其庞大的语料库知识来生成检索信息。我们还提出了一种基于网络的检索方法来实现细粒度知识检索，利用GPT-3.5强大的推理能力实现问题的语义分区。为了减轻GPT检索的幻觉并减少网络检索中的噪声，我们提出了一种名为MSRAG的多源检索框架，该框架结合了GPT检索和网络检索。在多个知识密集型问答数据集上的实验表明，本研究提出的框架在提高问答系统的整体效率和准确性方面优于现有的RAG框架。</p>
<h4 id="_46">一句话总结：</h4>
<p>本研究提出了一种结合GPT-3.5和网络检索的多源检索框架MSRAG，有效提高了问答系统的效率和准确性。</p>
<hr />
<h2 id="leveraging-many-to-many-relationships-for-defending-against-visual-language-adversarial-attacks"><a href="http://arxiv.org/abs/2405.18770v1">Leveraging Many-To-Many Relationships for Defending Against Visual-Language Adversarial Attacks</a></h2>
<p>发布时间：2024-05-29</p>
<p>作者：Futa Waseda, Antonio Tejero-de-Pablos</p>
<h4 id="_47">中文摘要：</h4>
<p>最近的研究表明，视觉-语言（VL）模型在图像-文本检索（ITR）方面容易受到对抗攻击。然而，现有的针对VL模型的防御策略主要关注零样本图像分类，这些策略没有考虑图像和文本的同时操纵，以及ITR固有的多对多（N:N）特性，即单一图像可以用多种方式描述，反之亦然。为此，本文首次研究了针对ITR的VL模型对抗攻击的防御策略。特别是，我们关注如何利用ITR中的N:N关系来增强对抗鲁棒性。我们发现，尽管对抗训练容易对训练数据中的特定一对一（1:1）图像-文本对产生过拟合，但通过创建一对多（1:N）/多对一（N:1）图像-文本对的多样化增强技术可以显著提高VL模型的对抗鲁棒性。此外，我们还表明，增强图像-文本对的匹配对于防御策略的有效性至关重要，不适当的增强甚至可能降低模型的表现。基于这些发现，我们提出了一种新的防御策略，该策略利用ITR中的N:N关系，通过基本的增强技术和基于生成模型的增强技术有效地生成多样化的高度匹配的N:N对。这项工作为防御VL任务中的对抗攻击提供了新的视角，并为未来的研究开辟了新的方向。</p>
<h4 id="_48">一句话总结：</h4>
<p>本文提出了一种基于ITR中N:N关系的VL模型对抗攻击防御策略，通过多样化且高度匹配的N:N对增强，有效提升了模型的对抗鲁棒性。</p>
<hr />
<h2 id="dont-forget-to-connect-improving-rag-with-graph-based-reranking"><a href="http://arxiv.org/abs/2405.18414v1">Don't Forget to Connect! Improving RAG with Graph-based Reranking</a></h2>
<p>发布时间：2024-05-28</p>
<p>作者：Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, Anton Tsitsulin</p>
<h4 id="_49">中文摘要：</h4>
<p>检索增强生成（Retrieval Augmented Generation，RAG）通过将生成内容与现有文档中的上下文相结合，显著提高了大型语言模型（Large Language Model，LLM）的响应性能。这些系统在文档与问题上下文明显相关时表现良好。但当文档只包含部分信息，或者与上下文之间的联系不那么明显时，又会如何呢？我们该如何推理文档之间的联系呢？在这项工作中，我们试图回答关于RAG生成中的这两个核心问题。我们引入了G-RAG，这是一种基于图神经网络（Graph Neural Networks，GNNs）的重新排序器，位于RAG中的检索器和阅读器之间。我们的方法结合了文档之间的联系和语义信息（通过抽象意义表示图），为RAG提供了一种上下文感知的排序器。G-RAG在性能上优于最先进的方法，同时具有更小的计算足迹。此外，我们还评估了PaLM 2作为重新排序器的性能，并发现其表现显著低于G-RAG。这一结果强调了即使在使用大型语言模型的情况下，重新排序对于RAG的重要性。</p>
<h4 id="_50">一句话总结：</h4>
<p>G-RAG通过结合文档间的联系和语义信息，为RAG提供了一种上下文感知的排序器，显著提升了检索增强生成的性能。</p>
<hr />
<h2 id="bridging-the-gap-dynamic-learning-strategies-for-improving-multilingual-performance-in-llms"><a href="http://arxiv.org/abs/2405.18359v1">Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs</a></h2>
<p>发布时间：2024-05-28</p>
<p>作者：Somnath Kumar, Vaibhav Balloli, Mercy Ranjit, Kabir Ahuja, Tanuja Ganu, Sunayana Sitaram, Kalika Bali, Akshay Nambi</p>
<h4 id="_51">中文摘要：</h4>
<p>大型语言模型（LLMs）在全球范围内正引领着众多领域的变革。然而，它们在非拉丁文字和低资源语言中的包容性和有效性仍然有限。本文针对提升LLMs多语言性能这一紧迫挑战，提出了一种无需大量训练或微调的方法。通过系统地调查和评估使用流行的问答（QA）数据集的多种语言，我们提出了新颖的技术，这些技术能够释放LLMs在多语言环境中的真正潜力。我们的方法包括三个关键策略，这些策略显著提高了多语言能力。首先，通过精心优化针对多语言LLMs的提示，我们释放了它们的潜在能力，从而在多种语言中实现了显著的性能提升。其次，我们引入了一种新的混合方法，该方法将LLMs检索增强生成（RAG）与多语言嵌入相结合，实现了改进的多语言任务性能。最后，我们介绍了一种新颖的学习方法，该方法在运行时动态选择每个查询的最佳提示策略、LLM模型和嵌入模型。这种动态适应最大化了LLMs在多种语言中的有效性，超越了最佳静态和随机策略。此外，我们的方法在离线和在线设置中都可以调整配置，并且可以无缝适应新的语言和数据集，从而在多种语言中实现了多语言理解和生成的重大进步。</p>
<h4 id="_52">一句话总结：</h4>
<p>本文提出了一种无需大量训练或微调的方法，显著提升了大型语言模型在多种语言环境中的多语言性能。</p>
<hr />
<h2 id="atm-adversarial-tuning-multi-agent-system-makes-a-robust-retrieval-augmented-generator"><a href="http://arxiv.org/abs/2405.18111v2">ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator</a></h2>
<p>发布时间：2024-05-28</p>
<p>作者：Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, Lei Sha</p>
<h4 id="_53">中文摘要：</h4>
<p>大型语言模型（LLMs）在应对知识密集型问题时的幻觉问题中，从检索增强生成（RAG）中受益良多。RAG采用信息检索技术，将语义相关的文档中的外部知识作为输入上下文注入。然而，由于当今互联网充斥着大量噪声和虚假内容，RAG系统不可避免地会受到这些噪声的影响，并倾向于给出错误的响应。为此，我们提出使用对抗调优多智能体系统（ATM）来优化检索增强生成器。ATM通过辅助攻击者智能体的帮助，引导生成器对问答中的有用文档具有稳健的视角。生成器和攻击者经过多次迭代对抗调优。经过多智能体迭代调优后，生成器最终能够更好地在虚假内容中区分有用文档。实验结果验证了ATM的有效性，我们还观察到生成器比最先进的基线实现了更好的性能。</p>
<h4 id="_54">一句话总结：</h4>
<p>本研究提出了一种基于对抗调优多智能体系统的检索增强生成器优化方法，有效提升了大型语言模型在知识密集型问题回答中的性能。</p>
<hr />
<h2 id="ragsys-item-cold-start-recommender-as-rag-system"><a href="http://arxiv.org/abs/2405.17587v1">RAGSys: Item-Cold-Start Recommender as RAG System</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Emile Contal, Garrin McGoldrick</p>
<h4 id="_55">中文摘要：</h4>
<p>大型语言模型（LLM）在现实应用中具有巨大潜力，但它们的通用知识往往无法满足特定领域的需求。微调（Fine-tuning）是一种常见的方法，但可能会遭受灾难性遗忘并阻碍泛化能力。上下文学习（In-Context Learning，ICL）提供了一种替代方案，它可以通过检索增强生成（Retrieval-Augmented Generation，RAG）为LLM提供相关演示以支持少样本学习任务。本文探讨了ICL中演示检索系统的理想特性。我们认为，在这种背景下，ICL检索类似于物品冷启动推荐系统，优先考虑发现并最大化信息增益，而非严格的关联性。我们提出了一种新的评估方法，该方法通过衡量LLM在NLP任务上的后续性能，消除了对主观多样性分数的需求。我们的发现证明了多样性和质量偏差在检索到的演示中对有效ICL的关键作用，并突出了推荐系统技术在该领域的潜力。</p>
<h4 id="_56">一句话总结：</h4>
<p>本文提出了一种基于上下文学习的演示检索系统，通过引入推荐系统技术，有效提升了大型语言模型在少样本学习任务中的性能。</p>
<hr />
<h2 id="qub-cirdan-at-discharge-me-zero-shot-discharge-letter-generation-by-open-source-llm"><a href="http://arxiv.org/abs/2406.00041v2">QUB-Cirdan at "Discharge Me!": Zero shot discharge letter generation by open-source LLM</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Rui Guo, Greg Farnan, Niall McLaughlin, Barry Devereux</p>
<h4 id="_57">中文摘要：</h4>
<p>本文针对BioNLP ACL'24共享任务中关于简化出院文档的挑战，旨在通过自动化生成患者出院信中的关键部分来减轻临床医生的工作负担。我们提出了一种使用Llama3 8B量化模型生成“简要住院过程”和“出院指导”部分的方法。我们采用了一种零样本方法，结合检索增强生成（RAG）技术，以生成简洁、语境准确的摘要。我们的贡献包括开发了一种基于精心制作的模板的方法，以确保可靠性和一致性，以及将RAG技术整合到单词计数预测中。我们还描述了几个不成功的实验，以提供对我们竞赛路径的见解。我们的结果表明，我们的方法在多个评估指标上均取得了高效和有效的成绩。</p>
<h4 id="_58">一句话总结：</h4>
<p>本文提出了一种基于Llama3 8B量化模型和RAG技术的自动化出院文档生成方法，有效减轻了临床医生的工作负担，并在多个评估指标上取得了优异成绩。</p>
<hr />
<h2 id="aligning-llms-through-multi-perspective-user-preference-ranking-based-feedback-for-programming-question-answering"><a href="http://arxiv.org/abs/2406.00037v1">Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Hongyu Yang, Liyang He, Min Hou, Shuanghong Shen, Rui Li, Jiahui Hou, Jianhui Ma, Junda Zhao</p>
<h4 id="_59">中文摘要：</h4>
<p>代码社区问答（CCQA）旨在解决编程相关的问题，从而提高软件工程和学术研究的生产力。近年来，基于人类反馈的强化学习（RLHF）在大型语言模型（LLMs）微调过程中的应用取得了显著进展，使得模型生成的回答能够更接近人类行为。利用RLHF增强的LLMs进行实际的CCQA应用已成为一个有前景的研究领域。与标准的代码问答任务不同，CCQA涉及多个可能的答案，每个答案都有不同的用户偏好。此外，代码社区通常倾向于使用新的API。这些挑战阻碍了LLMs生成满足CCQA任务中用户多样化偏好的回答。为了解决这些问题，我们提出了一种名为“基于多视角用户偏好排名反馈对编程问答进行LLMs对齐（ALMupQA）”的新框架，以创建以用户为中心的回答。我们的方法从多视角偏好排名对齐（MPRA）开始，该对齐基于代码社区答案的特征综合不同的用户偏好。然后，我们引入了一个检索增强的上下文学习（RIL）模块，通过从问题库中检索类似问题的回答来减轻过时答案的问题。由于高质量、多答案CCQA数据集的有限可用性，我们还开发了一个名为StaCCQA的数据集，该数据集来自真实的代码社区。广泛的实验证明了ALMupQA框架在准确性和用户偏好方面的有效性。与基线模型相比，ALMupQA在BLEU上提高了近11%，在BERTScore和CodeBERTScore上分别提高了20%和17.5%。</p>
<h4 id="_60">一句话总结：</h4>
<p>该研究提出了一种名为ALMupQA的新框架，通过多视角用户偏好排名和检索增强的上下文学习，显著提高了代码社区问答的准确性和用户满意度。</p>
<hr />
<h2 id="emerge-integrating-rag-for-improved-multimodal-ehr-predictive-modeling"><a href="http://arxiv.org/abs/2406.00036v1">EMERGE: Integrating RAG for Improved Multimodal EHR Predictive Modeling</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Yinghao Zhu, Changyu Ren, Zixiang Wang, Xiaochen Zheng, Shiyun Xie, Junlan Feng, Xi Zhu, Zhoujun Li, Liantao Ma, Chengwei Pan</p>
<p>：</p>
<h4 id="_61">中文摘要：</h4>
<p>多模态电子健康记录（EHR）数据的整合显著提升了临床预测能力。然而，当前利用临床笔记和多变量时间序列EHR数据的模型往往缺乏精确临床任务所需的医学背景。先前使用知识图谱（KGs）的方法主要关注结构化知识的提取。为了解决这个问题，我们提出了EMERGE，这是一个由检索增强生成（RAG）驱动的框架，旨在增强多模态EHR预测建模。我们的方法通过提示大型语言模型（LLMs）从时间序列数据和临床笔记中提取实体，并将它们与专业PrimeKG对齐以确保一致性。除了三元关系之外，我们还包括实体的定义和描述以提供更丰富的语义。然后，使用提取的知识生成与任务相关的患者健康状况摘要。这些摘要通过具有交叉注意力的自适应多模态融合网络与其他模态融合。在MIMIC-III和MIMIC-IV数据集上进行的关于院内死亡和30天再入院任务的广泛实验表明，与基线模型相比，EMERGE框架具有优越的性能。全面的消融研究和分析强调了每个设计模块的有效性和框架对数据稀疏性的鲁棒性。EMERGE显著增强了在医疗保健中使用多模态EHR数据，弥合了与细微医学背景之间的差距，这对于知情临床预测至关重要。</p>
<h4 id="_62">一句话总结：</h4>
<p>EMERGE通过结合大型语言模型和知识图谱，显著提升了多模态EHR数据在医疗保健中的预测能力。</p>
<hr />
<h2 id="recent-advances-in-text-embedding-a-comprehensive-review-of-top-performing-methods-on-the-mteb-benchmark"><a href="http://arxiv.org/abs/2406.01607v2">Recent advances in text embedding: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Hongliu Cao</p>
<h4 id="_63">中文摘要：</h4>
<p>文本嵌入方法因其在对自然语言处理任务中的关键作用而在工业和学术领域越来越受欢迎。随着大型语言模型（LLMs）应用，如检索增强系统（RAGs）的兴起，通用文本嵌入的重要性进一步凸显。尽管先前模型试图成为通用型，但它们往往难以在任务和领域之间进行泛化。然而，训练数据数量、质量和多样性的最近进步；从LLMs生成合成数据以及将LLMs用作主干，这些都极大地促进了追求通用文本嵌入的改进。在本文中，我们概述了通用文本嵌入模型在最近的研究进展，重点关注在大量文本嵌入基准（MTEB）上表现优异的文本嵌入。通过详细的比较和分析，我们突出了该领域的关键贡献和局限性，并提出了可能具有启发性的未来研究方向。</p>
<h4 id="_64">一句话总结：</h4>
<p>本文综述了通用文本嵌入模型的研究进展，重点关注MTEB上的高性能文本嵌入，并提出了未来研究的潜在方向。</p>
<hr />
<h2 id="empowering-large-language-models-to-set-up-a-knowledge-retrieval-indexer-via-self-learning"><a href="http://arxiv.org/abs/2405.16933v1">Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Xun Liang, Simin Niu, Zhiyu li, Sensen Zhang, Shichao Song, Hanyu Wang, Jiawei Yang, Feiyu Xiong, Bo Tang, Chenyang Xi</p>
<h4 id="_65">中文摘要：</h4>
<p>检索增强生成（RAG）提供了一种成本效益高的方法，将实时知识注入大型语言模型（LLMs）。然而，构建和验证高质量的知识库需要相当大的努力。我们提出了一种名为伪图检索增强生成（PG-RAG）的预检索框架，该框架将LLMs视为学生，通过提供丰富的原始阅读材料并鼓励他们进行自主阅读，用自己的话记录事实信息。由此产生的简洁、组织良好的心智索引通过共同主题或补充事实相互连接，形成一个伪图数据库。在检索阶段，PG-RAG模仿人类在翻阅笔记时的行为，识别事实路径并随后探索相关上下文。遵循“人多走的路是最好的路”的原则，它整合了高度证实的事实路径，为LLMs提供结构化和精细化的子图。我们在三个专业问答数据集上验证了PG-RAG。在单文档任务中，PG-RAG在所有关键评估指标上均显著优于当前最佳基线KGP-LLaMA，平均整体性能提高了11.6%。具体来说，其BLEU分数提高了约14.3%，QE-F1指标提高了23.7%。在多文档场景中，PG-RAG的平均指标至少比最佳基线高2.35%。值得注意的是，BLEU分数和QE-F1指标分别稳定提高了约7.55%和12.75%。我们的代码：https://github.com/IAAR-Shanghai/PGRAG。</p>
<h4 id="_66">一句话总结：</h4>
<p>PG-RAG通过模拟人类阅读行为，有效地提高了LLMs在问答任务中的性能。</p>
<hr />
<h2 id="video-enriched-retrieval-augmented-generation-using-aligned-video-captions"><a href="http://arxiv.org/abs/2405.17706v1">Video Enriched Retrieval Augmented Generation Using Aligned Video Captions</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Kevin Dela Rosa</p>
<h4 id="_67">中文摘要：</h4>
<p>在这项工作中，我们提出使用“对齐视觉字幕”作为将视频中的信息整合到检索增强生成（RAG）基于聊天助手系统的机制。这些字幕能够描述大量语料库中视频的视觉和音频内容，同时具有以下优势：以易于推理和整合到大型语言模型（LLM）提示中的文本格式呈现；通常需要插入到多模态LLM上下文窗口中的多媒体内容更少，因为典型的配置可以通过从源视频中采样视频帧来积极地填充上下文窗口；此外，视觉字幕可以通过提示原始基础模型/字幕生成器特定的视觉细节或微调来适应特定的用例。为了帮助推进该领域的进展，我们整理了一个数据集，并描述了在常见的RAG任务上的自动评估程序。</p>
<h4 id="_68">一句话总结：</h4>
<p>本研究提出使用对齐视觉字幕作为将视频信息整合到RAG聊天助手系统中的机制，以提升信息检索和生成能力。</p>
<hr />
<h2 id="augmenting-textual-generation-via-topology-aware-retrieval"><a href="http://arxiv.org/abs/2405.17602v1">Augmenting Textual Generation via Topology Aware Retrieval</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Yu Wang, Nedim Lipka, Ruiyi Zhang, Alexa Siu, Yuying Zhao, Bo Ni, Xin Wang, Ryan Rossi, Tyler Derr</p>
<h4 id="_69">中文摘要：</h4>
<p>尽管大型语言模型（LLMs）在生成文本方面取得了令人印象深刻的进步，但它们通常受限于输入中的知识，并容易产生不准确或虚构的内容。为了解决这些问题，检索增强生成（RAG）被用作一种有效策略，通过从外部数据库中检索额外的文本来增强可用的知识库，并通过锚定现实来确保响应的准确性。在现实世界的应用中，文本通常通过图中的实体相互关联，例如学术论文中的引用或社交网络中的评论。本文利用这些拓扑关系来指导RAG中的检索过程。具体来说，我们探索了两种类型的拓扑连接：基于邻近度的，关注紧密连接的节点，以及基于角色的，它考察共享相似子图结构的节点。我们的实证研究证实了这些拓扑关系与文本关系的相关性，从而促使我们开发了一个拓扑感知的检索增强生成框架。该框架包括一个检索模块，它根据文本的拓扑关系选择文本，以及一个聚合模块，它将这些文本集成到提示中，以刺激LLMs进行文本生成。我们已经整理了已建立的文本属性网络，并进行了全面的实验来验证该框架的有效性，证明了其通过拓扑感知增强RAG的潜力。</p>
<h4 id="_70">一句话总结：</h4>
<p>本文提出了一种拓扑感知的检索增强生成框架，通过利用文本之间的拓扑关系来提高大型语言模型生成文本的准确性和相关性。</p>
<hr />
<h2 id="exploring-backdoor-attacks-against-large-language-model-based-decision-making"><a href="http://arxiv.org/abs/2405.20774v1">Exploring Backdoor Attacks against Large Language Model-based Decision Making</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu</p>
<h4 id="_71">中文摘要：</h4>
<p>大型语言模型（LLMs）在经过特定应用的微调后，在决策任务中展现出巨大的潜力，这得益于它们从大量数据中学习到的内在常识和推理能力。然而，这些系统在微调阶段面临着巨大的安全和安全风险。在这项工作中，我们提出了针对LLM决策系统（LLM-DS）的首次全面框架——针对LLM的后门攻击（BALD），系统地探讨了如何在微调阶段通过各种渠道引入此类攻击。具体而言，我们提出了三种攻击机制和相应的后门优化方法，以攻击基于LLM的决策流程中的不同组件：词汇注入、场景操纵和知识注入。词汇注入将触发词直接嵌入到查询提示中。场景操纵发生在物理环境中，高级后门语义场景触发攻击。知识注入对基于检索增强生成（RAG）的LLM系统进行后门攻击，策略性地将词汇触发器注入受污染的知识库中，同时确保信息在隐蔽性方面保持事实上的准确性。我们使用三个流行的LLMs（GPT-3.5、LLaMA2、PaLM2）和两个数据集（HighwayEnv、nuScenes）进行了广泛的实验，证明了我们后门触发器和机制的有效性和隐蔽性。最后，我们批判性地评估了我们提出的方法的优缺点，突出了LLMs在决策任务中的固有脆弱性，并评估了保护基于LLM的决策系统的潜在防御措施。</p>
<h4 id="_72">一句话总结：</h4>
<p>本研究提出了一种针对大型语言模型决策系统的后门攻击框架，揭示了LLMs在决策任务中的安全风险，并评估了潜在防御措施。</p>
<hr />
<h2 id="large-language-models-llms-deployment-tokenomics-and-sustainability"><a href="http://arxiv.org/abs/2405.17147v1">Large Language Models (LLMs): Deployment, Tokenomics and Sustainability</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Haiwei Dong, Shuang Xie</p>
<h4 id="_73">中文摘要：</h4>
<p>本文首先探讨了与最先进的语言大模型（LLMs）相关的部署策略、经济考量以及可持续性挑战。具体而言，我们讨论了检索增强生成（RAG）与微调之间的部署辩论，突出了它们各自的优势和局限性。随后，我们定量分析了训练和推理过程中对xPUs的需求。此外，从用户体验（QoE）的角度，我们考察了LLM服务代币经济学中性能与成本之间的平衡。最后，我们展望了LLM处理未来的混合架构及其相应的可持续性问题，特别是在环境影响和碳排放方面。通过这些讨论，我们提供了对LLMs负责任开发和部署所必需的操作和战略考量的全面概述。</p>
<h4 id="_74">一句话总结：</h4>
<p>本文全面分析了LLMs的部署、经济、可持续性以及用户体验，为LLMs的负责任发展提供了战略指导。</p>
<hr />
<h2 id="wirelessllm-empowering-large-language-models-towards-wireless-intelligence"><a href="http://arxiv.org/abs/2405.17053v2">WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Jiawei Shao, Jingwen Tong, Qiong Wu, Wei Guo, Zijian Li, Zehong Lin, Jun Zhang</p>
<h4 id="_75">中文摘要：</h4>
<p>随着无线技术的快速发展和网络基础设施的日益复杂，通信网络的设计、配置和管理方式亟需发生范式转变。近年来，大型语言模型（LLMs）的进步引发了人们对其在无线通信系统革命性变革潜力的兴趣。然而，现有关于LLMs在无线系统中的应用研究仅限于电信语言理解方面的直接应用。为了赋予LLMs无线领域的知识和专业知识，本文提出了一种名为WirelessLLM的综合框架，用于适应和增强LLMs以应对无线通信网络独特的挑战和需求。我们首先确定了WirelessLLM的三个基础原则：知识对齐、知识融合和知识演化。接着，我们探讨了构建WirelessLLM的使能技术，包括提示工程、检索增强生成、工具使用、多模态预训练和领域特定微调。此外，我们展示了三个案例研究，以证明WirelessLLM在解决无线网络典型问题中的实际应用和益处。最后，本文通过强调关键挑战并概述未来研究的潜在途径来总结。</p>
<h4 id="_76">一句话总结：</h4>
<p>本文提出WirelessLLM框架，旨在通过知识对齐、融合和演化，利用大型语言模型解决无线通信网络的独特挑战。</p>
<hr />
<h2 id="a-cross-dataset-study-for-text-based-3d-human-motion-retrieval"><a href="http://arxiv.org/abs/2405.16909v1">A Cross-Dataset Study for Text-based 3D Human Motion Retrieval</a></h2>
<p>发布时间：2024-05-27</p>
<p>作者：Léore Bensabath, Mathis Petrovich, Gül Varol</p>
<h4 id="_77">中文摘要：</h4>
<p>本研究提供了基于文本的3D人体运动检索的结果，尤其关注跨数据集泛化。由于实际原因，如特定数据集的人体表示，现有工作通常通过在同一数据集的不同部分进行训练和测试来建立基准。在这里，我们采用统一的SMPL人体格式处理所有数据集，这使得我们能够在一个数据集上进行训练，在另一个数据集上进行测试，以及在一个数据集组合上进行训练。我们的结果表明，在标准的文本-运动基准（如HumanML3D、KIT Motion-Language和BABEL）中存在数据集偏差。我们表明，文本增强在一定程度上有助于缩小领域差距，但差距仍然存在。我们进一步提供了BABEL上的首次零样本动作识别结果，在训练过程中没有使用分类动作标签，为未来的研究开辟了新的途径。</p>
<h4 id="_78">一句话总结：</h4>
<p>本研究通过统一人体表示格式和文本增强技术，揭示了现有文本-运动基准中的数据集偏差，并实现了BABEL数据集上的零样本动作识别，为3D人体运动检索的跨数据集泛化提供了新的思路。</p>
<hr />
<h2 id="m-rag-reinforcing-large-language-model-performance-through-retrieval-augmented-generation-with-multiple-partitions"><a href="http://arxiv.org/abs/2405.16420v1">M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions</a></h2>
<p>发布时间：2024-05-26</p>
<p>作者：Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, Wei Shi</p>
<h4 id="_79">中文摘要：</h4>
<p>本文提出了一种名为M-RAG的多分区范式来增强大型语言模型（LLMs），通过从外部数据库中检索相关记忆。然而，现有的RAG方法通常将所有记忆组织在一个整个数据库中，这可能会限制对关键记忆的关注并引入噪声。基于此范式，我们提出了一种新的框架，该框架利用多智能体强化学习来优化不同的语言生成任务。通过在七个数据集上进行的全面实验，涵盖了三个语言生成任务和三种不同的语言模型架构，我们证实了M-RAG在文本摘要、机器翻译和对话生成方面均优于各种基线方法，分别实现了11%、8%和12%的改进。</p>
<h4 id="_80">一句话总结：</h4>
<p>本文提出的M-RAG多分区范式通过优化大型语言模型的语言生成任务，显著提升了文本摘要、机器翻译和对话生成的性能。</p>
<hr />
<h2 id="disentangling-and-integrating-relational-and-sensory-information-in-transformer-architectures"><a href="http://arxiv.org/abs/2405.16727v1">Disentangling and Integrating Relational and Sensory Information in Transformer Architectures</a></h2>
<p>发布时间：2024-05-26</p>
<p>作者：Awni Altabaa, John Lafferty</p>
<h4 id="_81">中文摘要：</h4>
<p>该论文介绍了Transformer架构的扩展，该架构通过实现一种神经信息传递的形式来处理序列，包括迭代的信息检索（注意力）和随后局部处理（位置感知MLP）。在这种通用计算范式下，两种类型的信息是至关重要的：“感官”信息关于单个对象，以及描述对象之间关系的“关系”信息。标准的注意力机制自然地编码了前者，但并未显式地编码后者。在本文中，我们提出了一种扩展的Transformer，其中多头注意力被两种不同类型的注意力头所增强，每个注意力头路由不同类型的信息。第一种类型是Transformer的标准注意力机制，它捕获对象级别的特征，而第二种类型是我们提出的用于显式捕获关系信息的创新注意力机制。这两种类型的注意力头各自具有不同的归纳偏差，使得最终的架构具有更高的效率和灵活性。这种方法在一系列任务中的潜力通过实证研究得到了证明。</p>
<h4 id="_82">一句话总结：</h4>
<p>本文提出了一种扩展的Transformer，通过引入两种不同类型的注意力头来显式捕获关系信息，从而提高了处理序列数据的效率和灵活性。</p>
<hr />
<h2 id="grag-graph-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2405.16506v1">GRAG: Graph Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-05-26</p>
<p>作者：Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, Liang Zhao</p>
<h4 id="_83">中文摘要：</h4>
<p>尽管检索增强生成（RAG）通过生成语言模型提高了响应的准确性和相关性，但在文本和拓扑信息都重要的图上下文中，它存在不足。简单的RAG方法本质上忽略了文本图的复杂结构，导致生成过程中的关键差距。为了解决这一挑战，我们引入了图检索增强生成（GRAG），通过强调子图结构的重要性，显著增强了检索和生成过程。与仅关注基于文本的实体检索的RAG方法不同，GRAG保持了对图拓扑的敏锐意识，这对于生成上下文和事实上一致的反应至关重要。我们的GRAG方法包括四个主要阶段：索引$k$-跳自我图、图检索、软剪枝以减轻无关实体的影响，以及使用剪枝文本子图的生成。GRAG的核心工作流程——检索文本子图后进行软剪枝——有效地识别了相关的子图结构，同时避免了典型于穷举子图搜索的计算不可行性，后者是NP难的。此外，我们提出了一种新颖的提示策略，实现了从文本子图到分层文本描述的无损转换。在图多跳推理基准上的大量实验表明，在需要基于文本图的多跳推理的场景中，我们的GRAG方法在性能上显著优于当前的RAG方法，同时有效地减轻了幻觉。</p>
<h4 id="_84">一句话总结：</h4>
<p>图检索增强生成（GRAG）通过强调子图结构的重要性，显著提高了基于文本图的生成模型的准确性和相关性，同时有效减轻了幻觉。</p>
<hr />
<h2 id="ecg-semantic-integrator-esi-a-foundation-ecg-model-pretrained-with-llm-enhanced-cardiological-text"><a href="http://arxiv.org/abs/2405.19366v1">ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with LLM-Enhanced Cardiological Text</a></h2>
<p>发布时间：2024-05-26</p>
<p>作者：Han Yu, Peikun Guo, Akane Sano</p>
<h4 id="_85">中文摘要：</h4>
<p>本研究利用深度学习在心电图（ECG）分析中的应用，提高了心脏健康诊断的准确性和效率。通过利用深度学习在语义理解方面的能力，特别是在特征提取和表示学习方面，本研究提出了一种新的多模态对比预训练框架，旨在提高12导联ECG信号学习表示的质量和鲁棒性。该框架包括两个关键组件，即心血管查询助手（CQA）和ECG语义集成器（ESI）。CQA集成了检索增强生成（RAG）管道，利用大型语言模型（LLMs）和外部医学知识生成详细的ECG文本描述，并丰富了关于人口统计学和波形模式的信息。ESI集成了对比和标题损失，以预训练ECG编码器，以增强表示。我们通过包括心律失常检测和基于ECG的主题识别在内的各种下游任务验证了我们的方法。我们的实验结果表明，在这些任务中，与强大的基线相比，我们的方法有显著的改进。这些基线包括监督和自监督学习方法，以及先前多模态预训练方法。</p>
<h4 id="_86">一句话总结：</h4>
<p>本研究提出了一种基于深度学习的多模态对比预训练框架，显著提高了ECG信号分析的质量和鲁棒性。</p>
<hr />
<h2 id="retrieval-augmented-conversational-recommendation-with-prompt-based-semi-structured-natural-language-state-tracking"><a href="http://arxiv.org/abs/2406.00033v1">Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking</a></h2>
<p>发布时间：2024-05-25</p>
<p>作者：Sara Kemper, Justin Cui, Kai Dicarlantonio, Kathy Lin, Danjie Tang, Anton Korikov, Scott Sanner</p>
<h4 id="_87">中文摘要：</h4>
<p>会话推荐（ConvRec）系统必须理解用户偏好和意图的丰富多样的自然语言（NL）表达，这些表达往往以间接的方式传达（例如，“我在注意我的体重”）。这种复杂的表述使得检索相关项目变得具有挑战性，尤其是在仅使用经常不完整或过时的元数据的情况下。幸运的是，许多领域都有丰富的项目评论，涵盖了标准的元数据类别，并提供了可能符合用户兴趣的复杂意见（例如，“适合约会的时尚场所”）。然而，只有最近，大型语言模型（LLMs）才使我们能够解锁用户偏好表述和用户生成评论中的复杂语言之间的常识性联系。此外，LLMs还使得半结构化对话状态跟踪、复杂意图和偏好理解以及生成推荐、解释和问答等新颖范式成为可能。因此，我们引入了一种新的技术RA-Rec，这是一个用于ConvRec的检索增强、LLM驱动的对话状态跟踪系统，通过视频、开源GitHub仓库和交互式Google Colab笔记本进行了展示。</p>
<h4 id="_88">一句话总结：</h4>
<p>该研究提出了一种基于大型语言模型和检索增强的会话推荐系统，通过理解用户复杂语言表达和生成个性化推荐，提升了推荐系统的准确性和用户体验。</p>
<hr />
<h2 id="towards-unlocking-insights-from-logbooks-using-ai"><a href="http://arxiv.org/abs/2406.12881v1">Towards Unlocking Insights from Logbooks Using AI</a></h2>
<p>发布时间：2024-05-25</p>
<p>作者：Antonin Sulc, Alex Bien, Annika Eichler, Daniel Ratner, Florian Rehm, Frank Mayet, Gregor Hartmann, Hayden Hoschouer, Henrik Tuennermann, Jan Kaiser, Jason St. John, Jennefer Maldonado, Kyle Hazelwood, Raimund Kammering, Thorsten Hellert, Tim Wilksen, Verena Kain, Wan-Lin Hu</p>
<h4 id="_89">中文摘要：</h4>
<p>电子日志包含有关其相关粒子加速器设施的活动和事件的宝贵信息。然而，日志条目的高度技术性质可能会阻碍其可用性和自动化。随着自然语言处理（NLP）的持续进步，它为解决日志所面临的各种挑战提供了机会。本研究探讨了联合测试一种定制的检索增强生成（RAG）模型，以提高DESY、BESSY、Fermilab、BNL、SLAC、LBNL和CERN等机构的粒子加速器日志的可用性。该RAG模型使用基于日志贡献的语料库，旨在通过利用设施数据集的检索（包括关于潜在多模态来源的讨论）来挖掘这些日志的洞察力。我们的目标是通过对日志的信息内容进行利用，以提高日志的FAIR-ness（可发现性、可访问性、互操作性和可重用性），简化日常使用，实现根本原因分析的宏观分析，并促进问题解决的自动化。</p>
<h4 id="_90">一句话总结：</h4>
<p>本研究通过开发定制的RAG模型，旨在提高粒子加速器日志的可用性，并利用自然语言处理技术实现日志信息的自动化处理和分析。</p>
<hr />
<h2 id="accelerating-inference-of-retrieval-augmented-generation-via-sparse-context-selection"><a href="http://arxiv.org/abs/2405.16178v1">Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection</a></h2>
<p>发布时间：2024-05-25</p>
<p>作者：Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, Jindong Chen</p>
<h4 id="_91">中文摘要：</h4>
<p>大型语言模型（LLMs）通过结合外部上下文展现出强大的性能和广泛的适用性。然而，随着检索到的文档数量的线性增长，输入长度也随之增加，导致延迟显著增加。在本文中，我们提出了一种名为稀疏RAG（Sparse RAG）的新范式，旨在通过稀疏性来降低计算成本。具体来说，稀疏RAG并行编码检索到的文档，从而消除了由检索文档的远程注意力引入的延迟。然后，LLMs通过仅自动回归地关注高度相关的缓存来选择性地解码输出，这些缓存是通过向LLMs提示特殊控制令牌来选择的。值得注意的是，稀疏RAG将每个单独文档的评估和响应生成合并为单一过程。在RAG系统中设计的稀疏机制可以促进在解码过程中减少加载的文档数量，从而加速RAG系统的推理。此外，过滤掉不希望的环境上下文增强了模型对相关上下文的关注，从而本质上提高了其生成质量。两个数据集的评估结果表明，稀疏RAG可以在生成质量和计算效率之间达到最佳平衡，展示了其在短篇和长篇生成任务中的泛化能力。</p>
<h4 id="_92">一句话总结：</h4>
<p>本文提出的稀疏RAG范式通过并行编码和选择性解码，有效降低了大型语言模型在结合外部上下文时的计算成本，同时保持了生成质量。</p>
<hr />
<h2 id="synthai-a-multi-agent-generative-ai-framework-for-automated-modular-hls-design-generation"><a href="http://arxiv.org/abs/2405.16072v2">SynthAI: A Multi Agent Generative AI Framework for Automated Modular HLS Design Generation</a></h2>
<p>发布时间：2024-05-25</p>
<p>作者：Seyed Arash Sheikholeslam, Andre Ivanov</p>
<h4 id="_93">中文摘要：</h4>
<p>本文介绍了一种名为SynthAI的新方法，用于自动化创建高级综合（HLS）设计。SynthAI将ReAct智能体、思维链（CoT）提示、网络搜索技术和检索增强生成（RAG）框架集成在一个结构化决策图中。这种创新方法能够将复杂的硬件设计任务系统性地分解成多个阶段和更小、更易于管理的模块。因此，SynthAI能够生成符合用户指定设计目标和功能要求的可综合设计。我们通过几个案例研究进一步验证了SynthAI的能力，突出了它从单个初始提示生成复杂、多模块逻辑设计的能力。SynthAI代码可通过以下仓库获取：\url{https://github.com/sarashs/FPGA_AGI}</p>
<h4 id="_94">一句话总结：</h4>
<p>SynthAI通过集成多种人工智能技术，实现了复杂硬件设计的自动化和高效生成。</p>
<hr />
<h2 id="amgpt-a-large-language-model-for-contextual-querying-in-additive-manufacturing"><a href="http://arxiv.org/abs/2406.00031v1">AMGPT: a Large Language Model for Contextual Querying in Additive Manufacturing</a></h2>
<p>发布时间：2024-05-24</p>
<p>作者：Achuth Chandrasekhar, Jonathan Chan, Francis Ogoke, Olabode Ajenifujah, Amir Barati Farimani</p>
<h4 id="_95">中文摘要：</h4>
<p>广义大型语言模型（LLMs）如GPT-4可能无法为材料科学研究者提出的查询提供具体答案。这些模型可能生成高级概述，但缺乏返回关于新型合金制造和材料特性的详细说明的能力。通过增强具有特定领域知识的小型模型，可能比那些无法快速重新训练以跟上金属增材制造（AM）研究快速步伐的大型语言模型具有优势。我们介绍了“AMGPT”，这是一种专为金属AM查询设计的专业LLM文本生成器。AMGPT的目标是帮助研究人员和用户在AM的广泛文献库中导航。我们不是从头开始训练，而是在检索增强生成（RAG）设置中使用了Hugging Face的预训练Llama2-7B模型，利用它动态地整合来自约50篇AM论文和教科书的PDF格式信息。Mathpix被用来将这些PDF文档转换为TeX格式，便于它们集成到由LlamaIndex管理的RAG管道中。对该项目的专家评估指出，RAG设置中的特定嵌入可以加速响应时间并保持生成文本的一致性。</p>
<h4 id="_96">一句话总结：</h4>
<p>AMGPT是一种专门为金属增材制造查询设计的LLM文本生成器，旨在通过整合大量文献信息，提高材料科学研究中查询的响应速度和准确性。</p>
<hr />
<h2 id="clustered-retrieved-augmented-generation-crag"><a href="http://arxiv.org/abs/2406.00029v1">Clustered Retrieved Augmented Generation (CRAG)</a></h2>
<p>发布时间：2024-05-24</p>
<p>作者：Simon Akesson, Frances A. Santos</p>
<h4 id="_97">中文摘要：</h4>
<p>向大型语言模型（LLMs）提供外部知识是这些模型在现实世界应用中的关键点，原因包括以实时方式融入最新内容、提供特定领域知识以及有助于防止幻觉。基于向量数据库的检索增强生成（RAG）方法已被广泛采用来实现这一目标。因此，任何外部知识的一部分都可以被检索并提供给某些LLM作为输入上下文。尽管RAG方法取得了成功，但它对于某些应用来说可能仍然不可行，因为检索到的上下文可能需要比LLM支持的上下文窗口更大的窗口大小。即使检索到的上下文适合上下文窗口大小，标记的数量也可能很大，从而影响成本和处理时间，对大多数应用来说变得不切实际。为了解决这些问题，我们提出了CRAG，这是一种新颖的方法，能够在不降低与使用RAG的解决方案相比生成的响应质量的情况下，有效地减少提示标记的数量。通过我们的实验，我们表明CRAG可以将标记数量至少减少46%，在某些情况下甚至超过90%，与RAG相比。此外，与RAG不同，当分析的评论数量更高时，CRAG的标记数量不会显著增加，在75条评论与4条评论相比时，RAG的标记数量几乎是9倍。</p>
<h4 id="_98">一句话总结：</h4>
<p>CRAG是一种能够有效减少提示标记数量，同时保持生成响应质量的方法，适用于大型语言模型在实际应用中的知识检索和生成任务。</p>
<hr />
<h2 id="certifiably-robust-rag-against-retrieval-corruption"><a href="http://arxiv.org/abs/2405.15556v1">Certifiably Robust RAG against Retrieval Corruption</a></h2>
<p>发布时间：2024-05-24</p>
<p>作者：Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, Prateek Mittal</p>
<h4 id="_99">中文摘要：</h4>
<p>检索增强生成（RAG）已被证明容易受到检索篡改攻击：攻击者可以向检索结果中注入恶意段落以诱导不准确响应。在本文中，我们提出了RobustRAG作为第一个针对检索篡改攻击的防御框架。RobustRAG的关键洞察是一个“隔离然后聚合”的策略：我们独立地从每个段落获取大型语言模型（LLM）的响应，然后安全地聚合这些独立的响应。为了实现RobustRAG，我们设计了基于关键词和基于解码的算法来安全地聚合非结构化文本响应。值得注意的是，RobustRAG可以实现可证明的鲁棒性：我们可以形式化证明并认证，对于某些查询，即使攻击者完全了解我们的防御措施并能任意注入少量恶意段落，RobustRAG也能始终返回准确响应。我们在开放域问答和长文本生成数据集上评估了RobustRAG，并展示了其在各种任务和数据集上的有效性和泛化能力。</p>
<h4 id="_100">一句话总结：</h4>
<p>本文提出的RobustRAG框架通过“隔离然后聚合”策略，实现了对检索篡改攻击的可证明鲁棒性，有效提升了检索增强生成系统的安全性。</p>
<hr />
<h2 id="large-language-models-reflect-human-citation-patterns-with-a-heightened-citation-bias"><a href="http://arxiv.org/abs/2405.15739v2">Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias</a></h2>
<p>发布时间：2024-05-24</p>
<p>作者：Andres Algaba, Carmen Mazijn, Vincent Holst, Floriano Tori, Sylvia Wenmackers, Vincent Ginis</p>
<h4 id="_101">中文摘要：</h4>
<p>引用实践对于塑造科学知识结构至关重要，然而它们往往受到当代规范和偏见的影响。大型语言模型（LLMs）如GPT-4的出现为这些实践引入了新的动态。有趣的是，完全依赖于其参数知识而非搜索或检索增强生成的LLMs推荐的参考文献的特征和潜在偏见尚未得到探索。在这里，我们通过使用包含166篇论文的数据集进行分析，这些论文来自AAAI、NeurIPS、ICML和ICLR，并发表于GPT-4的知识截止日期之后，总共包含3,066个参考文献。在我们的实验中，GPT-4被要求为这些论文中的匿名文本引用提出学术参考文献。我们的发现显示，人类和LLM的引用模式之间存在显著相似性，但GPT-4表现出更明显的高引用偏差，即使在控制了出版年份、标题长度、作者数量和发表地点之后，这种偏差仍然存在。此外，我们还观察到GPT-4生成的现有参考文献和非现有参考文献的特征之间具有很大的一致性，这表明模型已经内化了引用模式。通过分析引用图，我们发现GPT-4推荐的参考文献嵌入在相关的引用语境中，这表明对引用网络的更深层次的概念内化。虽然LLMs可以帮助生成引用，但它们也可能放大现有偏见并引入新的偏见，从而可能扭曲科学知识的传播。我们的结果强调了识别模型偏见以及开发与LLMs平衡互动方法的需求。</p>
<h4 id="_102">一句话总结：</h4>
<p>本研究揭示了大型语言模型在推荐参考文献时可能存在的偏差，并强调了在科学知识传播中识别和平衡这些偏差的重要性。</p>
<hr />
<h2 id="hybrid-context-retrieval-augmented-generation-pipeline-llm-augmented-knowledge-graphs-and-vector-database-for-accreditation-reporting-assistance"><a href="http://arxiv.org/abs/2405.15436v1">Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented Knowledge Graphs and Vector Database for Accreditation Reporting Assistance</a></h2>
<p>发布时间：2024-05-24</p>
<p>作者：Candace Edwards</p>
<h4 id="_103">中文摘要：</h4>
<p>在高等教育中，认证是一个质量保证过程，其中一所学校通过展示其对向学生提供高质量课程和服务的承诺来证明其资质。对于国内外商学院而言，美国商学院促进协会（AACSB）的认证是黄金标准。为了获得并维持认证，商学院必须进行严格、耗时的报告和同行评审流程，以证明其与AACSB标准的契合度。对于本项目，我们创建了一个混合的上下文检索增强生成管道，该管道可以帮助进行认证所需的文档对齐和报告流程。我们实施了一个向量数据库和知识图谱，作为包含机构数据和AACSB标准数据的知识存储。管道的输出可以被机构利益相关者用来构建他们的认证报告，双重地基于知识存储中的上下文。为了开发我们的知识图谱，我们利用了手动构建过程以及LLM增强知识图谱方法。我们使用RAGAs框架评估了管道，并观察到在答案相关性和答案正确性指标上表现出最佳性能。</p>
<h4 id="_104">一句话总结：</h4>
<p>本研究开发了一个混合上下文检索增强生成管道，以辅助商学院进行认证报告的文档对齐和报告流程。</p>
<hr />
<h2 id="grokked-transformers-are-implicit-reasoners-a-mechanistic-journey-to-the-edge-of-generalization"><a href="http://arxiv.org/abs/2405.15071v2">Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization</a></h2>
<p>发布时间：2024-05-23</p>
<p>作者：Boshi Wang, Xiang Yue, Yu Su, Huan Sun</p>
<h4 id="_105">中文摘要：</h4>
<p>本研究探讨了Transformer是否能够学习隐式地处理参数化知识，这是一种即使是能力最强的语言模型也难以掌握的技能。我们聚焦于两种代表性的推理类型——组合和比较，并一致发现Transformer能够学习隐式推理，但这需要通过“理解”（grokking），即超过过度拟合的扩展训练。不同推理类型的泛化水平也存在差异：当面对分布外示例时，Transformer在组合推理上无法进行系统性的泛化，但在比较推理上则能成功。我们通过分析实验深入研究了模型内部，揭示了：1）理解背后的机制，例如泛化电路的形成及其与泛化电路和记忆电路相对效率的关系，以及2）系统性与其泛化电路配置之间的联系。我们的发现指导了数据集和训练设置，以更好地诱导隐式推理，并提出了对Transformer架构的潜在改进建议，例如鼓励跨层知识共享。此外，我们还证明了对于具有大搜索空间的具有挑战性的推理任务，基于非参数化记忆的GPT-4-Turbo和Gemini-1.5-Pro，无论提示风格或检索增强如何，都表现不佳，而一个完全理解的Transformer可以实现近乎完美的准确率，展示了参数化记忆在复杂推理中的力量。</p>
<h4 id="_106">一句话总结：</h4>
<p>本研究发现，通过扩展训练，Transformer能够学习隐式推理，并揭示了其泛化机制和系统性配置，为复杂推理任务提供了新的视角。</p>
<hr />
<h2 id="agrame-any-granularity-ranking-with-multi-vector-embeddings"><a href="http://arxiv.org/abs/2405.15028v1">AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings</a></h2>
<p>发布时间：2024-05-23</p>
<p>作者：Revanth Gangi Reddy, Omar Attia, Yunyao Li, Heng Ji, Saloni Potdar</p>
<h4 id="_107">中文摘要：</h4>
<p>在搜索领域，排名是一个基本且普遍存在的问题。然而，现有的排名算法通常将排名的粒度限制在全文或要求为每个期望的粒度级别提供一个特定的密集索引。这种在粒度上的缺乏灵活性对许多可以从更细粒度排名中受益的应用产生了负面影响，例如开放域问答中的句子级排名或归因中的命题级排名。在本工作中，我们引入了任何粒度排名的概念，它利用多向量嵌入在多个粒度级别上进行排名，同时保持编码在单一（较粗）的粒度级别上。我们提出了一种多粒度对比损失，用于训练多向量方法，并通过以句子和命题作为排名单元来验证其有效性。最后，我们展示了命题级排名在检索增强生成中后置引用添加的应用，其性能超过了基于提示的引用生成。</p>
<h4 id="_108">一句话总结：</h4>
<p>本文提出了一种任何粒度排名方法，通过多向量嵌入和对比损失，实现了在多个粒度级别上的灵活排名，并在检索增强生成中展示了其优越性能。</p>
<hr />
<h2 id="re-adapt-reverse-engineered-adaptation-of-large-language-models"><a href="http://arxiv.org/abs/2405.15007v1">RE-Adapt: Reverse Engineered Adaptation of Large Language Models</a></h2>
<p>发布时间：2024-05-23</p>
<p>作者：William Fleshman, Benjamin Van Durme</p>
<h4 id="_109">中文摘要：</h4>
<p>我们提出了RE-Adapt，一种在新的领域上微调大型语言模型而不降低任何现有指令微调的方法。我们逆向工程了一个适配器，该适配器隔离了指令微调模型在其对应的预训练基础模型之外学到的内容。重要的是，这不需要额外的数据或训练。然后，我们可以在新领域上微调基础模型，并使用逆向工程适配器重新适应指令遵循。RE-Adapt和我们的低秩变体LoRE-Adapt在多个流行的LLMs和数据集上，甚至在模型与检索增强生成结合使用时，都优于其他微调方法。</p>
<h4 id="_110">一句话总结：</h4>
<p>RE-Adapt通过逆向工程适配器，实现了在无需额外数据或训练的情况下，对大型语言模型进行跨领域微调，并显著提升了模型性能。</p>
<hr />
<h2 id="a-textbook-remedy-for-domain-shifts-knowledge-priors-for-medical-image-analysis"><a href="http://arxiv.org/abs/2405.14839v1">A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis</a></h2>
<p>发布时间：2024-05-23</p>
<p>作者：Yue Yang, Mona Gandhi, Yufei Wang, Yifan Wu, Michael S. Yao, Chris Callison-Burch, James C. Gee, Mark Yatskar</p>
<h4 id="_111">中文摘要：</h4>
<p>尽管深度网络在分析自然图像方面取得了广泛的成功，但当应用于医学扫描时，它们往往在意外情况下失败。我们研究了这一挑战，并专注于模型对领域变化的敏感性，例如来自不同医院的数据或受性别、种族等人口统计学变量影响的数据，在胸部X光片和皮肤病变图像的背景下。我们通过实证研究的一个重要发现是，现有的视觉骨干网络缺乏适当的先验，无法在这些环境中实现可靠的泛化。受医学培训的启发，我们提出为深度网络提供一个基于自然语言中明确传达的医学知识的先验。为此，我们引入了知识增强瓶颈（KnoBo），一类概念瓶颈模型，它结合了知识先验，将其约束于与医学教科书中或PubMed中发现的临床相关因素进行推理。KnoBo使用检索增强的语言模型来设计适当的概念空间，并配以自动训练程序以识别概念。我们在20个数据集的广泛领域变化上评估了不同的知识资源和识别架构。在我们的综合评估中，KnoBo在受干扰的数据集上平均优于微调模型32.4%。最后，评估结果表明PubMed是使医学模型对领域变化不那么敏感的有希望的资源，在信息多样性和最终预测性能上都优于其他资源。</p>
<h4 id="_112">一句话总结：</h4>
<p>本研究提出了一种名为KnoBo的知识增强瓶颈模型，通过结合PubMed等资源中的医学知识，显著提高了深度网络在医学图像分析中对领域变化的鲁棒性。</p>
<hr />
<h2 id="hipporag-neurobiologically-inspired-long-term-memory-for-large-language-models"><a href="http://arxiv.org/abs/2405.14831v1">HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</a></h2>
<p>发布时间：2024-05-23</p>
<p>作者：Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su</p>
<h4 id="_113">中文摘要：</h4>
<p>为了在敌对且不断变化的自然环境中生存，哺乳动物的脑部进化出存储大量关于世界的知识，并持续整合新信息的同时避免灾难性的遗忘。尽管取得了令人印象深刻的成就，但即使是带有检索增强生成（RAG）的大语言模型（LLMs）在预训练后仍然难以有效地整合大量新经验。在这项工作中，我们引入了HippoRAG，这是一个受人类长期记忆的海马体索引理论启发的创新检索框架，旨在实现对新经验更深层次和更有效的知识整合。HippoRAG协同地协调LLMs、知识图谱和个性化PageRank算法，模仿人类记忆中新皮层和海马体的不同角色。我们在多跳问答任务上比较了HippoRAG与现有的RAG方法，并显示出我们的方法在性能上显著优于最先进的方法，最多提高20%。使用HippoRAG的单步检索在性能上与IRCoT等迭代检索相当甚至更好，同时成本降低10-30倍，速度提高6-13倍，将HippoRAG集成到IRCoT中还能带来进一步的实质性收益。最后，我们展示了我们的方法可以处理现有方法无法触及的新类型场景。代码和数据可在https://github.com/OSU-NLP-Group/HippoRAG上获取。</p>
<h4 id="_114">一句话总结：</h4>
<p>HippoRAG通过模仿人类大脑记忆机制，实现了大语言模型在知识整合方面的显著提升。</p>
<hr />
<h2 id="rafe-ranking-feedback-improves-query-rewriting-for-rag"><a href="http://arxiv.org/abs/2405.14431v1">RaFe: Ranking Feedback Improves Query Rewriting for RAG</a></h2>
<p>发布时间：2024-05-23</p>
<p>作者：Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang</p>
<h4 id="_115">中文摘要：</h4>
<p>随着大型语言模型（LLMs）和检索增强生成（RAG）技术的不断发展，查询重写已被广泛应用于RAG系统中的下游任务，如开放域问答。许多研究尝试利用小型模型通过强化学习来改进查询重写，而不是使用昂贵的LLMs。然而，当前的方法需要标注（例如，标记相关文档或下游答案）或预设计的奖励来进行反馈，这些方法缺乏泛化能力，并且未能利用针对查询重写的定制信号。在本文中，我们提出了一种无需标注的查询重写模型训练框架。通过利用公开可用的重排器，我们的框架提供了与重写目标高度一致的反馈。实验结果表明，我们的方法在性能上优于基线。</p>
<h4 id="_116">一句话总结：</h4>
<p>本文提出了一种无需标注的查询重写模型训练框架，通过公开重排器提供与重写目标一致的反馈，在性能上优于基线。</p>
<hr />
<h2 id="g3-an-effective-and-adaptive-framework-for-worldwide-geolocalization-using-large-multi-modality-models"><a href="http://arxiv.org/abs/2405.14702v1">G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models</a></h2>
<p>发布时间：2024-05-23</p>
<p>作者：Pengyue Jia, Yiding Liu, Xiaopeng Li, Xiangyu Zhao, Yuhao Wang, Yantong Du, Xiao Han, Xuetao Wei, Shuaiqiang Wang, Dawei Yin</p>
<h4 id="_117">中文摘要：</h4>
<p>全球地理定位旨在确定地球上任何地点拍摄的照片的精确坐标位置。由于1)捕捉细微的地理位置感知视觉语义的困难，以及2)图像数据的异构地理分布，这使得全球地理定位极具挑战性。因此，当现有研究扩展到全球范围时，它们存在明显的局限性。它们可能会轻易地将远距离的图像与相似视觉内容的图像混淆，或者无法适应全球不同地点的相关数据量。为了解决这些局限性，我们提出了G3，一个基于检索增强生成（RAG）的全新框架。具体来说，G3包括三个步骤，即地理对齐、地理多样化和地理验证，以优化全球地理定位的检索和生成阶段。在地理对齐阶段，我们的解决方案联合学习图像、GPS和文本描述的表达性多模态表示，这使得我们能够捕捉到地理位置感知语义，以便为给定查询检索附近的图像。在地理多样化阶段，我们利用了一种对不一致检索性能鲁棒的提示集成方法。最后，在地理验证阶段，我们将检索到的和生成的GPS候选者结合起来进行位置预测。在IM2GPS3k和YFCC4k两个成熟数据集上的实验验证了G3相较于其他最先进方法的优越性。</p>
<h4 id="_118">一句话总结：</h4>
<p>G3通过结合多模态表示、鲁棒的提示集成和综合验证步骤，实现了全球地理定位的精确性和适应性。</p>
<hr />
<h2 id="xrag-extreme-context-compression-for-retrieval-augmented-generation-with-one-token"><a href="http://arxiv.org/abs/2405.13792v1">xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token</a></h2>
<p>发布时间：2024-05-22</p>
<p>作者：Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, Dongyan Zhao</p>
<h4 id="_119">中文摘要：</h4>
<p>本文介绍了一种针对检索增强生成（Retrieval-Augmented Generation）的创新的上下文压缩方法——xRAG。xRAG重新解释了密集检索中使用的文档嵌入（传统上仅用于检索）作为检索模态的特征。通过采用模态融合方法，xRAG将这些嵌入无缝集成到语言模型表示空间中，有效地消除了其文本对应物的需求，并实现了极高的压缩率。在xRAG中，唯一可训练的组件是模态桥接器，而检索器和语言模型保持冻结状态。这种设计选择允许重用离线构建的文档嵌入，并保持了检索增强的即插即用特性。实验结果表明，xRAG在六个知识密集型任务上实现了平均超过10%的提升，适用于从密集的7B模型到8x7B专家混合配置的各种语言模型骨干。xRAG不仅显著优于先前的方法，而且在多个数据集上与未压缩模型的表现相当，同时将整体FLOPs减少了3.53倍。我们的工作从多模态融合的角度为检索增强生成开辟了新的方向，并希望为未来的高效和可扩展检索增强系统奠定基础。</p>
<h4 id="_120">一句话总结：</h4>
<p>xRAG通过模态融合技术实现了检索增强生成的高效上下文压缩，显著提升了生成模型的性能。</p>
<hr />
<h2 id="automated-evaluation-of-retrieval-augmented-language-models-with-task-specific-exam-generation"><a href="http://arxiv.org/abs/2405.13622v1">Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation</a></h2>
<p>发布时间：2024-05-22</p>
<p>作者：Gauthier Guinet, Behrooz Omidvar-Tehrani, Anoop Deoras, Laurent Callot</p>
<h4 id="_121">中文摘要：</h4>
<p>我们提出了一种新的方法来衡量检索增强大型语言模型（RAG）的任务特定准确性。通过在由与任务相关的文档语料库生成的多项选择题组成的自动生成的合成考试上对RAG进行评分来进行评估。我们的方法是一种自动的、成本效益高、可解释且鲁棒的策略，用于选择RAG系统的最佳组件。我们利用项目反应理论（IRT）来估计考试的质量及其对任务特定准确性的信息量。IRT还提供了一种自然的方式来迭代地改进考试，通过消除不足以反映模型能力的考试问题。我们在基于Arxiv摘要、StackExchange问题、AWS DevOps故障排除指南和SEC文件的四项新的开放式问答任务上展示了我们的方法。此外，我们的实验揭示了影响RAG性能的更普遍的因素，如大小、检索机制、提示和微调。最值得注意的是，我们的发现表明，选择正确的检索算法通常比简单地使用更大的语言模型带来更大的性能提升。</p>
<h4 id="_122">一句话总结：</h4>
<p>本研究提出了一种基于项目反应理论（IRT）的自动评估方法，用于提高检索增强大型语言模型（RAG）的任务特定准确性，并揭示了选择合适的检索算法对性能提升的重要性。</p>
<hr />
<h2 id="flashrag-a-modular-toolkit-for-efficient-retrieval-augmented-generation-research"><a href="http://arxiv.org/abs/2405.13576v1">FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research</a></h2>
<p>发布时间：2024-05-22</p>
<p>作者：Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou</p>
<h4 id="_123">中文摘要：</h4>
<p>随着大型语言模型（LLMs）的出现，检索增强生成（RAG）技术的潜力引起了相当多的研究关注。许多新颖的算法和模型被引入以增强RAG系统的各个方面。然而，缺乏一个标准化的实现框架，加上RAG过程本身的复杂性，使得研究人员在一致的环境中比较和评估这些方法变得具有挑战性和耗时。尽管现有的RAG工具包如LangChain和LlamaIndex可用，但它们通常较为沉重且难以操控，无法满足研究人员的个性化需求。为了应对这一挑战，我们提出了FlashRAG，这是一个高效且模块化的开源工具包，旨在帮助研究人员在统一框架内重现现有的RAG方法并开发他们自己的RAG算法。我们的工具包实现了12种先进的RAG方法，并收集和整理了32个基准数据集。我们的工具包具有各种功能，包括可定制的模块化框架、丰富的预实现RAG作品集合、全面的数据集、高效的辅助预处理脚本以及广泛的标准化评估指标。我们的工具包和资源可在https://github.com/RUC-NLPIR/FlashRAG上获取。</p>
<h4 id="_124">一句话总结：</h4>
<p>FlashRAG是一个高效且模块化的开源工具包，旨在简化RAG方法的复现和开发，并提供丰富的资源和标准化评估。</p>
<hr />
<h2 id="trojanrag-retrieval-augmented-generation-can-be-backdoor-driver-in-large-language-models"><a href="http://arxiv.org/abs/2405.13401v4">TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models</a></h2>
<p>发布时间：2024-05-22</p>
<p>作者：Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Ping Yi, Zhuosheng Zhang, Gongshen Liu</p>
<h4 id="_125">中文摘要：</h4>
<p>大型语言模型（LLMs）虽然在自然语言处理（NLP）领域表现出色，但同时也引发了潜在的网络安全威胁。后门攻击初步验证了LLMs在各个阶段都造成了实质性的损害，但其成本和鲁棒性受到了批评。攻击LLMs在安全审查中本身就具有内在风险，且成本高昂。此外，LLMs的持续迭代将降低后门的鲁棒性。在本文中，我们提出了TrojanRAG，它采用检索增强生成中的联合后门攻击，从而在通用攻击场景中操纵LLMs。具体来说，攻击者构建了复杂的目标上下文和触发集。通过对比学习正交优化多个后门快捷方式，从而将触发条件约束到参数子空间以提高匹配度。为了提高RAG对目标上下文的召回率，我们引入了知识图谱来构建结构化数据，以实现细粒度的硬匹配。此外，我们将LLMs中的后门场景进行归一化，从攻击者和用户的角度分析后门造成的实际危害，并进一步验证上下文是否是破解模型的有利工具。在真实性、语言理解和有害性方面的广泛实验结果表明，TrojanRAG展现出多方面的威胁，同时在正常查询上保持了检索能力。</p>
<h4 id="_126">一句话总结：</h4>
<p>本文提出的TrojanRAG通过在检索增强生成中实施联合后门攻击，对大型语言模型构成了多方面的威胁，同时保持了其检索能力。</p>
<hr />
<h2 id="rag-rlrc-laysum-at-biolaysumm-integrating-retrieval-augmented-generation-and-readability-control-for-layman-summarization-of-biomedical-texts"><a href="http://arxiv.org/abs/2405.13179v4">RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented Generation and Readability Control for Layman Summarization of Biomedical Texts</a></h2>
<p>发布时间：2024-05-21</p>
<p>作者：Yuelyu Ji, Zhuochun Li, Rui Meng, Sonish Sivarajkumar, Yanshan Wang, Zeshui Yu, Hui Ji, Yushui Han, Hanyu Zeng, Daqing He</p>
<h4 id="_127">中文摘要：</h4>
<p>本文介绍了RAG-RLRC-LaySum框架，该框架通过先进的自然语言处理（NLP）技术，旨在使复杂的生物医学研究对普通民众变得易于理解。我们的检索增强生成（RAG）解决方案，通过重排序方法得到增强，利用多个知识源确保了民众摘要的精确性和相关性。此外，我们的阅读理解控制（RLRC）策略通过提高可读性，使得科学内容对非专业人士变得可理解。使用公开可访问的PLOS和eLife数据集进行的评估显示，我们的方法超越了Plain Gemini模型，实现了可读性评分提高20%，ROUGE-2相关性评分提高15%，以及事实准确性提高10%。RAG-RLRC-LaySum框架有效地民主化了科学知识，增强了公众对生物医学发现的参与度。</p>
<h4 id="_128">一句话总结：</h4>
<p>该研究提出了一种名为RAG-RLRC-LaySum的框架，通过自然语言处理技术提高生物医学研究的可读性，使科学知识更加普及。</p>
<hr />
<h2 id="towards-retrieval-augmented-architectures-for-image-captioning"><a href="http://arxiv.org/abs/2405.13127v1">Towards Retrieval-Augmented Architectures for Image Captioning</a></h2>
<p>发布时间：2024-05-21</p>
<p>作者：Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Alessandro Nicolosi, Rita Cucchiara</p>
<h4 id="_129">中文摘要：</h4>
<p>本文旨在通过生成准确反映输入图像内容的自然语言描述，弥合视觉和语言模态之间的差距。近年来，研究人员利用基于深度学习的模型，在视觉特征提取和多模态连接设计方面取得了进展。本研究提出了一种新的图像描述模型开发方法，该方法利用外部kNN记忆来改进生成过程。具体来说，我们提出了两种模型变体，它们包含基于视觉相似性的知识检索组件、用于表示输入图像的可微分编码器，以及基于上下文线索和从外部记忆检索到的文本的kNN增强语言模型来预测标记。我们在COCO和nocaps数据集上实验验证了我们的方法，并表明引入显式的外部记忆可以显著提高描述的质量，尤其是在更大的检索语料库中。这项工作为检索增强的描述模型提供了宝贵的见解，并为在大规模上改进图像描述开辟了新的途径。</p>
<h4 id="_130">一句话总结：</h4>
<p>本研究提出了一种利用外部kNN记忆的图像描述模型，通过引入知识检索组件和增强语言模型，显著提高了描述的质量。</p>
<hr />
<h2 id="diffusion-rscc-diffusion-probabilistic-model-for-change-captioning-in-remote-sensing-images"><a href="http://arxiv.org/abs/2405.12875v1">Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in Remote Sensing Images</a></h2>
<p>发布时间：2024-05-21</p>
<p>作者：Xiaofei Yu, Yitong Li, Jie Ma</p>
<h4 id="_131">中文摘要：</h4>
<p>遥感图像变化描述（RSICC）旨在生成类似人类的语言来描述双时相遥感图像对之间的语义变化。它为环境动态和土地管理提供了有价值的见解。与传统的变化描述任务不同，RSICC不仅涉及跨模态检索相关信息和生成流畅的描述，还包括减轻像素级差异对地形变化定位的影响。由于长时间跨度导致的像素问题降低了生成描述的准确性。受扩散模型显著生成能力的启发，我们提出了一种用于RSICC的概率扩散模型来解决上述问题。在训练过程中，我们构建了一个基于跨模态特征的噪声预测器，在马尔可夫链下从真实描述分布学习到标准高斯分布。同时，在反向过程中，为噪声预测器设计了跨模态融合和堆叠自注意力模块。在测试阶段，经过良好训练的噪声预测器有助于估计分布的均值并逐步生成变化描述。在LEVIR-CC数据集上的大量实验证明了我们扩散-RSCC及其各个组件的有效性。定量结果表明，在传统和新增的指标上，我们的方法在性能上优于现有方法。代码和材料将在https://github.com/Fay-Y/Diffusion-RSCC上提供。</p>
<h4 id="_132">一句话总结：</h4>
<p>提出了一种基于概率扩散模型的遥感图像变化描述方法，有效解决了像素级差异和长时间跨度导致的描述准确性问题。</p>
<hr />
<h2 id="the-2nd-futuredial-challenge-dialog-systems-with-retrieval-augmented-generation-futuredial-rag"><a href="http://arxiv.org/abs/2405.13084v1">The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented Generation (FutureDial-RAG)</a></h2>
<p>发布时间：2024-05-21</p>
<p>作者：Yucheng Cai, Si Chen, Yi Huang, Junlan Feng, Zhijian Ou</p>
<h4 id="_133">中文摘要：</h4>
<p>本文介绍了第二届FutureDial挑战赛，该挑战赛的主题是检索增强生成（Retrieval Augmented Generation，简称RAG）的对话系统，并与2024年SLT（Speech and Language Technology）会议同期举行。</p>
<h4 id="_134">一句话总结：</h4>
<p>第二届FutureDial挑战赛聚焦于检索增强生成对话系统，旨在推动对话系统技术的发展。</p>
<hr />
<h2 id="generative-ai-and-large-language-models-for-cyber-security-all-insights-you-need"><a href="http://arxiv.org/abs/2405.12750v1">Generative AI and Large Language Models for Cyber Security: All Insights You Need</a></h2>
<p>发布时间：2024-05-21</p>
<p>作者：Mohamed Amine Ferrag, Fatima Alwahedi, Ammar Battah, Bilel Cherif, Abdechakour Mechri, Norbert Tihanyi</p>
<h4 id="_135">中文摘要：</h4>
<p>本文对通过生成式人工智能和大型语言模型（LLMs）的未来网络安全进行了全面综述。我们探讨了LLMs在各个领域的应用，包括硬件设计安全、入侵检测、软件工程、设计验证、网络威胁情报、恶意软件检测和钓鱼检测。我们概述了LLMs的演变及其当前状态，重点关注GPT-4、GPT-3.5、Mixtral-8x7B、BERT、Falcon2和LLaMA等模型的发展。我们的分析扩展到LLMs的漏洞，如提示注入、不安全的输出处理、数据中毒、DDoS攻击和对抗性指令。我们深入探讨了保护这些模型的缓解策略，全面审视潜在的攻击场景和预防技术。此外，我们评估了42个LLM模型在网络安全知识和硬件安全方面的性能，突出了它们的优缺点。我们对LLMs训练和测试的网络安全数据集进行了彻底评估，涵盖了从数据创建到使用的整个生命周期，并确定了未来研究的空白。此外，我们还回顾了利用LLMs的新策略，包括半二次量化（HQQ）、带人类反馈的强化学习（RLHF）、直接偏好优化（DPO）、量化低秩适配器（QLoRA）和检索增强生成（RAG）等技术。这些见解旨在增强实时网络安全防御，并提高LLM在威胁检测和响应中的应用复杂度。本文为将LLMs集成到未来网络安全框架中提供了基础理解和战略方向，强调创新和稳健的模型部署以防范不断发展的网络威胁。</p>
<h4 id="_136">一句话总结：</h4>
<p>本文全面分析了生成式AI和大型语言模型在网络安全领域的应用、挑战和未来发展方向。</p>
<hr />
<h2 id="question-based-retrieval-using-atomic-units-for-enterprise-rag"><a href="http://arxiv.org/abs/2405.12363v1">Question-Based Retrieval using Atomic Units for Enterprise RAG</a></h2>
<p>发布时间：2024-05-20</p>
<p>作者：Vatsal Raina, Mark Gales</p>
<h4 id="_137">中文摘要：</h4>
<p>企业检索增强生成（RAG）提供了一个高度灵活的框架，用于将强大的大型语言模型（LLMs）与内部、可能随时间变化的文档相结合。在RAG中，文档首先被分块。然后，针对特定用户查询检索相关块，并将这些块作为上下文传递给合成LLM以生成查询响应。然而，检索步骤可能会限制性能，因为错误的块可能导致合成LLM生成错误响应。这项工作提出了一种零样本适应标准密集检索步骤，以实现更准确的块召回。具体来说，首先将块分解为原子语句。然后在这些原子（以块为上下文）上生成一系列合成问题。密集检索涉及找到与用户查询最接近的合成问题集和相关块。研究发现，使用原子进行检索比使用块进行检索具有更高的召回率。使用在原子上生成的合成问题进行检索还能观察到进一步的性能提升。在检索步骤中实现更高的召回率，使得企业LLM在RAG管道中能够实现更高的性能。</p>
<h4 id="_138">一句话总结：</h4>
<p>该研究提出了一种基于原子语句的零样本密集检索方法，显著提高了企业检索增强生成（RAG）框架中块召回的准确性，从而提升了企业LLM的性能。</p>
<hr />
<h2 id="kg-rag-bridging-the-gap-between-knowledge-and-creativity"><a href="http://arxiv.org/abs/2405.12035v1">KG-RAG: Bridging the Gap Between Knowledge and Creativity</a></h2>
<p>发布时间：2024-05-20</p>
<p>作者：Diego Sanmartin</p>
<h4 id="_139">中文摘要：</h4>
<p>在保证事实准确性的同时，保持大型语言模型代理（LMAs）的创造性能力，在智能代理系统的发展中提出了重大挑战。LMAs在处理知识密集型任务时，面临着信息幻觉、灾难性遗忘以及在处理长上下文时的局限性等普遍问题。本文介绍了一种KG-RAG（知识图谱-检索增强生成）管道，这是一种新型框架，通过将结构化知识图谱（KGs）与LLMs（大型语言模型）的功能相结合，旨在增强LMAs的知识能力，从而显著减少对LLMs潜在知识的依赖。KG-RAG管道从非结构化文本中构建一个KG，然后在新创建的图上进行信息检索以执行KGQA（知识图谱问答）。检索方法利用了一种名为“探索链”（CoE）的新算法，该算法通过LLMs的推理能力，按顺序探索KG中的节点和关系。在ComplexWebQuestions数据集上的初步实验表明，在减少幻觉内容方面取得了显著改进，并暗示了一条开发擅长处理知识密集型任务的智能系统的可行路径。</p>
<h4 id="_140">一句话总结：</h4>
<p>本文提出了一种KG-RAG框架，通过结合知识图谱和大型语言模型，有效提升了智能代理在知识密集型任务中的事实准确性和创造性。</p>
<hr />
<h2 id="can-github-issues-be-solved-with-tree-of-thoughts"><a href="http://arxiv.org/abs/2405.13057v1">Can Github issues be solved with Tree Of Thoughts?</a></h2>
<p>发布时间：2024-05-20</p>
<p>作者：Ricardo La Rosa, Corey Hulse, Bangdi Liu</p>
<h4 id="_141">中文摘要：</h4>
<p>尽管在大语言模型（LLM）的代码生成方面已有广泛的研究，其中像HumanEval这样的基准测试已被以令人印象深刻的96.3%成功率超越，但这些基准主要评估模型在基本功能级代码生成方面的性能，缺乏现实场景如解决GitHub问题所需的批判性思维和范围概念。本研究引入了树状思维（ToT）语言模型推理框架的应用，以增强LLM在复杂任务中的决策和问题解决能力。与传统输入输出（IO）提示和检索增强生成（RAG）技术相比，ToT旨在通过促进多个推理轨迹的结构化探索和实现潜在解决方案的自我评估来提高性能。我们实验性地将ToT应用于解决SWE-bench实例中的一个GitHub问题。然而，我们的结果表明，仅ToT框架本身不足以赋予LLM超越现有方法的批判性推理能力。在本文中，我们分析了这些不足的潜在原因，并确定了关键改进领域，如深化思维过程和引入代理能力。本研究见解旨在为未来改进ToT的应用和更好地利用LLM在现实世界问题解决场景中的潜力提供指导。</p>
<h4 id="_142">一句话总结：</h4>
<p>本研究通过引入树状思维（ToT）框架，旨在提升大语言模型在复杂问题解决中的决策和推理能力，但发现仅ToT框架不足以超越现有方法，并提出了进一步改进的方向。</p>
<hr />
<h2 id="casegnn-graph-contrastive-learning-for-legal-case-retrieval-with-graph-augmentation"><a href="http://arxiv.org/abs/2405.11791v1">CaseGNN++: Graph Contrastive Learning for Legal Case Retrieval with Graph Augmentation</a></h2>
<p>发布时间：2024-05-20</p>
<p>作者：Yanran Tang, Ruihong Qiu, Yilun Liu, Xue Li, Zi Huang</p>
<h4 id="_143">中文摘要：</h4>
<p>法律案例检索（LCR）是一种专门的信息检索任务，旨在为给定的查询案例找到相关的案例。LCR对于帮助法律从业者找到先例具有关键意义。大多数现有的LCR方法都是基于传统的词汇模型和语言模型，这些模型在检索方面已经取得了有希望的性能。然而，法律文件中固有的特定领域结构信息尚未被充分利用以进一步提高性能。我们之前的工作CaseGNN成功地利用了文本属性图和图神经网络来解决法律结构信息忽视的问题。尽管如此，仍有两个方面需要进一步研究：（1）文本属性案例图中边缘信息的未充分利用限制了CaseGNN生成信息丰富的案例表示。（2）法律数据集中标记数据的不足阻碍了CaseGNN模型的训练。在本文中，CaseGNN++，它是从CaseGNN扩展而来的，被提出以同时利用边缘信息和额外的标记数据来发现LCR模型的潜在潜力。具体来说，提出了一种基于边缘特征的图注意力层（EUGAT），在图建模过程中全面更新节点和边缘特征，从而充分利用法律案例的结构信息。此外，CaseGNN++中开发了一种新的图对比学习目标，结合图增强，以提供额外的训练信号，从而增强CaseGNN++模型的法律理解能力。在COLIEE 2022和COLIEE 2023的两个基准数据集上进行的广泛实验表明，CaseGNN++不仅显著提高了CaseGNN的性能，而且与最先进的LCR方法相比也取得了优异的性能。代码已发布在https://github.com/yanran-tang/CaseGNN。</p>
<h4 id="_144">一句话总结：</h4>
<p>本文提出的CaseGNN++模型通过利用边缘信息和额外的标记数据，显著提升了法律案例检索的性能，并在基准数据集上取得了优异的效果。</p>
<hr />
<h2 id="can-public-llms-be-used-for-self-diagnosis-of-medical-conditions"><a href="http://arxiv.org/abs/2405.11407v2">Can Public LLMs be used for Self-Diagnosis of Medical Conditions ?</a></h2>
<p>发布时间：2024-05-18</p>
<p>作者：Nikil Sharan Prabahar Balasubramanian, Sagnik Dakshit</p>
<h4 id="_145">中文摘要：</h4>
<p>随着深度学习技术的进步，对基础深度学习模型的研究引起了广泛关注。大型语言模型（LLM）的发展已成为对话任务中的变革性范式，其集成和扩展甚至扩展到了医疗保健的关键领域。随着LLM的广泛应用和通过开源模型及其与其他应用的集成，有必要研究其潜力和局限性。在LLM被应用但需要更深入理解的关键任务之一是基于验证偏见的症状进行医疗状况自我诊断，以促进公共卫生。Gemini与谷歌搜索以及GPT-4.0与必应搜索的广泛集成导致了使用搜索引擎进行自我诊断的趋势转向对话LLM模型。鉴于该任务的敏感性，研究并理解公共LLM在自我诊断任务中的潜力和局限性是明智的。在本研究中，我们准备了一个包含10000个样本的提示工程数据集，并对其在自我诊断的一般任务上的性能进行了测试。我们比较了最先进的GPT-4.0和收费的Gemini模型在自我诊断任务上的性能，分别记录了63.07%和6.01%的对比准确率。我们还讨论了Gemini和GPT-4.0在自我诊断任务中的挑战、局限性和潜力，以促进未来的研究和推广公众知识的影响。此外，我们还展示了使用检索增强生成在自我诊断任务中的潜力和性能提升。</p>
<h4 id="_146">一句话总结：</h4>
<p>本研究通过比较GPT-4.0和Gemini在自我诊断任务上的表现，探讨了大型语言模型在医疗健康领域的潜力和局限性。</p>
<hr />
<h2 id="mitigating-interpretation-bias-in-rock-records-with-large-language-models-insights-from-paleoenvironmental-analysis"><a href="http://arxiv.org/abs/2407.09977v1">Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis</a></h2>
<p>发布时间：2024-05-17</p>
<p>作者：Luoqi Wang, Haipeng Li, Linshu Hu, Jiarui Cai, Zhenhong Du</p>
<h4 id="_147">中文摘要：</h4>
<p>地球历史的重建由于岩石记录中常常得出的非唯一解释而面临重大挑战。这个问题早已被认识到，但在实践中并没有系统性的解决方案。本研究引入了一种创新的方法，该方法利用大型语言模型（LLMs）以及检索增强生成和实时搜索功能来对抗解释偏差，从而提高地质分析的准确性和可靠性。通过将此框架应用于沉积学和古地理学，我们证明了其通过生成和评估相同数据的多个假设来减轻解释偏差的有效性，这可以有效地减少人为偏差。我们的研究揭示了LLMs在细化古环境研究中的变革潜力，并扩展了其在地球科学各个子学科中的应用，使对地球演化的描述更加深入和准确。</p>
<h4 id="_148">一句话总结：</h4>
<p>本研究利用大型语言模型和检索增强生成技术，有效减轻地质分析中的解释偏差，为地球科学各子学科提供更准确的历史重建方法。</p>
<hr />
<h2 id="retrieving-and-refining-a-hybrid-framework-with-large-language-models-for-rare-disease-identification"><a href="http://arxiv.org/abs/2405.10440v1">Retrieving and Refining: A Hybrid Framework with Large Language Models for Rare Disease Identification</a></h2>
<p>发布时间：2024-05-16</p>
<p>作者：Jinge Wu, Hang Dong, Zexi Li, Arijit Patra, Honghan Wu</p>
<h4 id="_149">中文摘要：</h4>
<p>罕见病的临床表现频率低且异质性高，这往往导致误诊以及它们被排除在结构化数据集之外。这需要利用非结构化文本数据进行全面分析。然而，从临床报告中手动识别是一项艰巨且本质上主观的任务。本研究提出了一种新颖的混合方法，该方法协同结合了基于词典的传统自然语言处理（NLP）工具和大型语言模型（LLMs）强大的能力，以增强从非结构化临床笔记中识别罕见疾病的能力。我们对六种不同大小和领域（通用和医学）的大型语言模型（LLMs）上的各种提示策略进行了全面评估。这种评估包括零样本、少样本和检索增强生成（RAG）技术，以增强LLMs对病人报告中上下文信息的推理和理解能力。结果表明，在罕见病识别方面具有有效性，突出了从临床笔记中识别误诊患者的潜力。</p>
<h4 id="_150">一句话总结：</h4>
<p>本研究提出了一种结合传统NLP工具和大型语言模型的混合方法，有效提升了从非结构化临床笔记中识别罕见疾病的能力。</p>
<hr />
<h2 id="synthesizrr-generating-diverse-datasets-with-retrieval-augmentation"><a href="http://arxiv.org/abs/2405.10040v2">SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation</a></h2>
<p>发布时间：2024-05-16</p>
<p>作者：Abhishek Divekar, Greg Durrett</p>
<h4 id="_151">中文摘要：</h4>
<p>由于计算和内存限制，通常希望将大型语言模型（LLMs）的能力提炼成更小的学生模型。对于分类任务，一种实现方式是通过数据集合成，这可以通过从LLM生成每个标签的示例来完成。先前的方法使用少量样本提示，这依赖于LLM的参数化知识来生成可用的示例。然而，这导致了重复、对流行实体的偏见以及与人类文本的风格差异。在这项工作中，我们提出了通过检索和细化（SynthesizRR）进行合成的方案，它使用检索增强来引入数据集合成过程中的多样性：随着检索到的段落的不同，LLM被注入不同的内容以生成其示例。我们实证研究了六个数据集的合成，涵盖了主题分类、情感分析、语调检测和幽默，需要复杂的合成策略。我们发现，与32样本提示和四种先前方法相比，SynthesizRR在提高词汇和语义多样性、与人类写作文本的相似性以及提炼性能方面有显著提升。我们将在https://github.com/amazon-science/synthesizrr发布我们的代码库。</p>
<h4 id="_152">一句话总结：</h4>
<p>SynthesizRR通过检索和细化技术显著提升了大型语言模型数据集合成的多样性和质量。</p>
<hr />
<h2 id="fintextqa-a-dataset-for-long-form-financial-question-answering"><a href="http://arxiv.org/abs/2405.09980v1">FinTextQA: A Dataset for Long-form Financial Question Answering</a></h2>
<p>发布时间：2024-05-16</p>
<p>作者：Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu, Junwei Liang</p>
<h4 id="_153">中文摘要：</h4>
<p>为了准确评估金融问答（QA）系统，需要包含多种问题类型和背景的综合数据集。然而，现有的金融QA数据集缺乏范围多样性和问题复杂性。本研究引入了FinTextQA，这是一个用于金融领域长文本问答（LFQA）的新数据集。FinTextQA包含从金融教科书和政府机构网站中提取和精选的1,262个高质量、来源标注的QA对。此外，我们开发了一个基于检索增强生成（RAG）的LFQA系统，该系统包括嵌入器、检索器、重排器和生成器。采用多方面的评估方法，包括人工排名、自动指标和GPT-4评分，以在高度噪声条件下评估不同LFQA系统配置的性能。结果表明：（1）在所有比较的生成器中，Baichuan2-7B在准确度得分上与GPT-3.5-turbo竞争激烈；（2）在我们的数据集上，最有效的系统配置是将嵌入器、检索器、重排器和生成器分别设置为Ada2、自动合并检索、Bge-Reranker-Base和Baichuan2-7B；（3）当上下文长度达到特定阈值后，模型对噪声的敏感性降低。</p>
<h4 id="_154">一句话总结：</h4>
<p>本研究提出了FinTextQA数据集，并开发了一种基于RAG的LFQA系统，通过多方面评估表明该系统在金融问答任务中具有较高的准确性和鲁棒性。</p>
<hr />
<h2 id="unirag-universal-retrieval-augmentation-for-multi-modal-large-language-models"><a href="http://arxiv.org/abs/2405.10311v1">UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models</a></h2>
<p>发布时间：2024-05-16</p>
<p>作者：Sahel Sharifymoghaddam, Shivani Upadhyay, Wenhu Chen, Jimmy Lin</p>
<h4 id="_155">中文摘要：</h4>
<p>最近，多模态（MM）大型语言模型（LLMs）解锁了许多需要多模态理解（例如，图像描述或视觉问答）和多模态生成（例如，文本引导的图像生成或编辑）能力的复杂用例。为了进一步提高MM-LLMs的输出保真度，我们引入了一种模型无关的UniRAG技术，该技术将相关检索到的信息作为推理过程中的少量示例添加到提示中。与普遍认为的检索增强（RA）主要提高对罕见实体的生成或理解能力的观点不同，我们在MSCOCO数据集上的评估结果表明，无论是GPT4和Gemini-Pro这样的专有模型，还是Llava、LaVIT和Emu2这样的较小开源模型，当它们的输入提示通过UniIR模型等MM检索器检索到的相关信息进行增强时，它们的生成质量都显著提高。</p>
<h4 id="_156">一句话总结：</h4>
<p>该研究提出了一种名为UniRAG的模型无关技术，通过在推理过程中添加相关检索信息，显著提高了MM-LLMs的生成质量。</p>
<hr />
<h2 id="im-rag-multi-round-retrieval-augmented-generation-through-learning-inner-monologues"><a href="http://arxiv.org/abs/2405.13021v1">IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues</a></h2>
<p>发布时间：2024-05-15</p>
<p>作者：Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Yi Zhang</p>
<h4 id="_157">中文摘要：</h4>
<p>尽管检索增强生成（RAG）范式可以利用外部知识来增强和巩固大型语言模型（LLMs）的输出，以减轻生成幻觉和静态知识库问题，但它们在采用具有不同能力的检索信息（IR）系统时仍存在灵活性有限的问题，在多轮检索过程中的可解释性受限，以及缺乏端到端优化。为了解决这些挑战，我们提出了一种新的以LLM为中心的方法，即IM-RAG，该方法通过学习内部独白（IM，即叙述个人思想的人的内声）来支持多轮RAG，将IR系统与LLMs集成。在IM过程中，LLM作为核心推理模型（即Reasoner）要么通过检索器提出查询以收集更多信息，要么基于对话上下文提供最终答案。我们还引入了一个Refiner，它改进了检索器的输出，有效地弥合了具有不同能力的推理器和IR模块之间的差距，并促进了多轮通信。整个IM过程通过强化学习（RL）进行优化，其中包含一个进度跟踪器以提供中间步骤奖励，并且答案预测通过监督微调（SFT）进一步单独优化。我们在HotPotQA数据集上进行了广泛的实验，HotPotQA是一个基于检索的多步问答的流行基准。结果表明，我们的方法在整合IR模块方面提供了高灵活性，同时在学习到的内部独白中展现了强大的可解释性。</p>
<h4 id="_158">一句话总结：</h4>
<p>IM-RAG通过学习内部独白和引入Refiner，实现了对检索增强生成过程的端到端优化，显著提升了检索问答的性能和可解释性。</p>
<hr />
<h2 id="exploring-the-potential-of-large-language-models-for-automation-in-technical-customer-service"><a href="http://arxiv.org/abs/2405.09161v2">Exploring the Potential of Large Language Models for Automation in Technical Customer Service</a></h2>
<p>发布时间：2024-05-15</p>
<p>作者：Jochen Wulf, Juerg Meierhofer</p>
<h4 id="_159">中文摘要：</h4>
<p>本研究旨在探讨大型语言模型（LLMs）通过自动化认知任务在技术客户服务（TCS）领域变革的潜力。研究采用原型设计方法，评估了使用LLMs（如GPT-4）在TCS中自动化认知任务的可行性，并使用了来自瑞士电信运营商的真实技术事件数据。研究发现，低级认知任务如翻译、摘要和内容生成可以有效地通过LLMs自动化，而高级任务如推理则需要更先进的技术方法，如检索增强生成（RAG）或微调；此外，研究强调了数据生态系统的重要性，通过促进不同参与者之间的数据共享，使更复杂的认知任务成为可能。本研究为服务管理中LLMs的潜力及技术可行性理论做出了贡献，为TCS部门的运营商提供了具体见解，并强调了进一步研究以解决局限性和验证LLMs在不同领域的适用性的必要性。</p>
<h4 id="_160">一句话总结：</h4>
<p>本研究探讨了大型语言模型在自动化技术客户服务认知任务中的潜力，并强调了数据生态系统在实现更复杂认知任务中的重要性。</p>
<hr />
<h2 id="tanq-an-open-domain-dataset-of-table-answered-questions"><a href="http://arxiv.org/abs/2405.07765v1">TANQ: An open domain dataset of table answered questions</a></h2>
<p>发布时间：2024-05-13</p>
<p>作者：Mubashara Akhtar, Chenxi Pang, Andreea Marzoca, Yasemin Altun, Julian Martin Eisenschlos</p>
<h4 id="_161">中文摘要：</h4>
<p>语言模型，可能通过检索等工具使用进行增强，正成为回答问题的首选方法。在现实世界的环境中理解和回答问题通常需要从不同的来源检索信息，处理和汇总数据以提取见解，并以结构化的形式如新表格、图表或信息图表呈现复杂的结果。在本文中，我们介绍了TANQ，这是第一个开放域问答数据集，其中答案需要从多个来源构建表格。我们发布了每个单元格的完整来源归属，并在开放、预言和闭卷设置中对最先进的语言模型进行了基准测试。我们表现最佳的基线GPT4达到了29.1的整体F1分数，落后于人类表现19.7个百分点。我们分析了基线在不同数据集属性上的表现，例如完成此任务所需的不同技能，包括多跳推理、数学运算和单位转换。我们进一步讨论了模型生成答案中的常见错误，指出TANQ是一个复杂任务，未来还有许多挑战。</p>
<h4 id="_162">一句话总结：</h4>
<p>本文提出了TANQ，一个需要从多个来源构建表格的开放域问答数据集，并分析了当前语言模型在处理此类复杂任务时的表现和挑战。</p>
<hr />
<h2 id="control-token-with-dense-passage-retrieval"><a href="http://arxiv.org/abs/2405.13008v1">Control Token with Dense Passage Retrieval</a></h2>
<p>发布时间：2024-05-13</p>
<p>作者：Juhwan Lee, Jisu Kim</p>
<h4 id="_163">中文摘要：</h4>
<p>本研究针对大型语言模型（LLMs）中的幻觉问题进行了探讨。我们采用了检索增强生成（RAG）技术（Lewis et al., 2020），该技术涉及在提示中嵌入相关信息以获得准确的答案。然而，RAG在检索正确信息方面也面临着固有的问题。为了解决这个问题，我们采用了密集段落检索（DPR）模型（Karpukhin et al., 2020）来获取与用户查询相关的特定领域文档。尽管如此，DPR模型在文档检索方面的准确性仍然不足。通过引入控制标记，我们对DPR模型进行了增强，与标准DPR模型相比，实现了显著优越的性能，Top-1准确率提高了13%，Top-20准确率提高了4%。</p>
<h4 id="_164">一句话总结：</h4>
<p>本研究通过引入控制标记增强了DPR模型，显著提高了大型语言模型在文档检索方面的准确性。</p>
<hr />
<h2 id="evaluation-of-retrieval-augmented-generation-a-survey"><a href="http://arxiv.org/abs/2405.07437v2">Evaluation of Retrieval-Augmented Generation: A Survey</a></h2>
<p>发布时间：2024-05-13</p>
<p>作者：Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu</p>
<h4 id="_165">中文摘要：</h4>
<p>检索增强生成（Retrieval-Augmented Generation，RAG）在自然语言处理领域近年来受到了广泛关注。许多研究和实际应用都利用其通过外部信息检索来增强生成模型的能力。然而，由于RAG系统的混合结构和对其动态知识源的依赖，评估这些系统面临着独特的挑战。为了更好地理解这些挑战，我们开展了RAG统一评估过程（A Unified Evaluation Process of RAG，Auepora），旨在提供一个关于RAG系统评估和基准的全面概述。具体来说，我们检查和比较了当前RAG基准中检索和生成组件的几个可量化指标，如相关性、准确性和忠实度，包括可能的输出和真实值对。然后，我们分析了各种数据集和指标，讨论了当前基准的局限性，并提出了推进RAG基准领域的潜在方向。</p>
<h4 id="_166">一句话总结：</h4>
<p>本研究提出了RAG统一评估过程，旨在全面评估和改进检索增强生成系统的性能。</p>
<hr />
<h2 id="from-questions-to-insightful-answers-building-an-informed-chatbot-for-university-resources"><a href="http://arxiv.org/abs/2405.08120v1">From Questions to Insightful Answers: Building an Informed Chatbot for University Resources</a></h2>
<p>发布时间：2024-05-13</p>
<p>作者：Subash Neupane, Elias Hossain, Jason Keith, Himanshu Tripathi, Farbod Ghiasi, Noorbakhsh Amiri Golilarz, Amin Amirlatifi, Sudip Mittal, Shahram Rahimi</p>
<h4 id="_167">中文摘要：</h4>
<p>本文介绍了BARKPLUG V.2，这是一个基于大型语言模型（LLM）的聊天机器人系统，它利用检索增强生成（RAG）管道构建，旨在提升学术环境中用户的信息获取体验。BARKPLUG V.2的目标是为用户提供关于各种校园资源的互动式信息，包括学术部门、项目、校园设施和大学环境中的学生资源。我们的系统利用大学数据作为外部数据语料库，并将其纳入RAG管道以进行特定领域的问答任务。我们通过定量措施，采用检索增强生成评估（RAGAS）等框架，评估了我们的系统在为密西西比州立大学生成准确和相关信息方面的有效性。此外，我们通过使用系统可用性量表（SUS）的主观满意度调查来评估该系统的可用性。我们的系统在定量性能上表现出色，平均RAGAS得分为0.96，并通过可用性评估得到了验证。</p>
<h4 id="_168">一句话总结：</h4>
<p>BARKPLUG V.2是一种基于大型语言模型的聊天机器人系统，通过检索增强生成技术，有效提升了学术环境中用户的信息获取体验。</p>
<hr />
<h2 id="pyzobot-a-platform-for-conversational-information-extraction-and-synthesis-from-curated-zotero-reference-libraries-through-advanced-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2405.07963v1">PyZoBot: A Platform for Conversational Information Extraction and Synthesis from Curated Zotero Reference Libraries through Advanced Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-05-13</p>
<p>作者：Suad Alshammari, Lama Basalelah, Walaa Abu Rukbah, Ali Alsuhibani, Dayanjan S. Wijesinghe</p>
<h4 id="_169">中文摘要：</h4>
<p>随着科学文献的指数级增长，信息过载问题日益严重，这给研究人员有效综合相关出版物带来了挑战。本文探讨了将传统参考文献管理软件与高级计算技术相结合的方法，包括大型语言模型和检索增强生成。我们介绍了PyZoBot，这是一个用Python开发的AI驱动平台，它将Zotero参考文献管理与OpenAI的复杂大型语言模型相结合。PyZoBot简化了从广泛的人类编纂的科学文献数据库中提取和综合知识的过程。它展示了处理复杂自然语言查询、整合来自多个来源的数据以及精心呈现参考文献以维护研究诚信和促进进一步探索的能力。通过利用LLM、RAG以及通过精选图书馆的人类专业知识，PyZoBot提供了一个有效解决信息过载问题并跟上快速科学进步步伐的方案。此类AI增强工具的开发有望在各种学科中显著提高研究效率和效果。</p>
<h4 id="_170">一句话总结：</h4>
<p>PyZoBot通过结合AI技术和参考文献管理软件，为研究人员提供了一种有效管理信息过载并提高研究效率的工具。</p>
<hr />
<h2 id="synthetic-test-collections-for-retrieval-evaluation"><a href="http://arxiv.org/abs/2405.07767v1">Synthetic Test Collections for Retrieval Evaluation</a></h2>
<p>发布时间：2024-05-13</p>
<p>作者：Hossein A. Rahmani, Nick Craswell, Emine Yilmaz, Bhaskar Mitra, Daniel Campos</p>
<h4 id="_171">中文摘要：</h4>
<p>测试集合在信息检索（IR）系统的评估中起着至关重要的作用。获取用于测试集合构建的多样化用户查询可能具有挑战性，而获取相关性判断（指示检索到的文档与查询的适当性）通常成本高昂且资源密集。最近，使用大型语言模型（LLMs）生成合成数据集在各种应用中受到了广泛关注。在信息检索领域，虽然先前的研究利用了LLMs的能力来生成合成查询或文档以增强训练数据并提高排名模型的性能，但使用LLMs构建合成测试集合相对较少被探索。先前的研究表明，LLMs有潜力生成用于评估IR系统的合成相关性判断。在本文中，我们全面研究了是否可以使用LLMs通过生成不仅包括合成判断还包括合成查询来构建完全合成的测试集合。特别是，我们分析了是否可以构建可靠的合成测试集合以及这种测试集合可能对基于LLM的模型展示的潜在偏差风险。我们的实验表明，使用LLMs可以构建可以可靠用于检索评估的合成测试集合。</p>
<h4 id="_172">一句话总结：</h4>
<p>本文探讨了利用大型语言模型构建完全合成的测试集合，以可靠地评估信息检索系统的性能。</p>
<hr />
<h2 id="prompt-based-code-completion-via-multi-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2405.07530v1">Prompt-based Code Completion via Multi-Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-05-13</p>
<p>作者：Hanzhuo Tan, Qi Luo, Ling Jiang, Zizheng Zhan, Jing Li, Haotian Zhang, Yuqun Zhang</p>
<h4 id="_173">中文摘要：</h4>
<p>自动代码补全旨在从未完成的代码中生成后续的标记，这一领域近年来从预训练的大型语言模型（LLMs）的进步中受益良多。然而，这些模型在处理复杂的代码逻辑或超出其训练数据范围的外推时，往往会出现连贯性和幻觉问题。现有的检索增强生成（RAG）技术通过使用一个独立的编码模型检索相关代码，其中检索到的代码片段作为代码补全的上下文参考，部分地解决了这些问题。然而，它们的检索范围受限于编码模型定义的单一视角，这很大程度上忽略了代码语义中固有的复杂性和多样性。为了解决这一局限性，我们提出了ProCC，一个利用提示工程和上下文多臂老虎机算法灵活地融入和适应代码多个视角的代码补全框架。ProCC首先采用基于提示的多检索器系统，通过制作提示模板来激发LLM的知识，以多个检索视角理解代码语义。然后，它采用自适应检索选择算法，将代码相似性纳入决策过程，以确定最适合LLM完成代码的最合适检索视角。实验结果表明，ProCC在我们的收集的开源基准套件上比最先进的代码补全技术提高了8.6%，在从一家拥有十亿用户的电子商务公司收集的私有领域基准套件上提高了10.1%，在精确匹配方面。ProCC还允许以即插即用的方式增强微调技术，在我们的研究模型上实现了5.6%的改进。</p>
<h4 id="_174">一句话总结：</h4>
<p>ProCC通过结合提示工程和多臂老虎机算法，实现了对代码语义的多元视角理解，显著提升了代码补全的精确度和效率。</p>
<hr />
<h2 id="duetrag-collaborative-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2405.13002v1">DuetRAG: Collaborative Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-05-12</p>
<p>作者：Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</p>
<h4 id="_175">中文摘要：</h4>
<p>本文提出了一种新型的协作检索增强生成（RAG）框架，名为DuetRAG。该框架旨在解决当前RAG方法在复杂领域问题（如HotPot QA）中由于缺乏相应领域知识而导致的无关知识检索问题，这些问题会导致生成质量低下。DuetRAG通过同时整合领域微调和RAG模型，以提高知识检索质量，从而提升生成质量。最后，我们在HotPot QA任务上展示了DuetRAG与专家人类研究者的匹配度。</p>
<h4 id="_176">一句话总结：</h4>
<p>DuetRAG通过结合领域微调和RAG模型，有效提升了复杂领域问题中的知识检索和生成质量。</p>
<hr />
<h2 id="sonifyar-context-aware-sound-generation-in-augmented-reality"><a href="http://arxiv.org/abs/2405.07089v2">SonifyAR: Context-Aware Sound Generation in Augmented Reality</a></h2>
<p>发布时间：2024-05-11</p>
<p>作者：Xia Su, Jon E. Froehlich, Eunyee Koh, Chang Xiao</p>
<h4 id="_177">中文摘要：</h4>
<p>声音在增强现实（AR）中扮演着至关重要的角色，它能够增强用户体验和沉浸感。然而，由于交互类型有限、收集和指定上下文信息存在挑战，以及获取匹配声音资产困难，现有的平台缺乏对AR声音创作的支持。我们提出了SonifyAR，这是一个基于大型语言模型（LLM）的AR声音创作系统，能够为AR体验生成上下文感知的声音效果。SonifyAR扩展了当前AR声音的设计空间，并实现了一个通过演示（PbD）的管道来自动收集AR事件的上下文信息，包括虚拟内容语义和现实世界上下文。然后，这些上下文信息通过一个大型语言模型进行处理，以获取推荐、检索、生成和迁移方法的声音效果。为了评估我们系统的可用性和性能，我们进行了一项包含八名参与者的用户研究，并创建了五个示例应用，包括基于AR的科学实验、提高AR头戴设备安全性的案例，以及辅助低视力AR用户的示例。</p>
<h4 id="_178">一句话总结：</h4>
<p>SonifyAR是一个基于LLM的AR声音创作系统，能够自动生成上下文感知的声音效果，以提升AR体验的沉浸感和用户体验。</p>
<hr />
<h2 id="mitigating-hallucinations-in-large-language-models-via-self-refinement-enhanced-knowledge-retrieval"><a href="http://arxiv.org/abs/2405.06545v1">Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval</a></h2>
<p>发布时间：2024-05-10</p>
<p>作者：Mengjia Niu, Hao Li, Jie Shi, Hamed Haddadi, Fan Mo</p>
<h4 id="_179">中文摘要：</h4>
<p>大型语言模型（LLMs）在各个领域展现了惊人的能力，尽管它们容易产生幻觉的特性给其在医疗保健等关键领域的应用带来了重大挑战。为了解决这个问题，从知识图谱（KGs）中检索相关事实被认为是一种有前景的方法。现有的KG增强方法往往资源密集，需要针对每个事实进行多轮检索和验证，这阻碍了它们在现实场景中的应用。在本研究中，我们提出了自我改进增强知识图谱检索（Re-KGR），以在医疗领域以更少的检索努力增强LLMs响应的事实性。我们的方法利用了不同标记和各个模型层之间的下一个标记预测概率分布的属性，主要识别出具有高幻觉潜力的标记，通过细化与这些标记相关的知识三元组来减少验证轮次。此外，我们在后处理阶段使用检索到的知识纠正不准确的内容，从而提高了生成响应的真实性。在医疗数据集上的实验结果表明，我们的方法可以增强LLMs在各种基础模型上的事实能力，如通过在真实性方面的最高得分所证明。</p>
<h4 id="_180">一句话总结：</h4>
<p>本研究提出了一种基于自我改进的知识图谱检索方法，以减少大型语言模型在医疗领域产生幻觉的风险，并提高其生成响应的真实性。</p>
<hr />
<h2 id="a-survey-on-rag-meeting-llms-towards-retrieval-augmented-large-language-models"><a href="http://arxiv.org/abs/2405.06211v3">A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models</a></h2>
<p>发布时间：2024-05-10</p>
<p>作者：Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li</p>
<h4 id="_181">中文摘要：</h4>
<p>作为人工智能领域最先进的技术之一，检索增强生成（RAG）能够提供可靠且最新的外部知识，为众多任务提供了极大的便利。特别是在人工智能生成内容（AIGC）时代，RAG在提供额外知识方面的强大检索能力使得它能够协助现有的生成式人工智能产生高质量的输出。近期，大型语言模型（LLMs）在语言理解和生成方面展现了革命性的能力，但仍然面临着固有的局限性，如幻觉和过时的内部知识。鉴于RAG在提供最新和有用辅助信息方面的强大能力，检索增强大型语言模型（RA-LLMs）应运而生，它们利用外部和权威的知识库，而不是仅仅依赖模型内部的知识，以增强LLMs的生成质量。在本调查中，我们全面回顾了RA-LLMs现有的研究，涵盖了三个主要技术视角：架构、训练策略和应用。作为初步知识，我们简要介绍了LLMs的基础和最近进展。然后，为了说明RAG对LLMs的实际意义，我们系统地回顾了主流的相关工作，按照它们的架构、训练策略和应用领域进行分类，具体阐述了每个方面的挑战以及RA-LLMs的相应能力。最后，为了提供更深入的见解，我们讨论了当前的限制以及未来研究的几个有希望的方向。关于本调查的最新信息可以在https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/找到。</p>
<h4 id="_182">一句话总结：</h4>
<p>本调查全面回顾了检索增强大型语言模型（RA-LLMs）的研究进展，探讨了其在架构、训练策略和应用方面的挑战与能力。</p>
<hr />
<h2 id="exoplanets-a-a-vo-database-for-host-stars-and-planetary-systems-the-effect-of-xuv-on-planet-atmospheres"><a href="http://arxiv.org/abs/2405.06577v1">ExoplANETS-A: A VO database for host stars and planetary systems: The effect of XUV on planet atmospheres</a></h2>
<p>发布时间：2024-05-10</p>
<p>作者：M. Morales-Calderón, S. R. G. Joyce, J. P. Pye, D. Barrado, M. García Castro, C. Rodrigo, E. Solano, J. D. Nichols, P. O. Lagage, A. Castro-González, R. A. García, M. Guedel, N. Huélamo, Y. Metodieva, R. Waters</p>
<h4 id="_183">中文摘要：</h4>
<p>ExoplANETS-A是一个欧盟Horizon-2020项目，其主要目标是建立关于系外行星大气层的新知识。与这一主题密切相关的是研究宿主恒星的辐射特性，以便了解系外行星所处的环境。本工作的目的是利用来自空间观测站和其他公共来源的存档数据，生成一组统一的恒星数据，以揭示宿主恒星对行星大气层的影响。我们收集了X射线和紫外线亮度，这些亮度影响行星的形成和大气特性，以及恒星参数，这些参数影响行星大气层特性的检索过程及其误差。我们的样本包括由HST或Spitzer观测到的所有凌星系外行星系统，包括205颗系外行星及其114颗宿主恒星。我们建立了一个目录，其中包含了从公共在线存档中提取的信息，以及由Exoplanets-A工作推导出的数量。利用这个目录，我们实施了一个在线数据库，其中还包括X射线和OHP光谱以及TESS光曲线。此外，我们还开发了一个工具，即exoVOSA，它能够拟合系外行星的光谱能量分布。我们提供了一个使用数据库来研究宿主恒星高能辐射对系外行星大气层影响的例子。样本中存在一个位于1.8地球半径处的行星半径谷，这与先前的研究一致。我们使用样本中的多行星系统来测试光蒸发模型，并发现14个系统中只有一个显著案例与该模型相矛盾（K2-3）。总之，ExoplANETS-A汇编和生成的系外行星和恒星资源为当前JWST观测以及Ariel时代未来的工作提供了一个坚实的基础。</p>
<h4 id="_184">一句话总结：</h4>
<p>ExoplANETS-A项目通过整合和分析系外行星及其宿主恒星的数据，为理解系外行星大气层特性提供了新的视角和工具。</p>
<hr />
<h2 id="can-large-language-models-understand-uncommon-meanings-of-common-words"><a href="http://arxiv.org/abs/2405.05741v1">Can large language models understand uncommon meanings of common words?</a></h2>
<p>发布时间：2024-05-09</p>
<p>作者：Jinyang Wu, Feihu Che, Xinxin Zheng, Shuai Zhang, Ruihan Jin, Shuai Nie, Pengpeng Shao, Jianhua Tao</p>
<h4 id="_185">中文摘要：</h4>
<p>大型语言模型（LLMs）如ChatGPT在包括智能对话和自主代理在内的多种自然语言理解（NLU）任务上取得了显著进步。然而，由于缺乏广泛认可的测试机制，关于“LLMs是随机鹦鹉还是真正理解世界”的问题仍然不明确，这催生了众多研究和激烈的辩论。现有研究主要关注表面层的NLU，忽略了细粒度的探索。然而，这种探索对于理解其独特的理解机制、与人类认知相一致以及最终提高LLMs的通用NLU能力至关重要。为了解决这一差距，我们的研究深入探讨了LLMs的细微语义理解能力，特别是关于常见词的不寻常含义。这一想法源于心理学中人类沟通的基础原则，强调了准确共享的词义理解。具体来说，本文提出了Lexical Semantic Comprehension（LeSC）数据集的创新构建，并引入了新颖的评估指标，这是第一个涵盖细粒度和跨语言维度的基准。我们引入了开源和闭源、不同规模和架构的模型，广泛的实证实验表明，现有模型在基本的词汇意义理解任务中的表现不佳。值得注意的是，即使是当前最先进的LLMs GPT-4和GPT-3.5，分别比16岁的人类落后了3.9%和22.3%。此外，还介绍了多种高级提示技术和检索增强生成，以帮助缓解这一问题，但局限性仍然存在。通过突出上述关键缺陷，本研究激励了进一步的调查，并为开发更智能的LLMs提供了新的见解。</p>
<h4 id="_186">一句话总结：</h4>
<p>本研究揭示了现有大型语言模型在理解常见词不寻常含义方面的不足，并提出了改进方向。</p>
<hr />
<h2 id="artificial-intelligence-as-the-new-hacker-developing-agents-for-offensive-security"><a href="http://arxiv.org/abs/2406.07561v1">Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security</a></h2>
<p>发布时间：2024-05-09</p>
<p>作者：Leroy Jacob Valencia</p>
<h4 id="_187">中文摘要：</h4>
<p>在网络安全这个广阔的领域，从被动防御转向主动攻击已成为保护数字基础设施的关键。本文探讨了人工智能（AI）在主动网络安全中的应用，特别是通过开发一个自主AI代理ReaperAI，该代理旨在模拟和执行网络攻击。利用大型语言模型（LLMs）如GPT-4的能力，ReaperAI展示了自主识别、利用和分析安全漏洞的潜力。本研究概述了可以提高一致性和性能的核心方法，包括任务驱动的渗透测试框架、AI驱动的命令生成和高级提示技术。AI代理在Python环境中运行，并利用检索增强生成（RAG）进行上下文理解和记忆保持。ReaperAI在包括Hack The Box在内的平台上进行了测试，成功利用了已知漏洞，展示了其潜在的力量。然而，AI在主动安全领域的部署面临着重大的伦理和操作挑战。代理的开发过程揭示了命令执行、错误处理和维持伦理约束的复杂性，突出了未来改进的领域。本研究通过展示AI如何增强主动安全策略，为网络安全中AI的作用讨论做出了贡献。它还提出了未来的研究方向，包括改进AI与网络安全工具的交互、增强学习机制以及讨论AI在主动角色中的伦理指南。研究结果倡导在网络安全中实施AI的独特方法，强调创新。</p>
<h4 id="_188">一句话总结：</h4>
<p>本文提出了一种利用AI增强主动网络安全策略的新方法，并通过开发自主AI代理ReaperAI展示了其在模拟和执行网络攻击方面的潜力。</p>
<hr />
<h2 id="redefining-information-retrieval-of-structured-database-via-large-language-models"><a href="http://arxiv.org/abs/2405.05508v1">Redefining Information Retrieval of Structured Database via Large Language Models</a></h2>
<p>发布时间：2024-05-09</p>
<p>作者：Mingzhu Wang, Yuzhe Zhang, Qihang Zhao, Juanyi Yang, Hong Zhang</p>
<h4 id="_189">中文摘要：</h4>
<p>检索增强在语言模型（LMs）在推理之前利用与查询相关的非参数知识库中的外部知识时至关重要。检索到的信息被作为与查询一起的上下文纳入LMs中，从而增强了针对事实问题的回答的可靠性。先前的研究在检索增强方面通常遵循检索器-生成器范式。在这种情况下，传统的检索器在从知识库中精确且无缝地提取与查询相关的信息时面临挑战。为了解决这个问题，本文介绍了一种名为ChatLR的新颖检索增强框架，该框架主要利用大型语言模型（LLMs）作为检索器的强大语义理解能力来实现精确和简洁的信息检索。此外，我们通过微调LLM在包括Text2API和API-ID识别在内的两个任务上构建了一个针对金融领域的LLM-based搜索和问答系统。实验结果表明，ChatLR在处理用户查询方面非常有效，整体信息检索准确率超过98.8%。</p>
<h4 id="_190">一句话总结：</h4>
<p>本文提出的ChatLR框架通过利用大型语言模型的语义理解能力，实现了精确的信息检索，显著提高了针对事实问题的回答的可靠性。</p>
<hr />
<h2 id="evaluating-students-open-ended-written-responses-with-llms-using-the-rag-framework-for-gpt-35-gpt-4-claude-3-and-mistral-large"><a href="http://arxiv.org/abs/2405.05444v1">Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large</a></h2>
<p>发布时间：2024-05-08</p>
<p>作者：Jussi S. Jauhiainen, Agustín Garagorry Guerra</p>
<h4 id="_191">中文摘要：</h4>
<p>评估学生开放式书面考试答案是教育工作者的一项基本且耗时的工作，这需要高度的努力、一致性和精确度。近年来，大型语言模型（LLMs）的发展为在彻底评估的同时高效利用教育工作者的时间提供了有希望的机遇。在我们的研究中，我们探讨了LLMs ChatGPT-3.5、ChatGPT-4、Claude-3和Mistral-Large在评估大学生对所研究参考资料问题的开放式答案方面的有效性。每个模型都被指示在两种条件下重复评估54个答案：10次（10-shot）的温度设置为0.0和10次温度为0.5，预计每个模型总共评估1,080次，所有模型总共评估4,320次。RAG（检索增强生成）框架被用作框架，使LLMs处理答案的评估。截至2024年春季，我们的分析显示，所研究的LLMs在一致性和评分结果方面存在显著差异。为了在教育环境中评估开放式书面答案，有必要理解LLMs的优缺点。进一步的比较研究对于确定使用LLMs进行教育评估的准确性和成本效益至关重要。</p>
<h4 id="_192">一句话总结：</h4>
<p>本研究评估了大型语言模型在评估大学生开放式书面答案方面的有效性，并揭示了其在一致性和评分结果方面的差异，强调了进一步比较研究的必要性。</p>
<hr />
<h2 id="automated-conversion-of-static-to-dynamic-scheduler-via-natural-language"><a href="http://arxiv.org/abs/2405.06697v1">Automated Conversion of Static to Dynamic Scheduler via Natural Language</a></h2>
<p>发布时间：2024-05-08</p>
<p>作者：Paul Mingzheng Tang, Kenji Kah Hoe Leong, Nowshad Shaik, Hoong Chuin Lau</p>
<h4 id="_193">中文摘要：</h4>
<p>本文探讨了大型语言模型（LLMs）在自动建模约束和生成动态调度问题的代码方面的潜在应用，这些模型基于现有的静态模型。静态调度问题通常由优化专家建模和编码。这些模型可能很快就会过时，因为底层约束可能需要微调以反映调度规则的变化。此外，为了应对环境中的干扰，可能有必要将静态模型转换为动态模型。在本文中，我们提出了一种基于检索增强生成（RAG）的LLM模型，用于自动化动态调度（RAGDyS）中约束实施的过程，无需寻求优化建模专家的帮助。我们的框架旨在最小化与数学建模相关的技术复杂性以及终端用户的工作负载，从而使用户能够快速获得一个接近原始调度且反映自然语言约束描述变化的新的调度方案。</p>
<h4 id="_194">一句话总结：</h4>
<p>本文提出了一种基于RAG的LLM模型，旨在自动化动态调度中约束的实施，降低技术复杂性和计算工作量，使用户能够快速获得新的调度方案。</p>
<hr />
<h2 id="flashbackefficient-retrieval-augmented-language-modeling-for-long-context-inference"><a href="http://arxiv.org/abs/2405.04065v3">FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference</a></h2>
<p>发布时间：2024-05-07</p>
<p>作者：Runheng Liu, Xingchen Xiao, Heyan Huang, Zewen Chi, Zhijing Wu</p>
<h4 id="_195">中文摘要：</h4>
<p>本文提出了一种名为FlashBack的模块化检索增强语言模型（RALM），该模型通过将检索到的外部语料库中的相关文档与大型语言模型（LLM）相结合，能够使LLM生成超出其预训练语料库范围的信息。与简单地将检索内容添加到输入前的方法相比，这种方法存在高运行时问题，降低了LLM的推理效率，因为它们未能有效地使用键值（KV）缓存。在本文中，我们提出了一种名为FlashBack的模块化RALM，旨在通过添加上下文模式来提高RALM的推理效率，同时在通过低秩适配进行微调后保持良好的性能。FlashBack将检索到的文档附加到上下文的末尾，以有效地利用KV缓存，而不是将它们添加到前面。我们还引入了标记标记作为两个特殊的提示标记，用于在微调期间标记附加上下文的边界。我们的实验表明，FlashBack在困惑度下可以保持良好的生成质量。在运行时测试中，FlashBack的推理速度比预添加的对应物快4倍，在7B LLM（Llama 2）上。通过避免不必要的重新计算，它通过实现显著更快的推理速度展示了进步，这种更高的效率将大大降低推理成本。</p>
<h4 id="_196">一句话总结：</h4>
<p>FlashBack通过优化上下文添加和利用KV缓存，显著提高了检索增强语言模型的推理效率。</p>
<hr />
<h2 id="robust-implementation-of-retrieval-augmented-generation-on-edge-based-computing-in-memory-architectures"><a href="http://arxiv.org/abs/2405.04700v1">Robust Implementation of Retrieval-Augmented Generation on Edge-based Computing-in-Memory Architectures</a></h2>
<p>发布时间：2024-05-07</p>
<p>作者：Ruiyang Qin, Zheyu Yan, Dewen Zeng, Zhenge Jia, Dancheng Liu, Jianbo Liu, Zhi Zheng, Ningyuan Cao, Kai Ni, Jinjun Xiong, Yiyu Shi</p>
<h4 id="_197">中文摘要：</h4>
<p>大型语言模型（LLMs）在边缘设备上通过微调和更新部分参数进行学习。尽管这种学习方法可以通过优化来减少资源利用率，但整体所需资源仍然对边缘设备构成沉重负担。相反，检索增强生成（RAG）作为一种资源高效的LLM学习方法，可以在不更新模型参数的情况下提高LLM生成内容的质量。然而，基于RAG的LLM可能在每次用户-LLM交互中都涉及对配置数据的重复搜索。这种搜索可能导致显著的延迟，并随着用户数据的积累而加剧。传统的降低延迟的努力导致限制保存的用户数据量，从而降低了RAG的可扩展性。这仍然是一个未解决的问题：如何在边缘设备上使RAG摆脱延迟和可扩展性的限制？在本文中，我们提出了一种新的框架，通过计算内存（CiM）架构来加速RAG。该框架通过在内存内部进行就地计算来加速矩阵乘法，同时避免了计算单元和内存之间昂贵的数据传输。我们的框架，即鲁棒的基于CiM的RAG（RoCR），利用了一种基于新颖对比学习的训练方法和噪声感知训练，可以使RAG利用CiM高效地搜索配置数据。据我们所知，这是第一个利用CiM来加速RAG的工作。</p>
<h4 id="_198">一句话总结：</h4>
<p>本文提出了一种基于计算内存架构的鲁棒RAG框架，以加速边缘设备上的RAG学习，从而提高LLM生成内容的效率和质量。</p>
<hr />
<h2 id="towards-accurate-and-efficient-document-analytics-with-large-language-models"><a href="http://arxiv.org/abs/2405.04674v1">Towards Accurate and Efficient Document Analytics with Large Language Models</a></h2>
<p>发布时间：2024-05-07</p>
<p>作者：Yiming Lin, Madelon Hulsebos, Ruiying Ma, Shreya Shankar, Sepanta Zeigham, Aditya G. Parameswaran, Eugene Wu</p>
<h4 id="_199">中文摘要：</h4>
<p>非结构化数据格式占据了当前存储数据的80%以上，从这些格式中提取价值仍然是一个巨大的挑战。特别是，目前管理非结构化文档的方法不支持对文档集合进行即兴分析查询。此外，直接应用于文档本身或通过检索增强生成（RAG）过程应用于文档部分的大型语言模型（LLMs）无法提供高精度的查询结果，而且在仅使用LLM的情况下，还会产生高昂的成本。由于一个集合中的许多非结构化文档通常遵循类似的模板，这些模板赋予它们共同的语义结构，因此我们引入了ZenDB，这是一个利用这种语义结构的文档分析系统，它结合了LLMs来回答文档集合上的即兴SQL查询。ZenDB能够有效地从这些模板化文档中提取语义层次结构，并引入了一种新颖的查询引擎，利用这些结构进行准确且成本效益高的查询执行。用户可以通过SQL对他们的文档施加模式，并查询它。在三个真实世界文档集合上的广泛实验表明，ZenDB具有优势，与基于LLM的基线相比，实现了高达30%的成本节约，同时保持了或提高了准确性，并且在略微更高的成本下，在精确度上超过了基于RAG的基线61%，在召回率上超过了80%。</p>
<h4 id="_200">一句话总结：</h4>
<p>ZenDB通过利用文档的语义结构并结合LLMs，实现了对非结构化文档集合的高精度和低成本查询。</p>
<hr />
<h2 id="chathuman-language-driven-3d-human-understanding-with-retrieval-augmented-tool-reasoning"><a href="http://arxiv.org/abs/2405.04533v1">ChatHuman: Language-driven 3D Human Understanding with Retrieval-Augmented Tool Reasoning</a></h2>
<p>发布时间：2024-05-07</p>
<p>作者：Jing Lin, Yao Feng, Weiyang Liu, Michael J. Black</p>
<h4 id="_201">中文摘要：</h4>
<p>众多方法被提出以检测、估计和分析图像中人的属性，包括3D姿态、形状、接触、人机交互、情绪等。这些方法各自独立工作，而非协同。本文针对这一问题，构建了一个语言驱动的对人理解系统——ChatHuman，该系统结合并整合了多种不同方法的技能。为此，我们对大型语言模型（LLM）进行微调，以便根据用户输入选择和使用各种现有工具。通过这种方式，ChatHuman能够结合来自多个工具的信息，比单个工具本身更准确地解决问题，并利用工具输出提高其对人进行推理的能力。ChatHuman的创新特性包括利用学术出版物来指导3D人体相关工具的应用、采用检索增强生成模型来生成处理新工具的情境学习示例，以及区分和整合工具结果以增强3D人体理解。我们的实验表明，ChatHuman在工具选择准确性和多个3D人体相关任务的表现上均优于现有模型。ChatHuman是将多种用于人体分析的方法整合为一个单一、强大的3D人体推理系统的步骤。</p>
<h4 id="_202">一句话总结：</h4>
<p>ChatHuman通过整合多种方法，构建了一个能够更准确理解和分析3D人体属性的语言驱动系统。</p>
<hr />
<h2 id="com3d-leveraging-cross-view-correspondence-and-cross-modal-mining-for-3d-retrieval"><a href="http://arxiv.org/abs/2405.04103v1">COM3D: Leveraging Cross-View Correspondence and Cross-Modal Mining for 3D Retrieval</a></h2>
<p>发布时间：2024-05-07</p>
<p>作者：Hao Wu, Ruochong LI, Hao Wang, Hui Xiong</p>
<h4 id="_203">中文摘要：</h4>
<p>本文研究了3D形状与文本描述之间的跨模态检索这一开放性研究课题。以往的方法主要依赖于点云编码器进行特征提取，这可能会忽略3D形状的关键固有特征，包括深度、空间层次、几何连续性等。为了解决这个问题，我们提出了COM3D，首次尝试利用跨视图对应和跨模态挖掘来增强检索性能。值得注意的是，我们通过场景表示转换器增强3D特征，以生成3D形状的跨视图对应特征，丰富了固有特征并增强了其与文本匹配的兼容性。此外，我们提出基于半硬负例挖掘方法优化跨模态匹配过程，旨在提高学习效率。大量的定量和定性实验证明了我们提出的COM3D的优越性，在Text2Shape数据集上实现了最先进的结果。</p>
<h4 id="_204">一句话总结：</h4>
<p>本文提出的COM3D方法通过跨视图对应和跨模态挖掘，有效提升了3D形状与文本描述之间的跨模态检索性能。</p>
<hr />
<h2 id="a-method-for-parsing-and-vectorization-of-semi-structured-data-used-in-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2405.03989v2">A Method for Parsing and Vectorization of Semi-structured Data used in Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-05-07</p>
<p>作者：Hang Yang, Jing Guo, Jianchuan Qi, Jinliang Xie, Si Zhang, Siqi Yang, Nan Li, Ming Xu</p>
<h4 id="_205">中文摘要：</h4>
<p>本文提出了一种新颖的方法，用于解析和矢量化半结构化数据，以增强大型语言模型（LLMs）中检索增强生成（RAG）的功能。我们开发了一个综合流程，将各种数据格式转换为.docx格式，从而实现高效的解析和结构化数据提取。我们的方法核心在于使用Pinecone构建一个矢量数据库，该数据库能够无缝集成到LLMs中，提供准确、具有特定上下文的响应，尤其是在环境管理和废水处理操作中。通过在多种文档格式中使用英语和中文文本进行严格的测试，我们的结果表明，LLMs输出的精确性和可靠性有了显著提高。RAG增强的模型显示出生成丰富上下文和精确技术响应的能力增强，突显了矢量知识库在显著提升LLMs在专业领域性能方面的潜力。这项研究不仅证明了我们方法的有效性，还突出了其在环境科学数据处理和分析领域革命性变革的潜力，为未来AI驱动应用的进步树立了先例。我们的代码可在https://github.com/linancn/TianGong-AI-Unstructure.git上获取。</p>
<h4 id="_206">一句话总结：</h4>
<p>本研究提出了一种基于矢量知识库的半结构化数据处理方法，显著提升了大型语言模型在特定领域如环境科学中的应用性能。</p>
<hr />
<h2 id="eratta-extreme-rag-for-table-to-answers-with-large-language-models"><a href="http://arxiv.org/abs/2405.03963v2">ERATTA: Extreme RAG for Table To Answers with Large Language Models</a></h2>
<p>发布时间：2024-05-07</p>
<p>作者：Sohini Roychowdhury, Marko Krema, Anvar Mahammad, Brian Moore, Arijit Mukherjee, Punit Prakashchandra</p>
<h4 id="_207">中文摘要：</h4>
<p>近年来，具有检索增强生成（RAG）功能的大型语言模型（LLMs）已成为可扩展生成人工智能解决方案的最佳选择。然而，将RAG与LLMs结合使用的用例要么过于通用，要么极端特定于某个领域，这质疑了RAG-LLM方法的可扩展性和泛化能力。在本研究中，我们提出了一种独特的基于LLM的系统，其中可以调用多个LLMs以实现数据认证、用户查询路由、数据检索以及针对高度变化且规模庞大的数据表进行定制提示以实现问答功能。我们的系统经过调整，可以从企业级数据产品中提取信息，并在10秒内提供实时响应。一个提示用于用户到数据的认证，随后是三个提示用于路由、获取数据和生成可定制的提示自然语言响应。此外，我们提出了一种五指标评分模块，用于检测和报告LLM响应中的幻觉。我们提出的系统和评分指标在可持续性、财务健康和社交媒体领域的数百个用户查询中实现了&gt;90%的置信度评分。对所提出的极端RAG架构的扩展可以使得使用LLMs进行异构源查询成为可能。</p>
<h4 id="_208">一句话总结：</h4>
<p>本研究提出了一种基于LLM的系统，通过多个LLMs实现数据认证、查询路由和数据检索，有效提高了RAG-LLM方法在可扩展性和泛化能力方面的表现。</p>
<hr />
<h2 id="self-improving-customer-review-response-generation-based-on-llms"><a href="http://arxiv.org/abs/2405.03845v1">Self-Improving Customer Review Response Generation Based on LLMs</a></h2>
<p>发布时间：2024-05-06</p>
<p>作者：Guy Azov, Tatiana Pelc, Adi Fledel Alon, Gila Kamhi</p>
<h4 id="_209">中文摘要：</h4>
<p>先前的研究表明，与用户评论的主动互动对应用用户的感知有积极影响，并鼓励他们提交修订后的评分。然而，开发者面临着管理大量评论的挑战，尤其是在每天有大量评论涌入的流行应用的情况下。因此，对旨在简化响应用户评论过程的自动化解决方案的需求日益增长。为了解决这个问题，我们开发了一个新的系统，通过利用用户贡献的文档，结合检索增强生成（RAG）和高级大型语言模型（LLMs）来生成自动回复。我们的解决方案，名为SCRABLE，代表了一种自适应的客户评论响应自动化系统，它通过自我优化的提示和基于LLMs的判断机制来增强自身。此外，我们引入了一种自动评分机制，模仿人类评估者的角色，以评估客户评论领域中生成的回复的质量。在真实世界数据集上进行的广泛实验和分析表明，我们的方法在生成高质量回复方面是有效的，与基线相比，提高了超过8.5%。通过手动检查生成的回复进行的进一步验证强调了我们所提出系统的有效性。</p>
<h4 id="_210">一句话总结：</h4>
<p>该研究开发了一种名为SCRABLE的自动回复系统，通过结合RAG和LLMs技术，有效提高了应用用户评论的响应质量。</p>
<hr />
<h2 id="lifelong-knowledge-editing-for-llms-with-retrieval-augmented-continuous-prompt-learning"><a href="http://arxiv.org/abs/2405.03279v2">Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning</a></h2>
<p>发布时间：2024-05-06</p>
<p>作者：Qizhou Chen, Taolin Zhang, Xiaofeng He, Dongyang Li, Chengyu Wang, Longtao Huang, Hui Xue</p>
<h4 id="_211">中文摘要：</h4>
<p>模型编辑旨在在不进行昂贵重新训练的情况下，纠正大型语言模型（LLMs）中的过时或错误知识。终身模型编辑是满足LLMs持续编辑需求的最具挑战性的任务。先前的研究主要关注单一或批量编辑；然而，由于灾难性的知识遗忘和模型性能的下降，这些方法在终身编辑场景中表现不足。尽管基于检索的方法可以缓解这些问题，但它们在将检索到的知识整合到模型中的缓慢和繁琐过程中受阻。在本工作中，我们引入了RECIPE，一种RetriEval增强的连续提示学习方法，以提高终身学习中的编辑效力和推理效率。RECIPE首先将知识语句转换为简短且信息丰富的连续提示，并将其作为前缀添加到LLM的输入查询嵌入中，以基于知识有效地细化响应。它进一步集成了知识哨兵（KS），作为中介计算动态阈值，以确定检索存储库是否包含相关知识。我们的检索器和提示编码器联合训练以实现编辑属性，即可靠性、通用性和局部性。在我们的实验中，RECIPE在多个LLMs和编辑数据集上进行了广泛评估，并实现了优越的编辑性能。RECIPE还展示了其维持LLMs整体性能的同时展示快速编辑和推理速度的能力。</p>
<h4 id="_212">一句话总结：</h4>
<p>RECIPE通过将知识转换为连续提示并利用知识哨兵进行动态阈值计算，实现了LLMs的终身模型编辑，提高了编辑效率和推理速度。</p>
<hr />
<h2 id="eragent-enhancing-retrieval-augmented-language-models-with-improved-accuracy-efficiency-and-personalization"><a href="http://arxiv.org/abs/2405.06683v1">ERAGent: Enhancing Retrieval-Augmented Language Models with Improved Accuracy, Efficiency, and Personalization</a></h2>
<p>发布时间：2024-05-06</p>
<p>作者：Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, Min Xu</p>
<h4 id="_213">中文摘要：</h4>
<p>检索增强生成（RAG）对于语言模型显著提升了语言理解系统。响应生成的基本检索-读取管道由于各种组件的集成而演变成一个更复杂的过程，有时甚至形成循环结构。尽管在提高响应准确性方面取得了进步，但如复杂问题的检索质量差（需要搜索多方面的语义信息）、长期服务中的知识检索效率低下以及缺乏个性化响应等挑战依然存在。为了超越这些限制，我们引入了ERAGent，这是一个在RAG领域具有突破性的框架。我们的贡献是引入了协同操作的模块：增强式问题重写和知识过滤器，以提升检索质量。通过检索触发器来限制无关的外部知识检索，同时不牺牲响应质量。ERAGent还通过整合学习到的用户档案来实现个性化响应。ERAGent的效率和个性化特性得到了经验学习模块的支持，这使得人工智能助手能够逐步扩展其知识和建模用户档案。在六个数据集和三个问答任务上的严格评估证明了ERAGent在准确性、效率和个性化方面的优越性，强调了其在推进RAG领域和其在实际系统中的应用潜力。</p>
<h4 id="_214">一句话总结：</h4>
<p>ERAGent通过引入增强式问题重写和知识过滤器，以及个性化用户档案，显著提升了检索增强生成（RAG）的准确性和效率，为语言模型提供了更强大的语言理解能力。</p>
<hr />
<h2 id="compressing-long-context-for-enhancing-rag-with-amr-based-concept-distillation"><a href="http://arxiv.org/abs/2405.03085v1">Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation</a></h2>
<p>发布时间：2024-05-06</p>
<p>作者：Kaize Shi, Xueyao Sun, Qing Li, Guandong Xu</p>
<h4 id="_215">中文摘要：</h4>
<p>大型语言模型（LLMs）在信息获取方面取得了显著进展。然而，它们对可能存在缺陷的参数化知识的过度依赖导致了幻觉和不准确，尤其是在处理长尾、特定领域的查询时。检索增强生成（RAG）通过结合外部、非参数化知识来解决这一局限性。尽管如此，检索到的长上下文文档往往包含噪声、无关信息以及关键知识，这不利于LLMs注意力的集中。受个体阅读理解中关键概念支持作用的启发，我们提出了一种基于概念的新型RAG框架，并引入了基于抽象意义表示（AMR）的概念蒸馏算法。该算法通过参考可靠的语用特征，将杂乱无章的原始检索文档压缩成由AMR信息节点提炼出的关键概念的紧凑集合。这些概念明确地约束LLMs在推理过程中仅关注关键信息。我们在开放域问答数据集上进行了广泛的实验，以实证评估所提出方法的有效性。结果表明，基于概念RAG框架优于其他基线方法，尤其是在支持文档数量增加时，同时表现出对各种骨干LLMs的鲁棒性。这强调了提炼的概念对于通过过滤干扰信息来增强RAG过程是有益的。据我们所知，这是首次将AMR引入RAG，提出了一种通过语义上下文压缩来增强推理性能的潜在解决方案。</p>
<h4 id="_216">一句话总结：</h4>
<p>本研究提出了一种基于AMR的概念蒸馏算法，用于增强RAG过程，从而提高LLMs在处理特定领域查询时的准确性和鲁棒性。</p>
<hr />
<h2 id="automatic-retrieval-augmented-generation-of-6g-network-specifications-for-use-cases"><a href="http://arxiv.org/abs/2405.03122v1">Automatic Retrieval-augmented Generation of 6G Network Specifications for Use Cases</a></h2>
<p>发布时间：2024-05-06</p>
<p>作者：Yun Tang, Weisi Guo</p>
<h4 id="_217">中文摘要：</h4>
<p>本文探讨了6G开放无线接入网络（ORAN）如何通过开放数据接口实现即插即用服务应用，其中许多应用面向消费者和企业。6G接入的开放降低了创新门槛，但也带来了挑战，即并非所有服务设计者都完全了解所需的通信规范。因此，商业创新者必须熟悉6G标准或咨询专家。实现一致、无偏见、快速且低成本的规范评估和生成对于ORAN创新生态系统至关重要。本文首先回顾了6G即插即用服务和能力、潜在用例以及大型语言模型（LLMs）的相关进展。我们确定了混合用例的充足创新空间，这些用例可能在其运行时间内需要多样化的无线功能。我们展示了网络规范的自动化可能性，并提出了第一个针对6G用例的自动检索增强规范生成（RAG）框架。为了促进公众接受和反馈，我们还发布了一个网站界面，供研究机构和工业界实验RAG框架。我们希望这篇综述强调了这一领域的需求和新兴的基础模型，并激励研究人员参与框架。</p>
<h4 id="_218">一句话总结：</h4>
<p>本文提出了一种自动化的6G规范生成框架，旨在解决网络服务提供商和商业创新者之间在服务规范生成方面的差距。</p>
<hr />
<h2 id="leveraging-lecture-content-for-improved-feedback-explorations-with-gpt-4-and-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2405.06681v1">Leveraging Lecture Content for Improved Feedback: Explorations with GPT-4 and Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-05-05</p>
<p>作者：Sven Jacobs, Steffen Jaschke</p>
<h4 id="_219">中文摘要：</h4>
<p>本文提出使用检索增强生成（RAG）技术来提升大型语言模型在编程任务中生成反馈的质量。为此，通过RAG技术将相应的课程录音转录并作为外部知识源提供给大型语言模型GPT-4，同时附带时间戳作为元信息，以防止幻觉并强制使用课程中的技术术语和短语。在一个为入门编程课程解决编程问题的练习平台上，学生可以请求GPT-4生成关于他们解决方案的反馈。为此，GPT-4接收学生的代码解决方案、编译器输出、单元测试结果以及通过RAG使用的相关课程笔记段落作为额外上下文。GPT-4生成的反馈应指导学生独立解决问题，并与课程内容相联系，使用转录的时间戳作为元信息。这样，学生可以立即在相应的位置查看相应的课程视频。在评估中，学生在研讨会上使用该工具，并决定每个反馈是否应该通过RAG进行扩展。基于问卷调查和收集到的使用数据的第一批结果表明，RAG的使用可以提高反馈生成质量，并且在某些情况下受到学生的青睐。由于反馈生成的速度较慢，其益处取决于具体情况。</p>
<h4 id="_220">一句话总结：</h4>
<p>本文通过检索增强生成技术提升了大型语言模型在编程任务中生成反馈的质量，并得到了学生的认可。</p>
<hr />
<h2 id="stochastic-rag-end-to-end-retrieval-augmented-generation-through-expected-utility-maximization"><a href="http://arxiv.org/abs/2405.02816v1">Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization</a></h2>
<p>发布时间：2024-05-05</p>
<p>作者：Hamed Zamani, Michael Bendersky</p>
<h4 id="_221">中文摘要：</h4>
<p>本文介绍了随机RAG——一种新的端到端优化检索增强生成（RAG）模型的方法，该方法放宽了大多数先前工作中提出的边际化和文档独立性等简化假设。随机RAG将RAG中的检索过程视为一个无放回的随机抽样过程。通过这种表述，我们采用了直通Gumbel-top-k方法，它为无放回抽样提供了一个可微近似的解决方案，并使得RAG的有效端到端优化成为可能。我们在七个不同数据集上进行了广泛的实验，涵盖了从开放域问答到事实核查，再到关系抽取中的槽填充和对话系统等众多任务。通过将此优化方法应用于一个最近且有效的RAG模型，我们在七个数据集中的六个上取得了最先进的结果。</p>
<h4 id="_222">一句话总结：</h4>
<p>本文提出的随机RAG方法通过放宽传统RAG模型的简化假设，实现了检索增强生成模型的有效端到端优化，并在多个数据集上取得了最先进的结果。</p>
<hr />
<h2 id="recall-them-all-retrieval-augmented-language-models-for-long-object-list-extraction-from-long-documents"><a href="http://arxiv.org/abs/2405.02732v1">Recall Them All: Retrieval-Augmented Language Models for Long Object List Extraction from Long Documents</a></h2>
<p>发布时间：2024-05-04</p>
<p>作者：Sneha Singhania, Simon Razniewski, Gerhard Weikum</p>
<h4 id="_223">中文摘要：</h4>
<p>文本关系抽取的方法大多侧重于高精度，但代价是有限的召回率。然而，高召回率对于填充与给定主题存在特定关系的对象实体长列表至关重要。在长文本中，相关对象的信息可能分散在多个段落中。这给从长文本中提取长列表带来了挑战。我们提出了L3X方法，该方法分为两个阶段来解决这一问题：（1）使用大型语言模型（LLM）和合理的检索增强技术进行召回率导向的生成，（2）进行精度导向的审查以验证或修剪候选对象。我们的L3X方法在性能上显著优于仅使用LLM的生成。</p>
<h4 id="_224">一句话总结：</h4>
<p>L3X方法通过结合大型语言模型和检索增强技术，实现了从长文本中高效提取高召回率的关系实体列表。</p>
<hr />
<h2 id="r4-reinforced-retriever-reorder-responder-for-retrieval-augmented-large-language-models"><a href="http://arxiv.org/abs/2405.02659v2">R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models</a></h2>
<p>发布时间：2024-05-04</p>
<p>作者：Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Longtao Huang, Hui Xue, Xiaofeng He, Jun Huang</p>
<h4 id="_225">中文摘要：</h4>
<p>检索增强的大型语言模型（LLMs）利用信息检索系统检索到的相关内容来生成正确的响应，旨在缓解幻觉问题。然而，现有的检索器-响应器方法通常将相关文档附加到LLMs的提示中，以执行文本生成任务，而没有考虑检索到的文档与LLMs之间细粒度结构语义的交互。这个问题对于准确响应生成尤为重要，因为当LLMs处理附加了长文档的输入提示时，往往会“中途迷失”。在这项工作中，我们提出了一种名为“强化检索-重排序-响应器”（R$^4$）的新流程，用于学习检索增强LLMs的文档顺序，从而在LLMs的大量参数保持冻结的情况下进一步增强其生成能力。根据生成响应的质量，重排序学习过程分为两个步骤：文档顺序调整和文档表示增强。具体来说，文档顺序调整旨在根据图注意力学习将检索到的文档顺序组织为开始、中间和结束位置，从而最大化响应质量的强化奖励。文档表示增强通过文档级梯度对抗学习进一步细化了检索文档的表示，以改善低质量响应。大量的实验表明，与各种公共数据集上的强大基线相比，我们提出的流程在知识密集型任务上实现了更好的事实问答性能。论文接受后，将发布源代码和训练模型。</p>
<h4 id="_226">一句话总结：</h4>
<p>提出了一种名为“强化检索-重排序-响应器”的新流程，以增强LLMs的生成能力，同时解决检索文档与LLMs之间结构语义交互的问题。</p>
<hr />
<h2 id="reasons-a-benchmark-for-retrieval-and-automated-citations-of-scientific-sentences-using-public-and-proprietary-llms"><a href="http://arxiv.org/abs/2405.02228v2">REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific Sentences using Public and Proprietary LLMs</a></h2>
<p>发布时间：2024-05-03</p>
<p>作者：Deepa Tilwani, Yash Saxena, Ali Mohammadi, Edward Raff, Amit Sheth, Srinivasan Parthasarathy, Manas Gaur</p>
<h4 id="_227">中文摘要：</h4>
<p>自动生成文档或报告中的句子引用对于情报分析师、网络安全专家、新闻机构和教育人员至关重要。在本研究中，我们探讨了大型语言模型（LLMs）是否能够根据两种形式的句子查询生成参考文献：（a）直接查询，即要求LLMs提供给定研究文章的作者姓名；（b）间接查询，即当提供来自不同文章的句子时，要求LLMs提供提及文章的标题。为了展示LLMs在此任务中的表现，我们引入了一个名为REASONS的大型数据集，该数据集包含arXiv上12个最受欢迎的科学研究领域的摘要。从大约20K篇研究文章中，我们对公共和专有LLMs进行了以下推断：（a）最先进的模型，通常被称为拟人化GPT-4和GPT-3.5，为了最小化幻觉率（HR），往往具有较高的通过率（PP）。当使用Perplexity.ai（7B）测试时，它们意外地犯了更多错误；（b）增加相关元数据降低了PP并给出了最低的HR；（c）使用Mistral进行高级检索增强生成（RAG）在间接查询上表现出一致且稳健的引用支持，并且与GPT-3.5和GPT-4的性能相匹配。所有领域和模型中的HR平均下降了41.93%，大多数情况下PP降至0%；（d）使用对抗样本进行测试表明，包括高级RAG Mistral在内的LLMs在理解上下文方面存在困难，但在Mistral和GPT-4-Preview中这个问题的影响很小。我们的研究为RAG在自动生成引用任务中的可靠性提供了宝贵的见解。</p>
<h4 id="_228">一句话总结：</h4>
<p>本研究探讨了大型语言模型在自动生成引用任务中的可靠性，并揭示了它们在处理不同查询类型时的表现和局限性。</p>
<hr />
<h2 id="incorporating-external-knowledge-and-goal-guidance-for-llm-based-conversational-recommender-systems"><a href="http://arxiv.org/abs/2405.01868v1">Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems</a></h2>
<p>发布时间：2024-05-03</p>
<p>作者：Chuang Li, Yang Deng, Hengchang Hu, Min-Yen Kan, Haizhou Li</p>
<h4 id="_229">中文摘要：</h4>
<p>本文旨在高效地使大型语言模型（LLMs）能够在对话推荐系统（CRS）任务中使用外部知识和目标引导。高级语言模型（例如，ChatGPT）在特定领域的CRS任务中存在局限性，包括1）生成具有推荐导向知识的基于事实的响应，或2）通过不同的对话目标主动引导对话。在本工作中，我们首先通过全面评估分析了这些局限性，表明外部知识和目标引导对于提高推荐准确性和语言质量至关重要。基于这一发现，我们提出了一种新的ChatCRS框架，通过实现1）一个使用工具增强方法进行外部知识库推理的知识检索代理，以及2）一个用于对话目标预测的目标规划代理，将复杂的CRS任务分解为几个子任务。在两个多目标CRS数据集上的实验结果表明，ChatCRS设定了新的最先进基准，信息性语言质量提高了17%，主动性提高了27%，推荐准确性提高了十倍。</p>
<h4 id="_230">一句话总结：</h4>
<p>本文提出了一种新的ChatCRS框架，通过外部知识和目标引导，显著提高了对话推荐系统的推荐准确性和语言质量。</p>
<hr />
<h2 id="codegrag-extracting-composed-syntax-graphs-for-retrieval-augmented-cross-lingual-code-generation"><a href="http://arxiv.org/abs/2405.02355v1">CodeGRAG: Extracting Composed Syntax Graphs for Retrieval Augmented Cross-Lingual Code Generation</a></h2>
<p>发布时间：2024-05-03</p>
<p>作者：Kounianhua Du, Renting Rui, Huacan Chai, Lingyue Fu, Wei Xia, Yasheng Wang, Ruiming Tang, Yong Yu, Weinan Zhang</p>
<h4 id="_231">中文摘要：</h4>
<p>利用大型语言模型生成代码在软件开发革命中展现出巨大的潜力。尽管通用大型语言模型展现出一定的智能，但由于自然语言与不同编程语言之间存在语法差距和词汇不匹配，其在代码生成方面的特定性仍有待提高。此外，编程语言本身具有内在的逻辑性和复杂性，使得正确生成代码变得困难。现有方法依赖于向大型语言模型提供多个提示以探索更好的解决方案，这成本高昂。在本文中，我们提出了语法图检索增强代码生成（CodeGRAG）方法，以提升LLM在单轮代码生成任务中的性能。CodeGRAG提取并总结代码块的控制流和数据流，以填补编程语言与自然语言之间的差距。提取的外部结构化知识模型化代码块的内在流程，这有助于LLM更好地理解代码语法，并作为不同编程语言之间的桥梁。CodeGRAG显著提高了LLM的代码生成能力，甚至可以为跨语言代码生成提供性能提升，例如将C++代码转换为Python代码。</p>
<h4 id="_232">一句话总结：</h4>
<p>本文提出的CodeGRAG方法通过语法图检索增强，显著提升了大型语言模型在单轮代码生成任务中的性能，并支持跨语言代码转换。</p>
<hr />
<h2 id="gaia-a-general-ai-assistant-for-intelligent-accelerator-operations"><a href="http://arxiv.org/abs/2405.01359v1">GAIA: A General AI Assistant for Intelligent Accelerator Operations</a></h2>
<p>发布时间：2024-05-02</p>
<p>作者：Frank Mayet</p>
<h4 id="_233">中文摘要：</h4>
<p>大型设备如粒子加速器通常由一支经验丰富的操作员团队运行。在粒子加速器的情况下，这些操作员既具备加速器物理方面的背景知识，也熟悉构成机器的技术。由于机器的复杂性，机器的特定子系统由专家负责，操作员可以向他们求助。在这项工作中，采用了推理和行动（ReAct）提示范式，将一个开放权重的大型语言模型（LLM）与高级机器控制系统框架以及其他工具（例如电子日志或机器设计文档）相结合。通过这种方式，实现了一个多专家检索增强生成（RAG）系统，该系统协助操作员在知识检索任务中，在需要时直接与机器交互，或编写高级控制系统脚本。这种专家知识和机器交互的整合可以简化并加快新操作员和经验丰富的操作员对机器的操作任务。</p>
<h4 id="_234">一句话总结：</h4>
<p>本研究通过结合大型语言模型和高级控制系统框架，实现了一个多专家检索增强生成系统，旨在简化并加速粒子加速器等大型设备的操作任务。</p>
<hr />
<h2 id="overcoming-llm-challenges-using-rag-driven-precision-in-coffee-leaf-disease-remediation"><a href="http://arxiv.org/abs/2405.01310v1">Overcoming LLM Challenges using RAG-Driven Precision in Coffee Leaf Disease Remediation</a></h2>
<p>发布时间：2024-05-02</p>
<p>作者：Dr. Selva Kumar S, Afifah Khan Mohammed Ajmal Khan, Imadh Ajaz Banday, Manikantha Gada, Vibha Venkatesh Shanbhag</p>
<h4 id="_235">中文摘要：</h4>
<p>本研究介绍了一种创新的基于人工智能的精准农业系统，该系统利用YOLOv8进行病害识别，并采用检索增强生成（RAG）技术进行情境感知诊断。该系统专注于解决卡纳塔克邦咖啡生产部门面临的病害挑战，将高级对象检测技术与语言模型相结合，以解决大型语言模型（LLMs）固有的限制。我们的方法不仅解决了LLMs中的幻觉问题，还引入了动态病害识别和修复策略。实时监控、协作数据集扩展和组织参与确保了系统在多样化的农业环境中的适应性。该系统的影响超越了自动化，旨在保障粮食供应、保护生计并促进环保的农业实践。通过促进精确的病害识别，系统有助于可持续和环保的农业，减少对农药的依赖。展望未来，该项目设想RAG集成对象检测系统的持续发展，强调可扩展性、可靠性和可用性。本研究旨在成为农业领域积极变革的灯塔，与全球可持续和科技增强型食品生产的努力相一致。</p>
<h4 id="_236">一句话总结：</h4>
<p>该研究开发了一种创新的AI驱动精准农业系统，旨在通过精确病害识别和环保农业实践，促进可持续农业发展。</p>
<hr />
<h2 id="biomedrag-a-retrieval-augmented-large-language-model-for-biomedicine"><a href="http://arxiv.org/abs/2405.00465v3">BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine</a></h2>
<p>发布时间：2024-05-01</p>
<p>作者：Mingchen Li, Halil Kilicoglu, Hua Xu, Rui Zhang</p>
<h4 id="_237">中文摘要：</h4>
<p>大型语言模型（LLMs）迅速成为生物医学和医疗保健领域不同应用的重要资源；然而，这些模型在生成不准确信息或幻觉方面遇到了问题。检索增强生成为这些模型提供了更新知识和提高性能的解决方案。与之前利用专用交叉注意力机制帮助LLM编码检索文本的检索增强LM不同，BiomedRAG采用了一种更简单的方法，通过直接将检索到的基于块段的文档输入到LLM中。这种简单的设计易于应用于现有的检索和语言模型，有效地绕过了检索文档中的噪声信息，尤其是在噪声密集型任务中。此外，我们展示了利用LLM监督生物医学领域检索模型的潜力，使其能够检索到帮助LLM改进预测的文档。我们的实验表明，通过调整评分器，BiomedRAG在5个生物医学NLP任务上取得了优异的性能，包括信息提取（三元组提取、关系提取）、文本分类、链接预测和问答，利用了超过9个数据集。例如，在三元组提取任务中，BiomedRAG在GIT和ChemProt语料库上分别以81.42和88.83的微F1分数优于其他三元组提取系统。</p>
<h4 id="_238">一句话总结：</h4>
<p>BiomedRAG通过直接输入检索到的文档到LLM中，有效地提高了生物医学NLP任务的性能。</p>
<hr />
<h2 id="rag-based-explainable-prediction-of-road-users-behaviors-for-automated-driving-using-knowledge-graphs-and-large-language-models"><a href="http://arxiv.org/abs/2405.00449v1">RAG-based Explainable Prediction of Road Users Behaviors for Automated Driving using Knowledge Graphs and Large Language Models</a></h2>
<p>发布时间：2024-05-01</p>
<p>作者：Mohamed Manzour Hussien, Angie Nataly Melo, Augusto Luis Ballardini, Carlota Salinas Maldonado, Rubén Izquierdo, Miguel Ángel Sotelo</p>
<h4 id="_239">中文摘要：</h4>
<p>近年来，在自动驾驶背景下预测道路使用者行为受到了科学界的广泛关注。大多数研究工作仅基于运动学信息进行行为预测，这种简化了现实的方法，因为道路使用者是人类，他们高度受到周围环境的影响。此外，大量研究工作依赖于强大的深度学习技术，这些技术在预测任务中表现出高指标，但可能缺乏充分理解和利用道路场景中包含的上下文语义信息的能力，更不用说它们无法提供人类可理解的解释性预测。在本工作中，我们提出了一种可解释的道路使用者行为预测系统，该系统通过使用检索增强生成（RAG）技术，集成了知识图谱（KG）的推理能力和大型语言模型（LLM）的表达能力。为此，知识图谱嵌入（KGE）和贝叶斯推理相结合，允许部署一个完全归纳推理系统，该系统能够发布依赖于图中包含的旧信息以及由车载传感器实时收集的当前证据的预测。根据提出的方法实现了两个用例：1）预测行人的过街行为；2）预测车道变更操作。在这两种情况下，所达到的性能在预测提前量和F1分数方面都超过了当前的最佳水平，显示出该领域未来研究的希望之路。</p>
<h4 id="_240">一句话总结：</h4>
<p>本研究提出了一种结合知识图谱和大型语言模型的可解释道路使用者行为预测系统，显著提升了预测准确性和可解释性。</p>
<hr />
<h2 id="courseassist-pedagogically-appropriate-ai-tutor-for-computer-science-education"><a href="http://arxiv.org/abs/2407.10246v3">CourseAssist: Pedagogically Appropriate AI Tutor for Computer Science Education</a></h2>
<p>发布时间：2024-05-01</p>
<p>作者：Ty Feng, Sa Liu, Dipak Ghosal</p>
<h4 id="_241">中文摘要：</h4>
<p>随着计算机科学课程报名人数的增加和班级规模的扩大，需要可扩展、自动化的辅导解决方案来充分支持学生的学习。尽管大型语言模型（LLMs）如GPT-4在通过问答协助学生方面显示出潜力，但教育工作者对学生的过度依赖、对生成代码的误解以及不准确答案的风险表示担忧。我们主张采取一种建设性的方法，利用AI的能力同时减轻潜在风险，而不是直接禁止这些工具。本海报介绍了CourseAssist，这是一个针对计算机科学教育定制的基于LLM的辅导系统。与通用的LLM系统不同，CourseAssist使用检索增强生成、用户意图分类和问题分解，将AI的响应与特定的课程材料和教学目标对齐，从而确保LLM在教育环境中的教学适宜性。我们使用来自编程语言课程的50个问答对数据集，以有用性、准确性和教学适宜性为标准，对CourseAssist与GPT-4的基线进行了评估。评估结果显示，CourseAssist显著优于基线，证明了其作为有效学习助手的潜力。我们还在一所大型公立R1研究大学的6门计算机科学课程中部署了CourseAssist，覆盖了超过500名学生。对20名用户学生的访谈显示，CourseAssist通过提高课程特定的辅导帮助的可访问性和缩短编程作业的反馈循环，改善了计算机科学教学。未来的工作将包括在更多大学进行广泛的试点测试，并探索学生、教育工作者和AI之间更好的协作关系，以改善计算机科学学习体验。</p>
<h4 id="_242">一句话总结：</h4>
<p>CourseAssist是一种创新的基于LLM的辅导系统，旨在提高计算机科学教育的质量和效率。</p>
<hr />
<h2 id="distance-sampling-based-paraphraser-leveraging-chatgpt-for-text-data-manipulation"><a href="http://arxiv.org/abs/2405.00367v1">Distance Sampling-based Paraphraser Leveraging ChatGPT for Text Data Manipulation</a></h2>
<p>发布时间：2024-05-01</p>
<p>作者：Yoori Oh, Yoseob Han, Kyogu Lee</p>
<h4 id="_243">中文摘要：</h4>
<p>随着对音频语言检索研究的兴趣日益增长，其目标是建立音频和文本模态之间的关联。然而，与音频样本相比，大多数音频-文本配对数据集往往缺乏丰富的文本数据表达。音频-文本数据集面临的一个显著挑战是，尽管音频样本不同，但存在相似或相同的字幕。因此，在多对一映射条件下，音频-文本数据集会导致检索任务性能不佳。在本文中，我们提出了一种新颖的方法来解决音频语言检索任务中的数据不平衡问题。为了克服这一限制，我们引入了一种方法，该方法采用基于距离采样的释义器，利用ChatGPT，通过距离函数生成可控的操纵文本数据分布。对于一组具有相同上下文的句子，使用距离来计算任意两个句子之间的操纵程度，并使用由Jaccard相似性定义的具有相似距离的文本聚类执行ChatGPT的少样本提示。因此，当ChatGPT应用于与文本聚类相关的少样本提示时，可以根据距离调整操纵文本的多样性。所提出的方法被证明可以显著提高音频-文本检索的性能，优于传统的文本增强技术。</p>
<h4 id="_244">一句话总结：</h4>
<p>本文提出了一种基于ChatGPT的音频语言检索方法，通过距离采样和文本聚类技术有效解决了数据不平衡问题，显著提升了检索性能。</p>
<hr />
<h2 id="integrating-ai-in-higher-education-protocol-for-a-pilot-study-with-samcares-an-adaptive-learning-hub"><a href="http://arxiv.org/abs/2405.00330v1">Integrating A.I. in Higher Education: Protocol for a Pilot Study with 'SAMCares: An Adaptive Learning Hub'</a></h2>
<p>发布时间：2024-05-01</p>
<p>作者：Syed Hasib Akhter Faruqui, Nazia Tasnim, Iftekhar Ibne Basith, Suleiman Obeidat, Faruk Yildiz</p>
<h4 id="_245">中文摘要：</h4>
<p>学习永无止境，自我提升没有年龄限制。然而，教育领域在有效满足学生包容性和多样化的学习需求方面可能会面临挑战。这些学生应能够接触到最先进的讲座交付方法、在线资源和科技需求。然而，在多样化的学习资源中，学生在短时间内理解大量知识变得更加困难。传统的辅助技术和学习辅助工具通常缺乏个性化教育计划所需的动态适应性。大型语言模型（LLM）已被用于语言翻译、文本摘要和内容生成应用。随着近年来人工智能的快速发展，AI驱动的聊天机器人和虚拟助手已经被开发出来。本研究旨在通过引入一个创新的学习伙伴，我们将称之为“SAMCares”，来弥合这一差距。该系统利用大型语言模型（LLM）（在我们的案例中，以LLaMa-2 70B作为基础模型）和检索增强生成（RAG）来提供实时、上下文感知和自适应的教育支持。模型的上下文将限制在Sam Houston State University（SHSU）课程笔记的知识库中。LLM组件使得能够以聊天的方式与之互动，以满足每个学生的独特学习需求。为此，我们将构建一个定制的基于Web的图形用户界面。同时，RAG增强了实时信息检索和文本生成，从而提供更准确和上下文特定的辅助。在需要额外知识支持的情况下，Web GUI中增加了上传额外学习材料的功能。该系统的有效性将通过控制试验和迭代反馈机制进行评估。</p>
<h4 id="_246">一句话总结：</h4>
<p>本研究开发了一个名为“SAMCares”的创新学习伙伴系统，利用大型语言模型和检索增强生成技术，为大学生提供实时、上下文感知和自适应的教育支持。</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
    
  </body>
</html>