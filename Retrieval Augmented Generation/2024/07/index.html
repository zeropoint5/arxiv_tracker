
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../08/">
      
      
        <link rel="next" href="../06/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.31">
    
    
      
        <title>2024-07(126) - ArXiv Tracker</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="ArXiv Tracker" class="md-header__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ArXiv Tracker
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2024-07(126)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Emission%20Trading%20System/2024/07/" class="md-tabs__link">
          
  
  Emission Trading System

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../08/" class="md-tabs__link">
          
  
  Retrieval Augmented Generation

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="ArXiv Tracker" class="md-nav__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ArXiv Tracker
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Emission Trading System
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Emission Trading System
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2016/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2011/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2011-10(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Retrieval Augmented Generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Retrieval Augmented Generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-08(19)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2024-07(126)
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(152)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-05(121)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-04(86)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-03(70)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(87)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(50)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(33)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-11(38)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(43)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(20)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-08(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-07(11)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(33)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-04(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-02(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(15)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-09(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-07(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-05(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-04(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-12(9)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-09(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-08(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-06(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-06(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-04(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-12(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2015/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2012/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2012-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2009/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2009-03(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="retrieval-augmented-generation-202407">Retrieval Augmented Generation - 2024年07月</h1>
<h2 id="adaptive-retrieval-augmented-generation-for-conversational-systems"><a href="http://arxiv.org/abs/2407.21712v1">Adaptive Retrieval-Augmented Generation for Conversational Systems</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Xi Wang, Procheta Sen, Ruizhe Li, Emine Yilmaz</p>
<h4 id="_1">中文摘要：</h4>
<p>尽管将大型语言模型集成到对话系统开发中取得了成功，但许多研究表明，检索和增强外部知识对于生成信息性响应是有效的。因此，许多现有研究通常假设在对话系统中始终需要检索增强生成（RAG）而没有明确的控制。这引发了一个关于这种必要性的研究问题。在本研究中，我们旨在调查系统响应的每一轮是否都需要增强外部知识。特别是，通过利用人类对自适应增强的二值选择的判断，我们开发了RAGate（一种门控模型），该模型通过模拟对话上下文和相关信息来预测对话系统是否需要RAG以改善响应。我们对设计并应用RAGate到对话模型以及不同对话场景的全面分析进行了广泛的实验。我们的实验结果和分析表明，RAGate在基于RAG的对话系统中有效地识别了系统响应，以实现高质量的响应和高生成置信度。本研究还确定了生成置信水平与增强知识的相关性。</p>
<h4 id="_2">一句话总结：</h4>
<p>本研究提出了一种名为RAGate的门控模型，用于预测对话系统何时需要检索增强生成（RAG），以提高响应质量和生成置信度。</p>
<hr />
<h2 id="towards-achieving-human-parity-on-end-to-end-simultaneous-speech-translation-via-llm-agent"><a href="http://arxiv.org/abs/2407.21646v1">Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Shanbo Cheng, Zhichao Huang, Tom Ko, Hang Li, Ningxin Peng, Lu Xu, Qini Zhang</p>
<h4 id="_3">中文摘要：</h4>
<p>本文提出了一种名为跨语言代理——同步口译（Cross Language Agent -- Simultaneous Interpretation，CLASI）的高质量、类似人类的同步语音翻译（SiST）系统。受专业人工译员启发，我们采用一种新颖的数据驱动读写策略来平衡翻译质量和延迟。为了解决翻译领域术语的挑战，CLASI采用多模态检索模块来获取相关信息以增强翻译。在LLMs的支持下，我们的方法通过考虑输入音频、历史上下文和检索信息来生成容错翻译。实验结果表明，我们的系统在性能上显著优于其他系统。与专业人工译员一致，我们使用更好的人类评估指标——有效信息比例（valid information proportion，VIP）来评估CLASI，该指标衡量能够成功传达给听众的信息量。在现实场景中，演讲通常是不流畅、非正式和不清晰的，CLASI在中文到英文和英文到中文翻译方向上分别实现了81.3%和78.0%的VIP。相比之下，最先进的商业或开源系统仅实现了35.4%和41.6%。在极具挑战性的数据集上，其他系统VIP低于13%，而CLASI仍能实现70%的VIP。</p>
<h4 id="_4">一句话总结：</h4>
<p>CLASI是一种创新的同步语音翻译系统，通过多模态检索和LLMs支持，实现了高VIP值，显著优于现有系统。</p>
<hr />
<h2 id="mllm-is-a-strong-reranker-advancing-multimodal-retrieval-augmented-generation-via-knowledge-enhanced-reranking-and-noise-injected-training"><a href="http://arxiv.org/abs/2407.21439v1">MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Zhanpeng Chen, Chengjin Xu, Yiyan Qi, Jian Guo</p>
<h4 id="_5">中文摘要：</h4>
<p>多模态大型语言模型（MLLMs）在处理和生成跨多种数据模态（包括文本、图像、音频和视频）的内容方面表现出卓越的能力。然而，MLLMs的一个显著缺点是它们依赖于静态的训练数据，导致信息过时和有限的上下文意识。这种静态性质阻碍了它们提供准确、及时响应的能力，尤其是在动态或快速发展的环境中。整合多模态检索增强生成（Multimodal RAG）提供了一种有希望的解决方案，但该系统不可避免地会遇到多粒度噪声对应（MNC）问题，这涉及到两种类型的噪声：粗粒度（查询-字幕）和细粒度（查询-图像）。这种噪声阻碍了准确的检索和生成。在这项工作中，我们提出了一个名为RagLLaVA的新框架，该框架具有知识增强的重排序和噪声注入训练，以解决这些限制。我们使用一个简单而有效的指令模板对MLLM进行指令微调，以诱导其排名能力，并将其作为重排序器精确地过滤前k个检索到的图像。对于生成，我们在数据和标记级别注入视觉噪声，以增强生成器的鲁棒性。我们在需要检索和推理图像以回答给定查询的两个数据集的子集上进行了大量实验。我们的结果表明，RagLLaVA在准确检索和稳健生成方面具有优越性。代码和模型可在https://github.com/IDEA-FinAI/RagLLaVA上找到。</p>
<h4 id="_6">一句话总结：</h4>
<p>RagLLaVA通过知识增强的重排序和噪声注入训练，有效提升了多模态大型语言模型在动态环境下的检索准确性和生成鲁棒性。</p>
<hr />
<h2 id="multi-level-querying-using-a-knowledge-pyramid"><a href="http://arxiv.org/abs/2407.21276v1">Multi-Level Querying using A Knowledge Pyramid</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Rubing Chen, Xulu Zhang, Jiaxin Wu, Wenqi Fan, Xiao-Yong Wei, Qing Li</p>
<h4 id="_7">中文摘要：</h4>
<p>本文针对现有检索增强生成（RAG）方法主要关注提高召回率而缺乏精度的需求，提出了一种在RAG框架内的多层知识金字塔方法，以实现精度和召回率之间的更好平衡。知识金字塔包括三个层次：本体（Ontologies）、知识图谱（Knowledge Graphs, KGs）和基于块的基础文本。我们采用跨层增强技术以实现全面的知识覆盖和本体模式和实例的动态更新。为确保紧凑性，我们在知识图谱中利用跨层过滤方法进行知识浓缩。我们的方法，称为PolyRAG，遵循瀑布模型进行检索，从金字塔的顶部开始，逐步向下进行，直到获得一个自信的答案。我们引入了两个针对特定领域的知识检索基准，一个在学术领域，另一个在金融领域。通过在19个SOTA方法中表现优异的全面实验验证了方法的有效性。一个鼓舞人心的观察结果是，所提出的方法增强了GPT-4，通过提高其性能从0.1636到0.8109，实现了395%的F1增益。</p>
<h4 id="_8">一句话总结：</h4>
<p>本文提出的多层知识金字塔方法PolyRAG，通过增强GPT-4的性能，显著提高了检索增强生成（RAG）的精度和召回率。</p>
<hr />
<h2 id="finch-prompt-guided-key-value-cache-compression"><a href="http://arxiv.org/abs/2408.00167v1">Finch: Prompt-guided Key-Value Cache Compression</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Giulio Corallo, Paolo Papotti</p>
<h4 id="_9">中文摘要：</h4>
<p>近年来，大型语言模型的应用，如检索增强生成和聊天机器人，导致了对处理更长的输入上下文的需求增加。然而，这一需求受到固有局限性的阻碍。从架构上讲，模型在训练期间被一个定义好的上下文窗口所限制。此外，处理大量文本需要大量的GPU内存。我们提出了一种新颖的方法，Finch，通过利用预训练模型的自注意力权重来压缩输入上下文。给定一个提示和一个长文本，Finch会迭代地识别基于提示的文本块中最相关的键（K）和值（V）对。只有这样的对被存储在KV缓存中，在上下文窗口的空间限制内，最终包含长文本的压缩版本。我们的提议使得模型能够在不进行微调的情况下，即使在高压缩率（高达93倍）下也能处理大量输入，同时保持语义完整性。</p>
<h4 id="_10">一句话总结：</h4>
<p>Finch通过利用预训练模型的自注意力权重，实现了对长输入上下文的压缩处理，从而允许模型在保持语义完整性的同时处理大量数据。</p>
<hr />
<h2 id="tabular-data-augmentation-for-machine-learning-progress-and-prospects-of-embracing-generative-ai"><a href="http://arxiv.org/abs/2407.21523v1">Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Lingxi Cui, Huan Li, Ke Chen, Lidan Shou, Gang Chen</p>
<h4 id="_11">中文摘要：</h4>
<p>表格数据上的机器学习（ML）应用广泛，然而获取大量高质量的表格数据用于模型训练仍然是一个重大挑战。众多研究工作集中于表格数据增强（TDA）以通过添加额外数据来增强原始表格，从而提高下游机器学习任务。最近，利用生成式人工智能（Generative AI）在TDA方面的能力引起了越来越多的兴趣。因此，我们认为现在是时候提供一个关于TDA进展和未来前景的全面回顾，特别强调趋势中的生成式人工智能。具体来说，我们提出了TDA管道的架构视图，包括三个主要步骤：预增强、增强和后增强。预增强包括便于后续TDA的准备任务，如错误处理、表格标注、表格简化、表格表示、表格索引、表格导航、模式匹配和实体匹配。增强系统性地分析了当前的TDA方法，这些方法分为基于检索的方法，它们检索外部数据，以及基于生成的方法，它们生成合成数据。我们进一步根据增强过程在行、列、单元格和表格级别的粒度对这些方法进行细分。后增强专注于TDA的数据集、评估和优化方面。我们还总结了TDA的当前趋势和未来方向，突出了生成式人工智能时代中的有希望的机会。此外，相关的论文和资源在GitHub仓库https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation中持续更新和维护，以反映该领域的持续进步。</p>
<h4 id="_12">一句话总结：</h4>
<p>本文全面回顾了表格数据增强（TDA）的进展和未来前景，特别关注生成式人工智能在TDA中的应用。</p>
<hr />
<h2 id="kemenkeugpt-leveraging-a-large-language-model-on-indonesias-government-financial-data-and-regulations-to-enhance-decision-making"><a href="http://arxiv.org/abs/2407.21459v1">KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Gilang Fajar Febrian, Grazziela Figueredo</p>
<h4 id="_13">中文摘要：</h4>
<p>数据对于基于证据的政策制定和提升公共服务（包括印度尼西亚共和国财政部）至关重要。然而，政府财政数据和法规的复杂性和动态性可能会阻碍决策过程。本研究探讨了大型语言模型（LLMs）解决这些挑战的潜力，重点关注印度尼西亚的财政数据和法规。尽管LLMs在金融领域效果显著，但其在印度尼西亚公共部门的应用尚未得到探索。本研究通过LangChain的检索增强生成（RAG）、提示工程和微调，进行迭代开发KemenkeuGPT。数据集从2003年至2023年收集自印度尼西亚财政部、统计局和国际货币基金组织（IMF）。对财政部官员的调查和访谈为模型提供了信息、增强了模型性能并进行了微调。我们通过人工反馈、基于LLMs的评估和基准测试来评估模型。模型的准确率从35%提高到了61%，正确率从48%提高到了64%。检索增强生成评估（RAGAS）框架显示，KemenkeuGPT实现了44%的正确率，73%的忠实度，40%的精确度和60%的召回率，优于其他几个基础模型。财政部专家的访谈表明，KemenkeuGPT有潜力成为决策的重要工具。这些结果预计会随着持续的人工反馈而进一步改善。</p>
<h4 id="_14">一句话总结：</h4>
<p>本研究通过开发KemenkeuGPT，利用大型语言模型在印度尼西亚财政部数据上实现了决策支持工具的显著提升。</p>
<hr />
<h2 id="metaopenfoam-an-llm-based-multi-agent-framework-for-cfd"><a href="http://arxiv.org/abs/2407.21320v1">MetaOpenFOAM: an LLM-based multi-agent framework for CFD</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Yuxuan Chena, Xu Zhua, Hua Zhoua, Zhuyin Rena</p>
<h4 id="_15">中文摘要：</h4>
<p>在基于大型语言模型（LLMs）的智能体社会中，自动问题解决取得了显著进展。计算流体动力学（CFD）作为一个复杂问题，在自动模拟中面临着独特的挑战，需要复杂的解决方案。MetaOpenFOAM作为一个新颖的多智能体协作框架，旨在仅通过自然语言输入来完成CFD模拟任务，包括网格预处理、模拟和后处理等。MetaOpenFOAM利用MetaGPT的流水线范式，为各种智能体分配不同的角色，有效地将复杂的CFD任务分解成可管理的子任务。Langchain通过整合检索增强生成（RAG）技术进一步补充了MetaOpenFOAM，通过为LLM整合一个可搜索的OpenFOAM教程数据库来增强框架的能力。在基于自然语言的CFD求解器的基准测试中，包括8个CFD模拟任务，MetaOpenFOAM实现了每个测试的高通过率（85%），每个测试案例的平均成本仅为0.22美元。这8个CFD模拟任务包括可压缩和不可压缩流动、二维和三维流动、热传递和燃烧，展示了仅使用自然语言输入自动化CFD模拟并迭代纠正错误以实现低成本所需模拟的能力。进行了一项消融研究，以验证多智能体系统中每个组件以及RAG技术的必要性。对LLM随机性的敏感性研究表明，低随机性的LLM可以获得更稳定和准确的结果。此外，MetaOpenFOAM具有识别和修改用户要求中的关键参数的能力，在发生故障时，无论是否有人的参与，都能出色地纠正错误，这证明了MetaOpenFOAM的泛化能力。</p>
<h4 id="_16">一句话总结：</h4>
<p>MetaOpenFOAM通过自然语言输入自动化CFD模拟，实现了高效、低成本且具有泛化能力的解决方案。</p>
<hr />
<h2 id="implementing-streaming-algorithm-and-k-means-clusters-to-rag"><a href="http://arxiv.org/abs/2407.21300v2">Implementing Streaming algorithm and k-means clusters to RAG</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang</p>
<h4 id="_17">中文摘要：</h4>
<p>检索增强生成（RAG）在信息检索中取得了巨大成功，因为它构建了一个外部知识数据库。然而，它也存在许多问题：由于数据库巨大，它消耗了大量的内存。当面对大量流数据时，它无法及时更新已建立的索引数据库。为了在构建数据库的同时节省内存并保持准确性，我们提出了一种结合流算法和k-means聚类与RAG的新方法。我们的方法将流算法应用于更新索引并减少内存消耗。然后使用k-means算法将具有高度相似性的文档聚在一起，这样做可以缩短查询时间。我们在四种方法上进行了比较实验，结果表明，结合流算法和k-means聚类的RAG在准确性和内存消耗方面表现良好。对于大量流数据，我们发现我们的方法比传统的RAG表现更佳。</p>
<h4 id="_18">一句话总结：</h4>
<p>本研究提出了一种结合流算法和k-means聚类的RAG新方法，有效提高了大规模流数据的检索效率和准确性。</p>
<hr />
<h2 id="multi-level-querying-using-a-knowledge-pyramid_1"><a href="http://arxiv.org/abs/2407.21276v2">Multi-Level Querying using A Knowledge Pyramid</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Rubing Chen, Xulu Zhang, Jiaxin Wu, Wenqi Fan, Xiao-Yong Wei, Qing Li</p>
<h4 id="_19">中文摘要：</h4>
<p>本文针对现有检索增强生成（RAG）方法主要关注提升召回率而缺乏精度的需求，提出了一种在RAG框架内的多层知识金字塔方法，以实现精度和召回率的更好平衡。知识金字塔由三个层次组成：本体（Ontologies）、知识图谱（Knowledge Graphs, KGs）和基于块段的原始文本。我们采用跨层增强技术以实现全面的知识覆盖和本体模式和实例的动态更新。为确保紧凑性，我们在知识图谱中利用跨层过滤方法进行知识浓缩。我们的方法，命名为PolyRAG，遵循瀑布模型进行检索，从金字塔的顶部开始，逐步向下直到获得一个自信的答案。我们引入了两个针对特定领域的知识检索基准，一个在学术领域，另一个在金融领域。通过在19个SOTA方法中表现优异的综合实验验证了方法的有效性。一个鼓舞人心的观察结果是，所提出的方法增强了GPT-4，通过提高其性能从0.1636提升到0.8109，实现了395%的F1增益。</p>
<h4 id="_20">一句话总结：</h4>
<p>本文提出的多层知识金字塔方法PolyRAG，通过跨层增强和过滤技术，显著提升了RAG的检索精度，并有效增强了GPT-4的性能。</p>
<hr />
<h2 id="metaopenfoam-an-llm-based-multi-agent-framework-for-cfd_1"><a href="http://arxiv.org/abs/2407.21320v2">MetaOpenFOAM: an LLM-based multi-agent framework for CFD</a></h2>
<p>发布时间：2024-07-31</p>
<p>作者：Yuxuan Chen, Xu Zhu, Hua Zhou, Zhuyin Ren</p>
<h4 id="_21">中文摘要：</h4>
<p>在基于大型语言模型（LLMs）的智能体社会中，自动化问题解决取得了显著进展。计算流体动力学（CFD）作为一个复杂的问题，在自动化模拟中面临着独特的挑战，需要复杂的解决方案。MetaOpenFOAM作为一个新颖的多智能体协作框架，旨在仅通过自然语言输入来完成CFD模拟任务，包括网格预处理、模拟等。MetaOpenFOAM利用MetaGPT的流水线范式，为各种智能体分配不同的角色，有效地将复杂的CFD任务分解成可管理的子任务。Langchain通过整合检索增强生成（RAG）技术进一步补充了MetaOpenFOAM，通过为LLM整合一个可搜索的OpenFOAM教程数据库来增强框架的能力。在基于自然语言的CFD求解器的基准测试中，包括八个CFD模拟任务，MetaOpenFOAM实现了每个测试的高通过率（85%），每个测试案例的平均成本仅为0.22美元。这八个CFD模拟任务涵盖了多种多维流动问题，包括可压缩和不可压缩流动以及不同的物理过程。这证明了仅使用自然语言输入自动化CFD模拟的能力，通过迭代纠正错误以达到预期的模拟效果。进行了一项消融研究，以验证多智能体系统中每个组件以及RAG技术的必要性。对LLM随机性的敏感性研究表明，低随机性的LLM可以获得更稳定和准确的结果。此外，MetaOpenFOAM还具有识别和修改用户需求中关键参数的能力，在失败匹配发生时表现出卓越的纠错能力，这证明了MetaOpenFOAM的泛化能力。</p>
<h4 id="_22">一句话总结：</h4>
<p>MetaOpenFOAM通过自然语言输入自动化CFD模拟，实现了高效、准确的计算流体动力学模拟。</p>
<hr />
<h2 id="industrial-grade-smart-troubleshooting-through-causal-technical-language-processing-a-proof-of-concept"><a href="http://arxiv.org/abs/2407.20700v1">Industrial-Grade Smart Troubleshooting through Causal Technical Language Processing: a Proof of Concept</a></h2>
<p>发布时间：2024-07-30</p>
<p>作者：Alexandre Trilla, Ossee Yiboe, Nenad Mijatovic, Jordi Vitrià</p>
<h4 id="_23">中文摘要：</h4>
<p>本文描述了一种基于经验回报记录中表达的技术语言对工业环境进行故障诊断的方法。提出的方法利用了大型语言模型分布式表示中包含的矢量化语言知识，以及工业资产嵌入式故障模式和机制所蕴含的因果关系。论文提出了该解决方案的基本但关键概念，该解决方案被构想为一个因果感知的检索增强生成系统，并在一个真实世界的预测性维护场景中进行了实验验证。最后，讨论了提高所利用因果技术成熟度的途径，以满足工业中日益复杂场景的鲁棒性挑战。</p>
<h4 id="_24">一句话总结：</h4>
<p>本文提出了一种基于大型语言模型和因果关联的工业环境故障诊断方法，通过因果感知的检索增强生成系统实现，并探讨了其在复杂工业场景中的应用前景。</p>
<hr />
<h2 id="mimicking-the-mavens-agent-based-opinion-synthesis-and-emotion-prediction-for-social-media-influencers"><a href="http://arxiv.org/abs/2407.20668v1">Mimicking the Mavens: Agent-based Opinion Synthesis and Emotion Prediction for Social Media Influencers</a></h2>
<p>发布时间：2024-07-30</p>
<p>作者：Qinglan Wei, Ruiqi Xue, Yutian Wang, Hongjiang Xiao, Yuhao Wang, Xiaoyan Duan</p>
<h4 id="_25">中文摘要：</h4>
<p>预测社交媒体上影响者（influencers）的观点和公众情绪对于预期社会趋势及指导战略应对至关重要。本研究介绍了一个新的计算框架，用于预测意见领袖的观点和公众的情感反应，解决了在线交流中固有的非结构化、上下文敏感和异质性问题。我们的研究引入了一个创新模块，从自动5W1H（何地、何人、何时、何事、何因、如何）问题生成引擎开始，针对新兴新闻故事和热门话题定制。我们随后在六个领域中建立了总共60个匿名意见领袖代理，并基于增强的大型语言模型（LLM）结合检索增强生成（RAG）实现观点生成。随后，我们综合了意见领袖的潜在观点，并预测了对不同事件的情感反应。我们的自动5W1H模块的有效性得到了平均GPT-4评分8.83/10的验证，显示出高保真度。影响者代理表现出一致的性能，在评估指标上平均获得了6.85/10的GPT-4评分。利用‘俄乌战争’作为案例研究，我们的方法准确预测了关键影响者的观点，并在各个领域中将情感预测与现实世界的情绪趋势对齐。</p>
<h4 id="_26">一句话总结：</h4>
<p>本研究提出了一个新颖的计算框架，通过自动5W1H问题生成和增强大型语言模型，准确预测社交媒体上意见领袖的观点和公众的情感反应。</p>
<hr />
<h2 id="qaea-dr-a-unified-text-augmentation-framework-for-dense-retrieval"><a href="http://arxiv.org/abs/2407.20207v1">QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval</a></h2>
<p>发布时间：2024-07-29</p>
<p>作者：Hongming Tan, Shaoxiong Zhan, Hai Lin, Hai-Tao Zheng, Wai Kin, Chan</p>
<h4 id="_27">中文摘要：</h4>
<p>在密集检索中，将长文本嵌入密集向量可能会导致信息损失，从而导致查询-文本匹配不准确。此外，含有过多噪声或稀疏关键信息的低质量文本与相关查询不太可能很好地对齐。最近的研究主要集中于改进句子嵌入模型或检索过程。在本工作中，我们提出了一种新颖的文本增强框架用于密集检索。该框架将原始文档转换为信息密集的文本格式，补充原始文本以有效解决上述问题，而无需修改嵌入或检索方法。通过大型语言模型（LLMs）的无提示提示，生成了两种文本表示：问答对和元素驱动的活动。我们将这种方法称为QAEA-DR：在密集检索的文本增强框架中统一问答生成和事件提取。为了进一步提高生成文本的质量，我们在LLM提示中引入了基于评分的评估和再生机制。我们的QAEA-DR模型对密集检索有积极影响，这得到了理论分析和实证实验的支持。</p>
<h4 id="_28">一句话总结：</h4>
<p>该研究提出了一种名为QAEA-DR的文本增强框架，通过问答生成和事件提取来提高密集检索的准确性。</p>
<hr />
<h2 id="improving-retrieval-augmented-language-model-with-self-reasoning"><a href="http://arxiv.org/abs/2407.19813v2">Improving Retrieval Augmented Language Model with Self-Reasoning</a></h2>
<p>发布时间：2024-07-29</p>
<p>作者：Yuan Xia, Jingbo Zhou, Zhenhui Shi, Jun Chen, Haifeng Huang</p>
<h4 id="_29">中文摘要：</h4>
<p>检索增强语言模型（RALM）通过在推理过程中整合外部知识，在知识密集型任务上表现出显著性能，这有助于减轻大型语言模型（LLMs）中继承的事实幻觉。尽管取得了这些进展，但RALMs的实施仍存在挑战，尤其是在可靠性和可追溯性方面。具体来说，无关文档的检索可能导致不实用的响应生成，甚至降低LLMs的性能，而生成输出中缺乏适当的引用则使验证模型的可信度变得复杂。为此，我们提出了一种新颖的自我推理框架，旨在提高RALMs的可靠性和可追溯性，其核心思想是利用LLM自身生成的推理轨迹。该框架涉及通过三个过程构建自我推理轨迹：一个相关性感知过程、一个证据感知选择性过程和一个轨迹分析过程。我们在四个公开数据集（两个短问答数据集、一个长问答数据集和一个事实验证数据集）上评估了我们的框架，以证明我们方法的优势，该方法可以优于现有的最先进模型，并且在与GPT-4相当的性能下，仅使用2,000个训练样本。</p>
<h4 id="_30">一句话总结：</h4>
<p>该研究提出了一种基于自我推理的框架，旨在提高检索增强语言模型的可靠性和可追溯性，通过利用LLM自身生成的推理轨迹，实现了在少量训练样本下与GPT-4相当的性能。</p>
<hr />
<h2 id="introducing-a-new-hyper-parameter-for-rag-context-window-utilization"><a href="http://arxiv.org/abs/2407.19794v1">Introducing a new hyper-parameter for RAG: Context Window Utilization</a></h2>
<p>发布时间：2024-07-29</p>
<p>作者：Kush Juvekar, Anupam Purwar</p>
<h4 id="_31">中文摘要：</h4>
<p>本文介绍了一种名为“上下文窗口利用率”的新超参数，用于检索增强生成（RAG）系统。RAG系统通过整合从外部知识库检索的相关信息来增强生成模型，从而提高生成响应的事实准确性和上下文相关性。检索和处理文本块的大小是影响RAG性能的关键因素。本研究旨在确定最大化答案生成质量的最佳块大小。通过系统实验，我们分析了不同块大小对RAG框架的效率和效果的影响。我们的发现表明，最佳块大小在提供足够上下文和最小化无关信息之间取得了平衡。这些见解对于提高RAG系统的设计和实现至关重要，强调了选择合适的块大小以实现卓越性能的重要性。</p>
<h4 id="_32">一句话总结：</h4>
<p>本文提出了一种新的超参数“上下文窗口利用率”，以优化检索增强生成系统中的文本块大小，从而提升生成响应的质量和准确性。</p>
<hr />
<h2 id="a-study-on-the-implementation-method-of-an-agent-based-advanced-rag-system-using-graph"><a href="http://arxiv.org/abs/2407.19994v1">A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph</a></h2>
<p>发布时间：2024-07-29</p>
<p>作者：Cheonsu Jeong</p>
<h4 id="_33">中文摘要：</h4>
<p>本研究旨在通过克服现有检索增强生成（RAG）模型的局限性，并基于图技术实现一个高级RAG系统，以提高知识库问答（QA）系统的性能，从而开发高质量的生成式人工智能服务。虽然现有的RAG模型通过利用检索到的信息表现出高准确性和流畅性，但它们在生成响应时可能因为使用预先加载的知识而未进行重新处理，从而导致准确度下降。此外，它们在RAG配置阶段之后无法整合实时数据，这导致了对上下文理解的问题和偏颇信息的出现。为了解决这些局限性，本研究实施了一个基于图技术的增强型RAG系统。该系统旨在高效地搜索和利用信息。具体来说，它使用LangGraph来评估检索信息的可靠性，并综合多种数据以生成更准确和完善的响应。此外，研究通过实现代码和验证结果详细解释了系统的操作、关键实施步骤和示例，从而增强了人们对高级RAG技术的理解。这种方法为在商业服务中实施高级RAG系统提供了实用指南，使其成为实际应用的宝贵资源。</p>
<h4 id="_34">一句话总结：</h4>
<p>本研究通过引入基于图技术的RAG系统，提高了知识库问答系统的准确性和实时性，为生成式人工智能服务的开发提供了实用指南。</p>
<hr />
<h2 id="enhancing-code-translation-in-language-models-with-few-shot-learning-via-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.19619v1">Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-07-29</p>
<p>作者：Manish Bhattarai, Javier E. Santos, Shawn Jones, Ayan Biswas, Boian Alexandrov, Daniel O'Malley</p>
<h4 id="_35">中文摘要：</h4>
<p>大型语言模型（LLMs）的出现显著推进了代码翻译领域，使得编程语言之间的自动翻译成为可能。然而，这些模型由于对上下文理解不足，在处理复杂翻译任务时往往遇到困难。本文提出了一种新颖的方法，通过结合基于检索的技术来增强代码翻译的少样本学习。通过利用现有的代码翻译库，我们动态检索最相关的示例来指导模型翻译新的代码段。我们的方法基于检索增强生成（RAG），通过提供模型可以实时学习的上下文示例，显著提高了翻译质量。我们选择RAG而不是传统的微调方法，是因为它能够利用现有的代码库或本地存储的代码语料库，这使得模型能够动态适应各种翻译任务，而无需进行大量的重新训练。在Starcoder、Llama3-70B Instruct、CodeLlama-34B Instruct、Granite-34B Code Instruct和Mixtral-8x22B等开源LLM模型以及GPT-3.5 Turbo和GPT-4o等商业LLM模型上进行的广泛实验表明，我们的方法在Fortran和CPP之间的翻译方面优于传统的零样本方法。我们还探索了不同的射击次数，即在推理过程中提供的示例数量，特别是1、2和3次射击，以及不同的RAG嵌入模型，包括Nomic-Embed、Starencoder和CodeBERT，以评估我们方法的可靠性和有效性。</p>
<h4 id="_36">一句话总结：</h4>
<p>本文提出了一种基于检索增强生成的代码翻译方法，通过利用现有代码库中的相关示例，显著提高了代码翻译的准确性和适应性。</p>
<hr />
<h2 id="faculty-perspectives-on-the-potential-of-rag-in-computer-science-higher-education"><a href="http://arxiv.org/abs/2408.01462v1">Faculty Perspectives on the Potential of RAG in Computer Science Higher Education</a></h2>
<p>发布时间：2024-07-28</p>
<p>作者：Sagnik Dakshit</p>
<h4 id="_37">中文摘要：</h4>
<p>大型语言模型（LLMs）的出现对自然语言处理领域产生了重大影响，并因其在各种应用和公共访问中的广泛应用而改变了各个领域的对话任务。关于LLMs在教育中应用的讨论引发了伦理担忧，尤其是关于剽窃和政策合规性。尽管LLMs在对话任务中表现出色，但可靠性和幻觉的限制加剧了对对话进行防护的需求，这促使我们对计算机科学高等教育中的RAG（检索增强生成）进行探究。我们开发了针对虚拟教学助手和教学辅助工具两项任务的检索增强生成（RAG）应用。在我们的研究中，我们收集了不同级别计算机科学本科和研究生课程中教师对个性化RAG系统的评价和意见。这项研究是首次收集教师对基于LLMs的RAG在教育中应用的反馈。调查发现，尽管教师承认RAG系统作为虚拟教学助手和教学辅助工具的潜力，但仍建议某些障碍和特性以实现其全面部署。这些发现有助于持续讨论高级语言模型在教育环境中的整合，强调了仔细考虑伦理影响和开发适当的安全措施以确保负责任和有效实施的必要性。</p>
<h4 id="_38">一句话总结：</h4>
<p>本研究通过调查教师对基于LLMs的RAG在教育中的应用反馈，探讨了RAG在教育中的潜力、挑战和伦理考量。</p>
<hr />
<h2 id="rlcoder-reinforcement-learning-for-repository-level-code-completion"><a href="http://arxiv.org/abs/2407.19487v1">RLCoder: Reinforcement Learning for Repository-Level Code Completion</a></h2>
<p>发布时间：2024-07-28</p>
<p>作者：Yanlin Wang, Yanli Wang, Daya Guo, Jiachi Chen, Ruikai Zhang, Yuchi Ma, Zibin Zheng</p>
<h4 id="_39">中文摘要：</h4>
<p>仓库级代码补全旨在在指定仓库的上下文中生成未完成代码片段的代码。现有的方法主要依赖于由于输入序列长度限制而采用的检索增强生成策略。然而，传统的基于词法的检索方法如BM25难以捕捉代码语义，而基于模型的方法由于缺乏用于训练的标记数据而面临挑战。因此，我们提出了RLCoder，一个新颖的强化学习框架，它可以使检索器在没有标记数据的情况下学习检索用于代码补全的有用内容。具体来说，我们根据提供检索内容作为额外上下文时目标代码的困惑度，迭代评估检索内容的效用，并提供反馈以更新检索器参数。这个迭代过程使检索器能够从其成功和失败中学习，逐渐提高其检索相关和高质量内容的能力。考虑到并非所有情况都需要超出代码文件的信息，并且并非所有检索到的上下文对生成都有帮助，我们还引入了一个停止信号机制，允许检索器自主决定何时检索以及保留哪些候选内容。大量的实验结果表明，RLCoder在CrossCodeEval和RepoEval上始终优于最先进的方法，比先前的方法实现了12.2%的EM改进。此外，实验表明，我们的框架可以跨不同编程语言泛化，并进一步改进先前的如RepoCoder等方法。我们在https://github.com/DeepSoftwareAnalytics/RLCoder上提供了代码和数据。</p>
<h4 id="_40">一句话总结：</h4>
<p>RLCoder通过强化学习实现了无需标记数据的代码补全，显著提升了代码补全的准确性和泛化能力。</p>
<hr />
<h2 id="learning-robust-named-entity-recognizers-from-noisy-data-with-retrieval-augmentation"><a href="http://arxiv.org/abs/2407.18562v1">Learning Robust Named Entity Recognizers From Noisy Data With Retrieval Augmentation</a></h2>
<p>发布时间：2024-07-26</p>
<p>作者：Chaoyi Ai, Yong Jiang, Shen Huang, Pengjun Xie, Kewei Tu</p>
<h4 id="_41">中文摘要：</h4>
<p>命名实体识别（NER）模型通常难以处理噪声输入，例如拼写错误或由光学字符识别（OCR）过程产生的错误，因此学习一个鲁棒的NER模型具有挑战性。现有的鲁棒NER模型利用噪声文本及其对应的金文本进行训练，但在许多实际应用中，由于没有金文本，这种方法是不可行的。在本文中，我们考虑了一个更现实的场景，其中只有噪声文本及其NER标签可用。我们提出从知识语料库中检索噪声文本的相关文本，并使用它来增强原始噪声输入的表示。我们设计了三种检索方法：基于词典相似性的稀疏检索、基于语义相似性的密集检索和基于特定任务文本的自检索。在检索相关文本后，我们将检索到的文本与原始噪声文本连接，并使用transformer网络对它们进行编码，利用自注意力机制通过检索到的文本增强噪声文本的上下文标记表示。我们进一步采用了一种多视角训练框架，在不检索文本的情况下提高了鲁棒的NER性能。实验表明，我们的检索增强模型在各种噪声NER设置中实现了显著的改进。</p>
<h4 id="_42">一句话总结：</h4>
<p>本文提出了一种基于检索增强的命名实体识别模型，通过从知识语料库中检索相关文本来提高噪声文本的鲁棒性。</p>
<hr />
<h2 id="modular-rag-transforming-rag-systems-into-lego-like-reconfigurable-frameworks"><a href="http://arxiv.org/abs/2407.21059v1">Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks</a></h2>
<p>发布时间：2024-07-26</p>
<p>作者：Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang</p>
<h4 id="_43">中文摘要：</h4>
<p>检索增强生成（RAG）显著提升了大型语言模型（LLMs）在处理知识密集型任务方面的能力。随着应用场景需求的增加，RAG技术不断进化，导致高级检索器、LLMs和其他互补技术的集成，从而增加了RAG系统的复杂性。然而，快速的发展速度超过了基础RAG范式，许多方法难以统一在“检索-生成”的过程中。在此背景下，本文探讨了现有RAG范式的局限性，并引入了模块化RAG框架。通过将复杂的RAG系统分解为独立的模块和专业操作符，它促进了一个高度可重构的框架。模块化RAG超越了传统的线性架构，采用了一种更先进的设计，集成了路由、调度和融合机制。借鉴广泛的研究，本文进一步确定了常见的RAG模式——线性、条件、分支和循环——并对它们各自的实现细节进行了全面分析。模块化RAG为RAG系统的概念化和部署提供了创新的机会。最后，本文探讨了新操作符和范式的潜在出现，为RAG技术的持续演化和实际部署奠定了坚实的理论基础和实践路线图。</p>
<h4 id="_44">一句话总结：</h4>
<p>本文提出了模块化RAG框架，以解决现有RAG范式的局限性，并推动RAG技术的持续发展和实际应用。</p>
<hr />
<h2 id="esac-eq-sans-assisting-chatbot-application-of-large-language-models-and-retrieval-augmented-generation-for-enhanced-user-experience-at-eq-sans"><a href="http://arxiv.org/abs/2407.19075v1">ESAC (EQ-SANS Assisting Chatbot): Application of Large Language Models and Retrieval-Augmented Generation for Enhanced User Experience at EQ-SANS</a></h2>
<p>发布时间：2024-07-26</p>
<p>作者：Changwoo Do, Gergely Nagy, William T. Heller</p>
<h4 id="_45">中文摘要：</h4>
<p>在过去几十年中，中子散射实验在探索材料性质方面发挥了至关重要的作用。尽管用户界面随着时间的推移得到了改进，但由于这种先进仪器的复杂性以及每个人每年可能进行的实验数量有限，中子散射实验仍然需要专家的特定知识或培训。本文介绍了一种创新的聊天机器人应用程序，该程序利用大型语言模型（LLM）和检索增强生成（RAG）技术，显著提升了橡树岭国家实验室快中子源中的小角中子散射仪器EQ-SANS的用户体验。通过以用户为中心的设计方法，EQ-SANS辅助聊天机器人（ESAC）充当用户的交互式参考资料，从而帮助访问科学家更方便地使用该仪器。通过弥合EQ-SANS用户与进行实验所需控制系统之间的差距，ESAC为使用大型科学设施的科学界设定了新的互动学习和支持标准。</p>
<h4 id="_46">一句话总结：</h4>
<p>本文提出了一种基于LLM和RAG技术的聊天机器人，旨在提升EQ-SANS用户的使用体验，并促进科学界在大型科学设施中的互动学习和支持。</p>
<hr />
<h2 id="chipexpert-the-open-source-integrated-circuit-design-specific-large-language-model"><a href="http://arxiv.org/abs/2408.00804v1">ChipExpert: The Open-Source Integrated-Circuit-Design-Specific Large Language Model</a></h2>
<p>发布时间：2024-07-26</p>
<p>作者：Ning Xu, Zhaoyang Zhang, Lei Qi, Wensuo Wang, Chao Zhang, Zihao Ren, Huaiyuan Zhang, Xin Cheng, Yanqi Zhang, Zhichao Liu, Qingwen Wei, Shiyang Wu, Lanlan Yang, Qianfeng Lu, Yiqun Ma, Mengyao Zhao, Junbo Liu, Yufan Song, Xin Geng, Jun Yang</p>
<h4 id="_47">中文摘要：</h4>
<p>集成电路（IC）设计领域高度专业化，对进入和研究开发提出了重大障碍。尽管大型语言模型（LLMs）在各个领域取得了显著的成功，但现有的LLMs往往无法满足学生、工程师和研究者的特定需求。因此，LLMs在IC设计领域的潜力在很大程度上仍未得到探索。为了解决这些问题，我们引入了ChipExpert，这是第一个专门针对IC设计领域的开源、教学型LLMs。ChipExpert基于当前最好的开源基础模型（Llama-3 8B）进行训练。整个训练过程包括多个关键阶段，包括数据准备、继续预训练、指令引导的监督微调、偏好对齐和评估。在数据准备阶段，我们通过手动选择和数据合成技术构建了多个高质量的定制数据集。在随后的两个阶段，ChipExpert获得了大量的IC设计知识，并学会了如何专业地回答用户查询。ChipExpert还经历了一个对齐阶段，使用直接偏好优化，以达到高标准的道德表现。最后，为了减轻ChipExpert的幻觉，我们开发了一个基于IC设计知识库的检索增强生成（RAG）系统。我们还发布了第一个IC设计基准ChipICD-Bench，以评估LLMs在多个IC设计子领域的功能。通过对该基准进行的全面实验，ChipExpert在IC设计知识问答任务中展示了高水平的专业知识。</p>
<h4 id="_48">一句话总结：</h4>
<p>ChipExpert是首个针对IC设计领域的开源教学型LLMs，通过多阶段训练和知识库增强，显著提升了IC设计领域的问答能力。</p>
<hr />
<h2 id="reaper-reasoning-based-retrieval-planning-for-complex-rag-systems"><a href="http://arxiv.org/abs/2407.18553v2">REAPER: Reasoning based Retrieval Planning for Complex RAG Systems</a></h2>
<p>发布时间：2024-07-26</p>
<p>作者：Ashutosh Joshi, Sheikh Muhammad Sarwar, Samarth Varshney, Sreyashi Nag, Shrivats Agrawal, Juhi Naik</p>
<h4 id="_49">中文摘要：</h4>
<p>复杂对话系统通常使用检索到的证据来促进事实性回答。这类基于检索增强生成（RAG）的系统从通常由多个索引或API架构的庞大异构数据存储中检索信息，而不是单一的大型源。对于给定的查询，需要从可能的检索源之一或一小部分源中检索相关证据。复杂查询甚至可能需要多步检索。例如，在零售网站上回答客户关于过往订单的问题的对话代理，首先需要检索适当的客户订单，然后检索与客户问题相关的、在订购产品上下文中的证据。大多数RAG代理通过交错推理和检索步骤来处理此类思维链（CoT）任务。然而，每个推理步骤都会直接增加系统的延迟。对于大型模型来说，这种延迟成本是显著的——达到数秒级别。多代理系统可能会将查询分类到与检索源关联的单个代理，尽管这意味着一个（小型）分类模型决定了大型语言模型的表现。在这项工作中，我们提出了REAPER（基于推理的计划器）——一个基于大型语言模型（LLM）的计划器，用于在对话系统中生成检索计划。我们展示了与基于代理的系统相比，在延迟方面有显著的改进，并且与基于分类的计划相比，能够轻松扩展到新的和未见过的用例。尽管我们的方法可以应用于任何RAG系统，但我们展示了在对话购物助手上下文中的结果。</p>
<h4 id="_50">一句话总结：</h4>
<p>本研究提出了一种基于推理的计划器REAPER，用于对话系统中的检索计划生成，显著降低了延迟并能够适应新的应用场景。</p>
<hr />
<h2 id="the-geometry-of-queries-query-based-innovations-in-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.18044v1">The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-07-25</p>
<p>作者：Eric Yang, Jonathan Amar, Jong Ha Lee, Bhawesh Kumar, Yugang Jia</p>
<h4 id="_51">中文摘要：</h4>
<p>基于大型语言模型（LLMs）的数字健康聊天机器人有潜力通过提供便捷和按需的健康指导和问答，显著改善慢性病的个人健康管理。然而，这些聊天机器人存在提供未经验证和不准确信息的风险，因为LLMs的响应是基于从各种互联网数据中学习到的模式生成的。检索增强生成（RAG）可以通过基于可靠内容来帮助减轻LLM响应中的幻觉和不准确性。然而，高效且准确地检索与实时用户问题最相关的内容集仍然是一个挑战。在这项工作中，我们引入了基于查询的检索增强生成（QB-RAG），这是一种新颖的方法，它使用LLMs从内容库中预先计算潜在查询的数据库。对于 incoming patient question（待翻译），QB-RAG通过向量搜索有效地将其与预先生成的查询数据库进行匹配，从而提高了用户问题与内容之间的对齐。我们为QB-RAG建立了理论基础，并提供了对现有RAG系统检索增强技术的比较分析。最后，我们的实证评估表明，QB-RAG显著提高了医疗问答的准确性，为数字健康中稳健且值得信赖的LLM应用铺平了道路。</p>
<h4 id="_52">一句话总结：</h4>
<p>基于查询的检索增强生成（QB-RAG）通过预先计算查询数据库，显著提高了数字健康聊天机器人在慢性病健康管理中的问答准确性。</p>
<hr />
<h2 id="bailicai-a-domain-optimized-retrieval-augmented-generation-framework-for-medical-applications"><a href="http://arxiv.org/abs/2407.21055v1">Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications</a></h2>
<p>发布时间：2024-07-24</p>
<p>作者：Cui Long, Yongbin Liu, Chunping Ouyang, Ying Yu</p>
<h4 id="_53">中文摘要：</h4>
<p>大型语言模型（LLMs）在自然语言理解方面表现出色，促使人们对其在各个领域的潜在应用进行了广泛探索。在医疗领域，经过特定领域微调的开源LLMs显示出中等的有效性；然而，它们在性能上仍然显著低于GPT-4和GPT-3.5等专有模型。这些开源模型在特定领域知识的全面性方面存在局限性，并且在文本生成过程中容易出现“幻觉”。为了缓解这些问题，研究人员实施了检索增强生成（RAG）方法，该方法通过保留模型内部参数的同时，将外部知识库中的背景信息增强到LLMs中。然而，文档噪声会负面影响性能，并且RAG在医疗领域的应用仍处于起步阶段。本研究提出了Bailicai框架：一种将检索增强生成与针对医疗领域优化的LLMs相结合的创新框架。Bailicai框架通过实施四个子模块来增强LLMs在医学领域的性能。实验结果表明，Bailicai方法在多个医疗基准测试中超越了现有的医疗领域LLMs，并且其性能超过了GPT-3.5。此外，Bailicai方法有效地减轻了LLMs在医疗应用中普遍存在的“幻觉”问题，并改善了处理无关或伪相关文档时与传统RAG技术相关的噪声相关挑战。</p>
<h4 id="_54">一句话总结：</h4>
<p>Bailicai框架通过检索增强生成技术显著提升了大型语言模型在医疗领域的性能，有效解决了幻觉和噪声问题。</p>
<hr />
<h2 id="retrieval-augmented-generation-or-long-context-llms-a-comprehensive-study-and-hybrid-approach"><a href="http://arxiv.org/abs/2407.16833v1">Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach</a></h2>
<p>发布时间：2024-07-23</p>
<p>作者：Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky</p>
<h4 id="_55">中文摘要：</h4>
<p>检索增强生成（RAG）一直是大型语言模型（LLMs）处理过长上下文的有力工具。然而，最近的LLMs如Gemini-1.5和GPT-4在直接理解长上下文方面表现出卓越的能力。我们进行了RAG和长上下文（LC）LLMs的全面比较，旨在利用两者的优势。我们使用三种最新的LLMs在多个公共数据集上对RAG和LC进行了基准测试。结果显示，当资源充足时，LC在平均性能方面始终优于RAG。然而，RAG显著降低的成本仍然是一个明显的优势。基于这一观察，我们提出了Self-Route，这是一种简单而有效的方法，根据模型自我反思将查询路由到RAG或LC。Self-Route显著降低了计算成本，同时保持了与LC相当的性能。我们的发现为使用RAG和LC的LLMs在长上下文应用中提供了指导。</p>
<h4 id="_56">一句话总结：</h4>
<p>本研究提出了一种名为Self-Route的方法，通过模型自我反思智能路由查询，以在降低计算成本的同时，保持与长上下文LLMs相当的性能。</p>
<hr />
<h2 id="retrieve-generate-evaluate-a-case-study-for-medical-paraphrases-generation-with-small-language-models"><a href="http://arxiv.org/abs/2407.16565v1">Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models</a></h2>
<p>发布时间：2024-07-23</p>
<p>作者：Ioana Buhnila, Aman Sinha, Mathieu Constant</p>
<h4 id="_57">中文摘要：</h4>
<p>近年来，大型语言模型（LLMs）对普通大众的可用性激增可能导致这些模型在医疗相关建议中的不可追踪使用。通过LLMs模型进行语言生成存在两个主要问题：首先，它们容易产生幻觉，因此，对于任何医疗目的，它们都需要科学和事实依据；其次，由于模型规模巨大，LLMs对计算资源提出了巨大的挑战。在这项工作中，我们引入了pRAGe，这是一个用于使用小型语言模型（SLM）检索增强生成和评估医疗释义生成的管道。我们研究了SLMs在法语医疗释义生成中的有效性和外部知识库的影响。</p>
<h4 id="_58">一句话总结：</h4>
<p>本研究提出了一种名为pRAGe的管道，用于利用小型语言模型生成和评估医疗释义，以应对大型语言模型在医疗建议中的潜在问题。</p>
<hr />
<h2 id="aptness-incorporating-appraisal-theory-and-emotion-support-strategies-for-empathetic-response-generation"><a href="http://arxiv.org/abs/2407.21048v1">APTNESS: Incorporating Appraisal Theory and Emotion Support Strategies for Empathetic Response Generation</a></h2>
<p>发布时间：2024-07-23</p>
<p>作者：Yuxuan Hu, Minghuan Tan, Chenwei Zhang, Zixuan Li, Xiaodan Liang, Min Yang, Chengming Li, Xiping Hu</p>
<h4 id="_59">中文摘要：</h4>
<p>共情反应生成旨在理解他人的情绪并选择最合适的策略来帮助他们解决情绪挑战。共情可以分为认知共情和情感共情。前者涉及理解和辨别他人情绪问题和情境的能力，而后者则包括提供安慰的能力。为了提高一个人的共情能力，发展这两个方面都是至关重要的。因此，我们开发了一个创新框架，该框架结合了检索增强和情感支持策略整合。我们的框架从引入一个全面的共情情感调色板开始。然后，我们应用评估理论来分解这个调色板并创建一个共情反应数据库。这个数据库作为外部资源，通过整合语义检索机制来增强LLM的共情能力。此外，我们的框架非常重视响应策略的正确表述。通过融入情感支持策略，我们旨在丰富模型在认知和情感共情方面的能力，从而实现更细腻和全面的共情反应。最后，我们从基于对话长度的共情对话数据集\textsc{EmpatheticDialogues}和ExTES中提取了数据集ED和ET。实验表明，我们的框架可以从认知和情感共情的视角增强LLM的共情能力。我们的代码发布在https://github.com/CAS-SIAT-XinHai/APTNESS。</p>
<h4 id="_60">一句话总结：</h4>
<p>该研究提出了一种结合检索增强和情感支持策略的共情反应生成框架，有效提升了大型语言模型在认知和情感共情方面的能力。</p>
<hr />
<h2 id="radiorag-factual-large-language-models-for-enhanced-diagnostics-in-radiology-using-dynamic-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.15621v1">RadioRAG: Factual Large Language Models for Enhanced Diagnostics in Radiology Using Dynamic Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-07-22</p>
<p>作者：Soroosh Tayebi Arasteh, Mahshad Lotfinia, Keno Bressem, Robert Siepmann, Dyke Ferber, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn</p>
<h4 id="_61">中文摘要：</h4>
<p>大型语言模型（LLMs）在医学人工智能（AI）领域取得了显著进展。然而，LLMs往往基于静态训练数据集生成过时或不准确的信息。检索增强生成（RAG）通过整合外部数据源来缓解这一问题。尽管之前的RAG系统使用了预先组装的、固定的数据库，具有有限的灵活性，但我们开发了放射学RAG（RadioRAG）作为一个端到端框架，能够实时从权威的放射学在线来源检索数据。RadioRAG使用专门的放射学问答数据集（RadioQA）进行评估。我们评估了各种LLMs在有无通过RAG访问额外在线信息的情况下回答放射学特定问题的诊断准确性。使用来自RSNA案例收集的80个问题（涵盖放射学亚专业）和24个额外的专家评审问题（其中提供了正确的金标准答案），LLMs（GPT-3.5-turbo、GPT-4、Mistral-7B、Mixtral-8x7B和Llama3 [8B和70B]）在有和没有RadioRAG的情况下被提示。RadioRAG实时从www.radiopaedia.org检索上下文特定的信息并将其纳入其回复中。RadioRAG在所有LLMs上均一致提高了诊断准确性，相对改进范围从2%到54%。它在放射学亚专业中与或超过了没有RAG的问答，特别是在乳腺成像和急诊放射学方面。然而，改进的程度在不同模型之间有所不同；GPT-3.5-turbo和Mixtral-8x7B-instruct-v0.1取得了显著进步，而Mistral-7B-instruct-v0.2没有显示出任何改进，突出了其有效性的变化。当LLMs能够访问其训练数据之外的特定领域数据时，它们会受益。对于放射学，RadioRAG建立了一个稳健的框架，显著提高了放射学问答中的诊断准确性和真实性。</p>
<h4 id="_62">一句话总结：</h4>
<p>RadioRAG通过实时整合权威在线放射学数据，显著提高了大型语言模型在放射学问答中的诊断准确性。</p>
<hr />
<h2 id="an-empirical-study-of-retrieval-augmented-generation-with-chain-of-thought"><a href="http://arxiv.org/abs/2407.15569v1">An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought</a></h2>
<p>发布时间：2024-07-22</p>
<p>作者：Yuetong Zhao, Hongyu Cao, Xianyu Zhao, Zhijian Ou</p>
<h4 id="_63">中文摘要：</h4>
<p>自2022年底ChatGPT发布以来，以ChatGPT为代表的生成对话模型迅速成为日常生活中的必备工具。随着用户期望的提高，增强生成对话模型解决复杂问题的能力已成为当前研究的一个焦点。本文深入探讨了RAFT（检索增强微调）方法在提高生成对话模型性能方面的有效性。RAFT结合了思维链（Chain-of-Thought）与模型监督微调（SFT）以及检索增强生成（RAG），这显著增强了模型的信息提取和逻辑推理能力。我们在多个数据集上评估了RAFT方法，并分析了其在各种推理任务中的性能，包括长文本问答（QA）和短文本问答任务，以及中英文的问答任务和支持性及比较推理任务。值得注意的是，它填补了先前研究中关于长文本问答任务和中文数据集的空白。此外，我们还评估了RAFT方法中思维链（CoT）的好处。这项工作为专注于提高生成对话模型性能的研究提供了宝贵的见解。</p>
<h4 id="_64">一句话总结：</h4>
<p>本文通过引入RAFT方法，显著提升了生成对话模型的信息提取和逻辑推理能力，为解决复杂问题提供了有效途径。</p>
<hr />
<h2 id="customized-retrieval-augmented-generation-and-benchmarking-for-eda-tool-documentation-qa"><a href="http://arxiv.org/abs/2407.15353v2">Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA</a></h2>
<p>发布时间：2024-07-22</p>
<p>作者：Yuan Pu, Zhuolun He, Tairu Qiu, Haoyuan Wu, Bei Yu</p>
<h4 id="_65">中文摘要：</h4>
<p>检索增强生成（RAG）通过从外部数据库中获取事实信息来提高生成式人工智能模型的准确性和可靠性，这在基于文档的问答（QA）任务中得到了广泛应用。现成的RAG流程在通用文档上进行了良好的预训练，但在应用于知识密集型垂直领域，如电子设计自动化（EDA）时，会遇到重大挑战。本文针对这一问题，提出了一种定制的RAG框架，以及三个针对EDA工具文档QA的特定领域技术，包括用于文本嵌入模型微调的对比学习方案、从专有大型语言模型（LLM）中提取的再排序器，以及使用高质量领域语料库微调的生成LLM。此外，我们还为OpenROAD（一个先进的RTL到GDSII设计平台）开发并发布了一个文档QA评估基准，ORD-QA。实验结果表明，与现有技术相比，我们提出的RAG流程和技术在ORD-QA以及一个商业工具上均实现了优异的性能。ORD-QA基准和定制RAG流程的训练数据集在https://github.com/lesliepy99/RAG-EDA上开源。</p>
<h4 id="_66">一句话总结：</h4>
<p>本文提出了一种针对EDA工具文档问答的定制RAG框架和特定技术，显著提升了问答系统的性能。</p>
<hr />
<h2 id="nv-retriever-improving-text-embedding-models-with-effective-hard-negative-mining"><a href="http://arxiv.org/abs/2407.15831v1">NV-Retriever: Improving text embedding models with effective hard-negative mining</a></h2>
<p>发布时间：2024-07-22</p>
<p>作者：Gabriel de Souza P. Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, Even Oldridge</p>
<h4 id="_67">中文摘要：</h4>
<p>文本嵌入模型在信息检索应用中，如语义搜索和基于检索增强生成（RAG）的问答系统中非常流行。这些模型通常是经过对比学习目标微调的Transformer模型。许多论文介绍了新的嵌入模型架构和训练方法，然而，其中一个关键成分——挖掘负样本的过程，仍然没有得到充分探索或描述。微调嵌入模型的一个挑战性方面是选择高质量难负样本进行对比学习。在本文中，我们提出了一组正样本感知的挖掘方法，这些方法利用正样本的相关性分数以更有效地去除假负样本。我们还对硬负样本挖掘方法及其配置进行了全面的消融研究，探索了不同的教师模型和基础模型。我们通过引入NV-Retriever-v1模型来展示我们提出的方法的有效性，该模型在MTEB检索（BEIR）基准测试中得分为60.9，比先前的方法高出0.65分。该模型在2024年7月7日发布到MTEB检索时排名第一。</p>
<h4 id="_68">一句话总结：</h4>
<p>本文提出了一种基于正样本感知的挖掘方法，有效提升了文本嵌入模型的检索性能，并在MTEB Retrieval基准测试中取得了优异成绩。</p>
<hr />
<h2 id="morse-bridging-the-gap-in-cybersecurity-expertise-with-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.15748v1">MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-07-22</p>
<p>作者：Marco Simoni, Andrea Saracino, Vinod P., Mauro Conti</p>
<h4 id="_69">中文摘要：</h4>
<p>本文介绍了MoRSE（混合RAG安全专家），这是第一个专门针对网络安全的人工智能聊天机器人。MoRSE旨在提供关于网络安全的全面和完整的知识。MoRSE使用了两个RAG（检索增强生成）系统，旨在从多维网络安全环境中检索和组织信息。与传统的RAG不同，MoRSE使用并行检索器，这些检索器协同工作以检索不同格式和结构中语义相关的信息。与依赖于参数化知识库的传统大型语言模型（LLMs）不同，MoRSE在响应用户查询时从非参数化知识库中检索相关文档。随后，MoRSE使用这些信息生成准确的答案。此外，MoRSE受益于其知识库的实时更新，使其能够在不重新训练的情况下持续丰富知识。我们对MoRSE与其他最先进的LLMs的有效性进行了评估，在600个网络安全特定问题上对系统进行了评估。实验评估表明，与GPT-4和Mixtral 7x8等已知解决方案相比，在答案的相关性和正确性方面的改进超过10%。</p>
<h4 id="_70">一句话总结：</h4>
<p>MoRSE是一种新型的网络安全AI聊天机器人，通过并行检索和多维度知识库，显著提升了网络安全问答的准确性和相关性。</p>
<hr />
<h2 id="taskgen-a-task-based-memory-infused-agentic-framework-using-strictjson"><a href="http://arxiv.org/abs/2407.15734v1">TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON</a></h2>
<p>发布时间：2024-07-22</p>
<p>作者：John Chong Min Tan, Prince Saroj, Bharat Runwal, Hardik Maheshwari, Brian Lim Yi Sheng, Richard Cottrill, Alankrit Chona, Ambuj Kumar, Mehul Motani</p>
<h4 id="_71">中文摘要：</h4>
<p>TaskGen是一个开源的智能体框架，它通过将任务分解为子任务，并使用智能体或配备的功能来执行每个子任务，来解决任意任务。为了减少冗余（从而减少令牌使用），TaskGen使用了严格的JSON格式，确保从大型语言模型（LLM）输出的JSON，并附带类型检查和迭代错误纠正等额外功能。TaskGen的核心理念是基于需要知道的信息/记忆管理。我们在各种环境中对TaskGen进行了实证评估，包括40x40动态迷宫导航（障碍物位置变化，解决率为100%）、TextWorld逃脱房间解决（具有密集奖励和详细目标，解决率为96%）、网页浏览（69%的动作成功）、解决MATH数据集（在100个Level-5问题上的解决率为71%）、在NaturalQuestions数据集上的检索增强生成（F1分数为47.03%）。</p>
<h4 id="_72">一句话总结：</h4>
<p>TaskGen是一个高效的智能体框架，通过分解任务并使用严格JSON格式，在多种环境中实现了高解决率。</p>
<hr />
<h2 id="decoding-bacnet-packets-a-large-language-model-approach-for-packet-interpretation"><a href="http://arxiv.org/abs/2407.15428v1">Decoding BACnet Packets: A Large Language Model Approach for Packet Interpretation</a></h2>
<p>发布时间：2024-07-22</p>
<p>作者：Rashi Sharma, Hiroyuki Okada, Tatsumi Oba, Karthikk Subramanian, Naoto Yanai, Sugiri Pranata</p>
<h4 id="_73">中文摘要：</h4>
<p>工业控制系统（ICS）环境包含了一系列复杂的通信协议，这对负责监控、解释和应对网络安全事件的安全运营中心（SOC）分析师来说构成了重大挑战。传统的监控工具和技术往往难以清晰地理解ICS特定通信的性质和意图。为了提高理解能力，我们提出了一种基于大型语言模型（LLM）的软件解决方案。该解决方案目前专注于BACnet协议，通过映射数据库处理数据包文件并提取上下文，同时使用检索增强生成（RAG）的当代上下文检索方法。处理后的数据包信息与提取的上下文相结合，作为LLM的输入，为用户生成简洁的数据包文件摘要。该软件提供了清晰、连贯且易于理解的网络活动摘要，使SOC分析师能够更好地评估控制系统的当前状态。</p>
<h4 id="_74">一句话总结：</h4>
<p>该研究提出了一种基于LLM的软件解决方案，用于提高SOC分析师对工业控制系统网络活动理解的准确性和效率。</p>
<hr />
<h2 id="fact-aware-multimodal-retrieval-augmentation-for-accurate-medical-radiology-report-generation"><a href="http://arxiv.org/abs/2407.15268v1">Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation</a></h2>
<p>发布时间：2024-07-21</p>
<p>作者：Liwen Sun, James Zhao, Megan Han, Chenyan Xiong</p>
<h4 id="_75">中文摘要：</h4>
<p>本文提出了一种基于事实的多模态检索增强管道（FactMM-RAG），用于生成准确的放射学报告。该管道首先利用RadGraph挖掘事实性报告对，然后整合事实知识来训练一个通用的多模态检索器。给定一张放射学图像，我们的检索器能够识别高质量的参考报告来增强多模态基础模型，从而提高报告生成的事实完整性和正确性。在两个基准数据集上的实验表明，我们的多模态检索器在语言生成和放射学特定指标上均优于最先进的检索器，F1CheXbert和F1RadGraph的得分分别提高了6.5%和2%。进一步的分析表明，采用我们的基于事实的训练策略能够提供有效的监督信号，无需依赖明确的诊断标签指导，并成功地将事实感知能力从多模态检索器传播到放射学报告生成中的多模态基础模型。</p>
<h4 id="_76">一句话总结：</h4>
<p>本文提出了一种基于事实的多模态检索增强管道，显著提高了放射学报告生成的准确性和事实完整性。</p>
<hr />
<h2 id="assessing-brittleness-of-image-text-retrieval-benchmarks-from-vision-language-models-perspective"><a href="http://arxiv.org/abs/2407.15239v2">Assessing Brittleness of Image-Text Retrieval Benchmarks from Vision-Language Models Perspective</a></h2>
<p>发布时间：2024-07-21</p>
<p>作者：Mariya Hendriksen, Shuo Zhang, Ridho Reinanda, Mohamed Yahya, Edgar Meij, Maarten de Rijke</p>
<h4 id="_77">中文摘要：</h4>
<p>图像-文本检索（ITR）是信息检索（IR）中的一个重要任务，它依赖于预训练的视觉-语言模型（VLMs）并持续取得最先进的性能。然而，现有ITR基准的脆弱性是一个重大挑战。在标准的任务数据集中，描述通常提供场景的广泛总结，而忽略了关于特定概念的详细信息。此外，当前的评估设置假设图像和文本之间的简单二元匹配，并侧重于模态内而非跨模态关系，这可能导致对模型性能的错误解读。受此差距的启发，本研究专注于检查ITR评估流程的脆弱性，重点关注概念粒度。我们首先分析了两个常见的基准，MS-COCO和Flickr30k，并将它们与它们的增强版本MS-COCO-FG和Flickr30k-FG进行比较，后者提供了一个特定的语言特征集来捕捉概念粒度。我们发现Flickr30k-FG和MS COCO-FG在所有选定的特征上均持续获得更高的分数。为了调查VLMs在粗粒度和细粒度数据集上的性能，我们引入了一个扰动分类法。我们将这些扰动应用于所选数据集。我们在零样本条件下，在有和无扰动的情况下，对四个最先进的模型——ALIGN、AltCLIP、CLIP和GroupViT——在标准和细粒度数据集上进行了评估。结果表明，尽管扰动通常会降低模型性能，但细粒度数据集的性能下降幅度小于其标准对应版本。此外，所有设置中的相对性能下降在所有模型和数据集上是一致的，这表明问题在于基准。我们通过提供一个改进ITR评估流程的议程来结束论文。</p>
<h4 id="_78">一句话总结：</h4>
<p>本研究揭示了现有ITR基准的脆弱性，并通过引入细粒度数据集和扰动分类法，揭示了VLMs在不同数据集上的性能差异，为改进ITR评估流程提供了新的视角。</p>
<hr />
<h2 id="autovcoder-a-systematic-framework-for-automated-verilog-code-generation-using-llms"><a href="http://arxiv.org/abs/2407.18333v1">AutoVCoder: A Systematic Framework for Automated Verilog Code Generation using LLMs</a></h2>
<p>发布时间：2024-07-21</p>
<p>作者：Mingzhe Gao, Jieru Zhao, Zhe Lin, Wenchao Ding, Xiaofeng Hou, Yu Feng, Chao Li, Minyi Guo</p>
<h4 id="_79">中文摘要：</h4>
<p>最近，大型语言模型（LLMs）在软件代码生成（例如C/C++和Python）方面的应用取得了巨大成功。然而，当涉及到生成寄存器传输级（RTL）代码，如Verilog时，LLMs在语法和功能正确性方面仍然存在不足。为了解决这个问题，本文开发了一个名为AutoVCoder的系统化开源框架，该框架显著提高了LLMs生成Verilog代码的正确性，并同时提升了其输出质量。我们的框架集成了三种新颖的技术，包括高质量硬件数据集生成方法、两轮LLM微调方法和领域特定检索增强生成（RAG）机制。实验结果表明，AutoVCoder在Verilog代码生成方面优于工业和学术LLMs。具体来说，与BetterV相比，AutoVCoder在EvalMachine和EvalHuman基准测试中在功能正确性上分别提高了0.5%和2.2%，在RTLLM基准测试中与RTLCoder相比，语法正确性和功能正确性分别提高了3.4%。</p>
<h4 id="_80">一句话总结：</h4>
<p>AutoVCoder通过集成创新技术，显著提升了LLMs生成Verilog代码的正确性和输出质量。</p>
<hr />
<h2 id="automatic-generation-of-fashion-images-using-prompting-in-generative-machine-learning-models"><a href="http://arxiv.org/abs/2407.14944v1">Automatic Generation of Fashion Images using Prompting in Generative Machine Learning Models</a></h2>
<p>发布时间：2024-07-20</p>
<p>作者：Georgia Argyrou, Angeliki Dimitriou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou</p>
<h4 id="_81">中文摘要：</h4>
<p>人工智能的兴起为时尚行业带来了颠覆性的变革，以前所未有的方式重新定义了创造性和创新。本研究探讨了使用两种不同的大型语言模型和一种稳定的扩散模型来生成定制时尚描述的方法。强调人工智能驱动时尚创造的可适应性，我们摒弃了传统方法，专注于提示技术，如零样本学习和少样本学习，以及思维链（CoT），这导致了各种颜色和纹理的产生，增强了输出的多样性。我们的方法的核心是检索增强生成（RAG），通过从时尚来源中获取见解来丰富模型，以确保当代的表现。评估结合了定量指标，如CLIPscore，以及定性的主观判断，突出了在多种风格中的创造力、连贯性和审美吸引力。在参与者中，RAG和少样本学习技术因其产生更相关和吸引人的时尚描述的能力而被偏爱。我们的代码可在https://github.com/georgiarg/AutoFashion上找到。</p>
<h4 id="_82">一句话总结：</h4>
<p>本研究通过结合大型语言模型和检索增强生成技术，探索了人工智能在时尚描述生成中的创新应用，提高了时尚描述的多样性和吸引力。</p>
<hr />
<h2 id="retrieval-augmented-generation-integrated-large-language-models-in-smart-contract-vulnerability-detection"><a href="http://arxiv.org/abs/2407.14838v1">Retrieval Augmented Generation Integrated Large Language Models in Smart Contract Vulnerability Detection</a></h2>
<p>发布时间：2024-07-20</p>
<p>作者：Jeffy Yu</p>
<h4 id="_83">中文摘要：</h4>
<p>DeFi（去中心化金融）的快速增长伴随着由于智能合约漏洞导致的重大财务损失，这突显了有效安全审计的迫切需求。随着攻击变得越来越频繁，审计服务的必要性和需求也在增加。这尤其给独立开发者和小型企业带来了财务负担，因为他们通常对这些服务的可用资金有限。我们的研究在现有框架的基础上，通过整合检索增强生成（RAG）与大型语言模型（LLMs），特别是采用GPT-4-1106（其128k个令牌的上下文窗口）来构建。我们构建了一个包含830个已知漏洞合约的向量存储库，利用Pinecone进行向量存储，OpenAI的text-embedding-ada-002进行嵌入，以及LangChain构建RAG-LLM管道。提示被设计为提供漏洞检测的二进制答案。我们首先对52个智能合约进行了40次测试，以验证提供的漏洞类型，验证了RAG-LLM的复现性和一致性。观察到了令人鼓舞的结果，在引导检测漏洞中成功率达到了62.7%。其次，我们在没有在提示中提供漏洞类型的情况下对模型进行了“盲”审计设置，其中219个合约进行了40次测试。这种设置评估了没有提示上下文帮助的一般漏洞检测能力。在这些条件下，观察到了60.71%的成功率。虽然结果很有希望，但我们仍然强调目前需要人工审计。我们提供这项研究作为低成本智能合约审计过程的证明概念，朝着安全民主化访问迈进。</p>
<h4 id="_84">一句话总结：</h4>
<p>本研究通过整合RAG和LLMs，提出了一种成本效益高的智能合约审计方法，以应对DeFi领域日益增长的智能合约漏洞问题。</p>
<hr />
<h2 id="differential-privacy-of-cross-attention-with-provable-guarantee"><a href="http://arxiv.org/abs/2407.14717v1">Differential Privacy of Cross-Attention with Provable Guarantee</a></h2>
<p>发布时间：2024-07-20</p>
<p>作者：Jiuxiang Gu, Yingyu Liang, Zhenmei Shi, Zhao Song, Yufa Zhou</p>
<h4 id="_85">中文摘要：</h4>
<p>跨注意力（Cross-attention）已成为许多重要人工智能应用（如检索增强生成（RAG）、系统提示、引导稳定扩散等）中的基本模块。确保跨注意力隐私至关重要且迫切需要，因为其键值矩阵可能包含关于公司和用户敏感信息，许多公司仅从其系统提示或RAG数据中获利。在本工作中，我们设计了一种新颖的差分隐私（DP）数据结构，以理论保证来解决跨注意力的隐私安全问题。具体来说，设$n$为系统提示/RAG数据的输入标记长度，$d$为特征维度，$0 &lt; \alpha \le 1$为相对误差参数，$R$为查询和键矩阵的最大值，$R_w$为值矩阵的最大值，$r,s,\epsilon_s$为多项式核方法的参数。然后，我们的数据结构需要$\widetilde{O}(ndr^2)$的内存消耗，$\widetilde{O}(nr^2)$的初始化时间复杂度和$\widetilde{O}(\alpha^{-1} r^2)$的单个标记查询时间复杂度。此外，我们的数据结构可以保证用户查询是$(\epsilon, \delta)$-DP，具有$\widetilde{O}(n^{-1} \epsilon^{-1} \alpha^{-1/2} R^{2s} R_w r^2)$的加性误差和$n^{-1} (\alpha + \epsilon_s)$的相对误差，介于我们的输出和真实答案之间。此外，我们的结果对自适应查询具有鲁棒性，其中用户可以故意攻击跨注意力系统。据我们所知，这是第一个为跨注意力提供差分隐私（DP）的工作。我们相信它将能够启发在大规模生成模型（LGMs）中的更多隐私算法设计。</p>
<h4 id="_86">一句话总结：</h4>
<p>本研究设计了一种新型的差分隐私数据结构，以理论保证解决跨注意力模块的隐私安全问题，为大规模生成模型中的隐私算法设计提供了新的思路。</p>
<hr />
<h2 id="adversarial-databases-improve-success-in-retrieval-based-large-language-models"><a href="http://arxiv.org/abs/2407.14609v1">Adversarial Databases Improve Success in Retrieval-based Large Language Models</a></h2>
<p>发布时间：2024-07-19</p>
<p>作者：Sean Wu, Michael Koo, Li Yo Kao, Andy Black, Lesley Blum, Fabien Scalzo, Ira Kurtz</p>
<h4 id="_87">中文摘要：</h4>
<p>开源大型语言模型（LLMs）在微调聊天机器人方面展现出巨大潜力，并在推理能力上表现出色，超越了众多现有基准。检索增强生成（RAG）是一种通过利用外部知识数据库来提高LLMs在未明确训练任务上的性能的技术。许多研究表明，使用包含相关背景信息的向量数据集时，RAG能够更成功地完成下游任务。该领域的人士普遍隐含地假设，如果在此情境下使用对抗性背景信息，基于RAG的方法的成功将不复存在，甚至可能对结果产生负面影响。为了验证这一假设，我们对几个开源LLMs进行了测试，以检验RAG在提高它们在肾病专科领域回答多项选择题（MCQ）成功率的潜力。与以往的研究不同，我们考察了RAG在利用相关和对抗性背景数据库方面的效果。我们设置了几个开源LLMs，包括Llama 3、Phi-3、Mixtral 8x7b、Zephyrβ和Gemma 7B Instruct，在一个零样本RAG管道中进行测试。作为对抗性信息源，我们使用了《圣经》文本和一个随机词汇生成的数据库进行比较。我们的数据显示，当结合相关信息的向量数据库时，大多数开源LLMs如预期的那样提高了多项选择题的成功率。然而，令人惊讶的是，对抗性的《圣经》文本显著提高了许多LLMs的成功率，甚至随机词汇文本也提高了某些模型的表现能力。总之，我们的结果表明，对抗性信息数据集能够提高基于RAG的LLMs成功率，这是一种反直觉的能力。</p>
<h4 id="_88">一句话总结：</h4>
<p>研究发现，对抗性信息数据集能够提高基于检索增强生成（RAG）的开源大型语言模型（LLMs）在回答医学专科领域多项选择题时的成功率。</p>
<hr />
<h2 id="chatqa-2-bridging-the-gap-to-proprietary-llms-in-long-context-and-rag-capabilities"><a href="http://arxiv.org/abs/2407.14482v1">ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities</a></h2>
<p>发布时间：2024-07-19</p>
<p>作者：Peng Xu, Wei Ping, Xianchao Wu, Zihan Liu, Mohammad Shoeybi, Bryan Catanzaro</p>
<h4 id="_89">中文摘要：</h4>
<p>在这项工作中，我们介绍了ChatQA 2，这是一个基于Llama3的模型，旨在弥合开放访问LLM（例如GPT-4-Turbo）在长上下文理解和检索增强生成（RAG）能力之间的差距。这两个能力对于LLM处理无法适应单个提示的大量信息至关重要，并且根据下游任务和计算预算，它们是相互补充的。我们提出了一种详细的持续训练配方，将Llama3-70B-base的上下文窗口从8K扩展到128K个token，以及一个三阶段的指令微调过程，以增强模型的指令遵循、RAG性能和长上下文理解能力。我们的结果表明，Llama3-ChatQA-2-70B模型在许多长上下文理解任务上达到了与GPT-4-Turbo-2024-0409相当的准确率，并在RAG基准测试中超过了它。有趣的是，我们发现最先进的长期上下文检索器可以缓解RAG中的top-k上下文碎片化问题，从而进一步提高基于RAG的长上下文理解任务的结果。我们还提供了使用最先进的长期上下文LLM对RAG和长上下文解决方案的广泛比较。</p>
<h4 id="_90">一句话总结：</h4>
<p>ChatQA 2模型通过扩展上下文窗口和优化指令微调，显著提升了长上下文理解和检索增强生成能力，在多个任务上超越了GPT-4-Turbo。</p>
<hr />
<h2 id="improving-retrieval-in-sponsored-search-by-leveraging-query-context-signals"><a href="http://arxiv.org/abs/2407.14346v1">Improving Retrieval in Sponsored Search by Leveraging Query Context Signals</a></h2>
<p>发布时间：2024-07-19</p>
<p>作者：Akash Kumar Mohankumar, Gururaj K, Gagan Madan, Amit Singh</p>
<h4 id="_91">中文摘要：</h4>
<p>在赞助搜索中，准确检索与用户查询相关的关键词至关重要，但对于短而模糊的查询，这仍然是一个挑战。现有的密集和生成式检索模型往往无法捕捉到这些情况下的细微用户意图。为了解决这个问题，我们提出了一种通过增强查询以包含来自网页搜索结果和大型语言模型（存储在在线缓存中）的丰富上下文信号来提高查询理解的方法。具体来说，我们使用网页搜索标题和摘要将查询定位到现实世界信息，并利用GPT-4生成查询重写和解释，以阐明用户意图。这些信号通过基于Unity架构的Fusion-in-Decoder方法有效地整合，使得密集和生成式检索在服务成本上与传统无上下文模型相当。为了解决缓存中不可用上下文的情况，我们引入了上下文浏览，这是一种课程学习策略，即使在推理过程中没有上下文信号，也能提高模型的鲁棒性和性能。大量的离线实验表明，我们的上下文感知方法在性能上显著优于无上下文模型。此外，在160多个国家的知名搜索引擎上的在线A/B测试显示，用户参与度和收入都有显著提升。</p>
<h4 id="_92">一句话总结：</h4>
<p>该研究提出了一种通过增强查询上下文信息来提高赞助搜索中关键词检索准确性的方法，显著提升了用户参与度和收入。</p>
<hr />
<h2 id="unipa-gpt-large-language-models-for-university-oriented-qa-in-italian"><a href="http://arxiv.org/abs/2407.14246v2">Unipa-GPT: Large Language Models for university-oriented QA in Italian</a></h2>
<p>发布时间：2024-07-19</p>
<p>作者：Irene Siragusa, Roberto Pirrone</p>
<h4 id="_93">中文摘要：</h4>
<p>本文阐述了Unipa-GPT的架构和训练过程，这是一个基于大型语言模型的聊天机器人，旨在帮助帕勒莫大学的学生选择本/硕学位课程。Unipa-GPT依赖于gpt-3.5-turbo模型，并在欧洲研究者之夜（SHARPER之夜）的背景下进行了展示。在实验中，我们采用了检索增强生成（RAG）方法和微调来开发该系统。本文详细介绍了Unipa-GPT的整体架构，并对RAG和微调系统进行了比较，同时对其性能进行了简要讨论。此外，还与其他大型语言模型进行了比较，并展示了SHARPER之夜的实验结果。</p>
<h4 id="_94">一句话总结：</h4>
<p>本文介绍了基于大型语言模型的聊天机器人Unipa-GPT的架构和训练，旨在辅助学生选择学位课程，并通过实验验证了其性能。</p>
<hr />
<h2 id="rag-qa-arena-evaluating-domain-robustness-for-long-form-retrieval-augmented-question-answering"><a href="http://arxiv.org/abs/2407.13998v1">RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering</a></h2>
<p>发布时间：2024-07-19</p>
<p>作者：Rujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu, William Yang Wang, Bonan Min, Vittorio Castelli</p>
<h4 id="_95">中文摘要：</h4>
<p>基于检索增强生成（RAG-QA）的问答是自然语言处理（NLP）中的一个重要研究课题，并在现实世界中有着广泛的应用。然而，大多数现有的用于此任务的语料库要么是使用单一来源的语料库构建的，要么是由简短的抽取式答案组成，这不足以评估基于大型语言模型（LLM）的RAG-QA系统在跨领域泛化方面的能力。为了解决这些局限性，我们创建了长格式鲁棒问答（LFRQA）数据集，它包含人类编写的长格式答案，这些答案将来自多个文档的简短抽取式答案整合成一个单一、连贯的叙述，涵盖了26K个查询和跨越七个不同领域的大规模语料库。我们进一步提出了RAG-QA竞技场，通过直接比较模型生成的答案与LFRQA的答案，并使用LLM作为评估者。通过广泛的实验，我们表明RAG-QA竞技场与人类对答案质量的判断高度相关。此外，只有41.3%的最具竞争力的LLM答案被优先选择于LFRQA的答案，这证明了RAG-QA竞技场是未来研究的一个具有挑战性的评估平台。</p>
<h4 id="_96">一句话总结：</h4>
<p>本研究提出了长格式鲁棒问答数据集和RAG-QA竞技场，以评估基于LLM的RAG-QA系统在跨领域泛化方面的能力，并发现现有模型在答案质量上仍有待提高。</p>
<hr />
<h2 id="auditnet-a-conversational-ai-based-security-assistant-demo"><a href="http://arxiv.org/abs/2407.14116v1">AuditNet: A Conversational AI-based Security Assistant [DEMO]</a></h2>
<p>发布时间：2024-07-19</p>
<p>作者：Shohreh Deldari, Mohammad Goudarzi, Aditya Joshi, Arash Shaghaghi, Simon Finn, Flora D. Salim, Sanjay Jha</p>
<h4 id="_97">中文摘要：</h4>
<p>在信息过载的时代，各个领域的专业人士面临着在大量文档和不断变化的规范中导航的挑战。确保符合标准、法规和合同义务是各个专业领域中的一个关键且复杂的任务。我们提出了一种通用的对话式人工智能助手框架，旨在促进各种领域（包括但不限于网络基础设施、法律合同、教育标准、环境法规和政府政策）中的合规性检查。通过利用大型语言模型进行的检索增强生成，我们的框架自动化了相关、上下文感知信息的审查、索引和检索，简化了验证遵守既定指南和要求的流程。这个人工智能助手不仅减少了合规性检查中的手动工作量，还提高了准确性和效率，支持专业人士保持高标准的实践并确保各自领域的法规合规。我们提出了AuditNet，这是第一个旨在通过提供即时访问安全标准、政策和法规来协助物联网网络安全专家的对话式人工智能安全助手。</p>
<h4 id="_98">一句话总结：</h4>
<p>本研究提出了一种基于人工智能的对话式助手框架，旨在自动化和优化不同领域的合规性检查过程，以支持专业人士确保法规遵守。</p>
<hr />
<h2 id="black-box-opinion-manipulation-attacks-to-retrieval-augmented-generation-of-large-language-models"><a href="http://arxiv.org/abs/2407.13757v1">Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models</a></h2>
<p>发布时间：2024-07-18</p>
<p>作者：Zhuo Chen, Jiawei Liu, Haotan Liu, Qikai Cheng, Fan Zhang, Wei Lu, Xiaozhong Liu</p>
<h4 id="_99">中文摘要：</h4>
<p>本文研究了检索增强生成（RAG）模型在面临黑盒攻击时的脆弱性，特别是针对意见操纵的攻击。通过操纵RAG中检索模型的排名结果，并使用这些结果作为数据训练一个代理模型，本文实现了对RAG的黑盒迁移攻击。实验表明，这种攻击策略可以显著改变RAG生成内容的意见极性，揭示了模型在用户认知和决策上的潜在负面影响，从而更容易误导用户接受错误或偏颇的信息。研究主要探索了RAG在白盒和封闭域问答任务中的不可靠性，并提供了新的见解来增强RAG模型的可靠性和安全性。（Retrieval-Augmented Generation, RAG; 黑盒攻击, black-box attack; 意见操纵, opinion manipulation）</p>
<h4 id="_100">一句话总结：</h4>
<p>本文揭示了检索增强生成模型在黑盒攻击下对意见操纵的脆弱性，并展示了其可能对用户认知和决策产生的负面影响。</p>
<hr />
<h2 id="can-open-source-llms-compete-with-commercial-models-exploring-the-few-shot-performance-of-current-gpt-models-in-biomedical-tasks"><a href="http://arxiv.org/abs/2407.13511v1">Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks</a></h2>
<p>发布时间：2024-07-18</p>
<p>作者：Samy Ateia, Udo Kruschwitz</p>
<h4 id="_101">中文摘要：</h4>
<p>商业大型语言模型（LLMs），如OpenAI的GPT-4（驱动ChatGPT）和Anthropic的Claude 3 Opus，在不同领域的自然语言处理（NLP）基准测试中占据主导地位。新的开源替代品，如Mixtral 8x7B或Llama 3，已经出现，并且似乎正在缩小差距，同时通常提供更高的吞吐量和更低的成本。开源LLMs还可以自行托管，这使得它们对于企业和使用敏感数据的临床用例来说很有吸引力。我们参与了第12届BioASQ挑战赛，这是一个检索增强生成（RAG）设置，并探讨了当前GPT模型Claude 3 Opus、GPT-3.5-turbo和Mixtral 8x7b在上下文学习（零样本、少样本）和QLoRa微调下的性能。我们还探讨了将来自维基百科的额外相关知识添加到LLM的上下文窗口中可能会如何提高其性能。Mixtral 8x7b在10样本设置中具有竞争力，无论是带微调还是不带微调，但在零样本设置中未能产生可用的结果。QLoRa微调和维基百科上下文并没有导致可测量的性能提升。我们的结果表明，在RAG设置中，商业模型与开源模型之间的性能差距主要存在于零样本设置，并且可以通过简单地收集特定用例的少样本示例来缩小。</p>
<h4 id="_102">一句话总结：</h4>
<p>本研究探讨了开源大型语言模型在检索增强生成任务中的性能，发现通过收集特定领域的少样本示例可以缩小与商业模型之间的性能差距。</p>
<hr />
<h2 id="retrieval-augmented-generation-for-natural-language-processing-a-survey"><a href="http://arxiv.org/abs/2407.13193v2">Retrieval-Augmented Generation for Natural Language Processing: A Survey</a></h2>
<p>发布时间：2024-07-18</p>
<p>作者：Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, Chun Jason Xue</p>
<h4 id="_103">中文摘要：</h4>
<p>大型语言模型（LLMs）在各种领域取得了巨大成功，得益于其庞大的参数量，这些参数存储了知识。然而，LLMs仍然存在一些关键问题，例如幻觉问题、知识更新问题和缺乏特定领域的专业知识。检索增强生成（RAG）的出现，通过利用外部知识数据库来增强LLMs，弥补了LLMs的这些不足。本文回顾了RAG的所有重要技术，特别是在检索器和检索融合方面。此外，还提供了实现RAG代表性技术的教程代码。本文进一步讨论了RAG的训练，包括带/不带数据存储更新的RAG。然后，我们介绍了RAG在代表性自然语言处理任务和工业场景中的应用。最后，本文讨论了RAG的未来发展方向和挑战，以促进其发展。</p>
<h4 id="_104">一句话总结：</h4>
<p>本文全面回顾了检索增强生成（RAG）技术，探讨了其在自然语言处理任务和工业场景中的应用，并展望了其未来发展方向。</p>
<hr />
<h2 id="retrieve-summarize-plan-advancing-multi-hop-question-answering-with-an-iterative-approach"><a href="http://arxiv.org/abs/2407.13101v1">Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach</a></h2>
<p>发布时间：2024-07-18</p>
<p>作者：Zhouyu Jiang, Mengshu Sun, Lei Liang, Zhiqiang Zhang</p>
<h4 id="_105">中文摘要：</h4>
<p>多跳问答是一个具有独特工业相关性的挑战性任务，基于大型语言模型（LLMs）的检索增强生成（RAG）方法已成为解决这一任务的一种流行方法。由于可能无法在一次迭代中检索到所有必要的信息，最近开发了一系列迭代RAG方法，显示出显著的性能提升。然而，现有方法仍然面临两个关键挑战：由于多次检索导致的上下文过载，以及由于缺乏记录的检索轨迹而导致的过度规划和重复规划。在本文中，我们提出了一种名为ReSP的新型迭代RAG方法，配备了一个具有双重功能的摘要器。该摘要器同时压缩检索到的文档中的信息，针对总体问题和当前子问题。在多跳问答数据集HotpotQA和2WikiMultihopQA上的实验结果表明，我们的方法在性能上显著优于现有方法，并且在上下文长度方面表现出卓越的鲁棒性。</p>
<h4 id="_106">一句话总结：</h4>
<p>本文提出了一种名为ReSP的迭代RAG方法，通过双重功能摘要器有效解决了多跳问答中的上下文过载和过度规划问题，显著提升了问答性能。</p>
<hr />
<h2 id="pragyan-connecting-the-dots-in-tweets"><a href="http://arxiv.org/abs/2407.13909v1">PRAGyan -- Connecting the Dots in Tweets</a></h2>
<p>发布时间：2024-07-18</p>
<p>作者：Rahul Ravi, Gouri Ginde, Jon Rokne</p>
<h4 id="_107">中文摘要：</h4>
<p>随着社交媒体平台的增长，理解事件和声明背后的深层原因对于企业、政策制定者和研究人员来说变得至关重要。本研究探讨了知识图谱（KGs）与大型语言模型（LLMs）的集成，以对推文数据集进行因果分析。LLM辅助的分析技术在揭示驱动观察到的效果的因果关系方面往往缺乏深度。通过利用编码丰富语义关系和时间信息的KGs和LLMs，本研究旨在揭示影响因果动态的复杂因素之间的相互作用，并比较使用GPT-3.5 Turbo获得的结果。我们采用了一种检索增强生成（RAG）模型，利用存储在Neo4j（又称PRAGyan）数据格式中的KG来检索因果推理的相关上下文。我们的方法表明，与基线LLM（GPT-3.5 Turbo）模型相比，当源语料库的规模增加时，KG增强的LLM RAG可以提供改进的结果。我们的定性分析突出了将KGs与LLMs结合以改善可解释性和可操作见解的优势，这有助于在各个领域促进基于信息的决策。而使用BLEU和余弦相似度等指标进行的定量分析表明，我们的方法比基线方法提高了10%。</p>
<h4 id="_108">一句话总结：</h4>
<p>本研究通过结合知识图谱和大型语言模型，实现了对社交媒体推文数据集的因果分析，显著提升了分析结果的准确性和可解释性。</p>
<hr />
<h2 id="semi-supervised-contrastive-learning-of-musical-representations"><a href="http://arxiv.org/abs/2407.13840v1">Semi-Supervised Contrastive Learning of Musical Representations</a></h2>
<p>发布时间：2024-07-18</p>
<p>作者：Julien Guinot, Elio Quinton, György Fazekas</p>
<h4 id="_109">中文摘要：</h4>
<p>尽管对比学习在音乐信息检索（MIR）中取得了成功，但对比自监督的固有歧义仍然是一个挑战。仅仅依赖于增强链和自监督正样本采样策略可能导致预训练目标无法捕捉到下游任务所需的关键音乐信息。我们引入了半监督对比学习（SemiSupCon），这是一种简单的方法，用于在音乐表示的对比学习中利用音乐信息的有标签数据（监督信号）。我们的方法通过在比先前方法更简单的框架中结合监督和自监督对比目标，将音乐相关的监督信号引入自监督对比学习。这个框架在具有适度标签数据的各种下游MIR任务上提高了下游性能和对抗音频损坏的鲁棒性。我们的方法通过选择有标签数据来塑造学习到的相似性度量，这些数据（1）将音乐领域知识融入表示中，并且（2）在最小化下游性能损失的情况下提高跨领域性能。我们在音乐相关但并非显而易见相似的任务上展示了强大的迁移学习性能，例如音高和调式估计。此外，我们的方法在自动标记任务上的性能优于仅包含5%可用标签的自监督方法，这些标签被包含在预训练中。</p>
<h4 id="_110">一句话总结：</h4>
<p>该研究提出了一种半监督对比学习方法，通过结合监督和自监督信息，有效提升了音乐信息检索任务中的模型性能和鲁棒性。</p>
<hr />
<h2 id="visual-haystacks-answering-harder-questions-about-sets-of-images"><a href="http://arxiv.org/abs/2407.13766v1">Visual Haystacks: Answering Harder Questions About Sets of Images</a></h2>
<p>发布时间：2024-07-18</p>
<p>作者：Tsung-Han Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph E. Gonzalez, Trevor Darrell, David M. Chan</p>
<h4 id="_111">中文摘要：</h4>
<p>近年来，大型多模态模型（LMMs）在单图像视觉问答领域取得了显著进展。然而，当这些模型面对需要跨越大量图像集的查询时，例如在大型相册中搜索、在互联网上查找特定信息或通过卫星图像监测环境变化等现实场景，它们面临着巨大的挑战。本文探讨了多图像视觉问答（MIQA）任务：给定一组图像和自然语言查询，任务是生成一个相关且基于事实的回答。我们提出了一种新的公共基准，称为“视觉稻草堆”（VHs），专门用于评估LMMs在处理无关图像集的视觉检索和推理方面的能力。我们进行了全面的评估，表明即使是稳健的闭源模型也面临着巨大的挑战。为了解决这些不足，我们引入了MIRAGE（多图像检索增强生成），这是一个为LMMs量身定制的检索/问答框架，它通过显著提高效率和准确性来应对MIQA的挑战。我们的评估表明，MIRAGE在VHs基准上超过了闭源的GPT-4o模型，并且比以文本为重点的多阶段方法提高了高达3.4倍的效率。</p>
<h4 id="_112">一句话总结：</h4>
<p>本文提出了一种名为MIRAGE的新型检索/问答框架，显著提高了大型多模态模型在多图像视觉问答任务中的效率和准确性。</p>
<hr />
<h2 id="halu-j-critique-based-hallucination-judge"><a href="http://arxiv.org/abs/2407.12943v1">Halu-J: Critique-Based Hallucination Judge</a></h2>
<p>发布时间：2024-07-17</p>
<p>作者：Binjie Wang, Steffi Chern, Ethan Chern, Pengfei Liu</p>
<h4 id="_113">中文摘要：</h4>
<p>大型语言模型（LLMs）经常生成非事实内容，被称为幻觉。现有的基于检索增强的幻觉检测方法通常将其视为一个分类任务，通过评估幻觉与检索到的证据的一致性来检测幻觉。然而，这种方法通常缺乏对这些评估的详细解释，并且没有评估这些解释的可靠性。此外，检索系统的缺陷可能导致检索到无关或不完全相关的证据，从而损害检测过程。此外，尽管现实世界的幻觉检测需要分析多个证据，但当前系统通常对所有证据一视同仁，不考虑其与内容的关联性。为了解决这些挑战，我们引入了Halu-J，一个具有70亿参数的基于批评的幻觉判断器。Halu-J通过选择相关证据并提供详细批评来增强幻觉检测。我们的实验表明，Halu-J在多证据幻觉检测方面优于GPT-4o，并在批评生成和证据选择方面的能力与其相匹配。我们还引入了ME-FEVER，一个专为多证据幻觉检测设计的新数据集。我们的代码和数据集可在https://github.com/GAIR-NLP/factool 找到。</p>
<h4 id="_114">一句话总结：</h4>
<p>Halu-J通过选择相关证据和提供详细批评，显著提升了多证据幻觉检测的准确性和可靠性。</p>
<hr />
<h2 id="struct-x-enhancing-large-language-models-reasoning-with-structured-data"><a href="http://arxiv.org/abs/2407.12522v1">Struct-X: Enhancing Large Language Models Reasoning with Structured Data</a></h2>
<p>发布时间：2024-07-17</p>
<p>作者：Xiaoyu Tan, Haoyu Wang, Xihe Qiu, Yuan Cheng, Yinghui Xu, Wei Chu, Yuan Qi</p>
<h4 id="_115">中文摘要：</h4>
<p>结构化数据，富含逻辑和关系信息，具有增强大型语言模型（LLMs）推理能力的潜力。然而，由于其可能给LLMs带来过多的标记和无关的上下文信息，其整合面临挑战。为了解决这个问题，我们提出了Struct-X，一个新颖的框架，通过五个关键阶段“读取-建模-填充-反思-推理”有效地使LLMs能够利用结构化数据。它首先使用图嵌入将结构化数据编码到拓扑空间中，然后通过知识检索模块填充缺失的实体信息，并通过自监督模块过滤掉无关的标记。最后阶段涉及使用选定的标记构建拓扑网络，以进一步减少总标记长度，从而更有效地进行LLMs推理。此外，Struct-X还包括一个辅助模块，该模块经过训练以生成提示，帮助LLMs分析结构化数据。在知识图谱问答任务和长文档阅读理解任务等基准测试上的大量实验表明，Struct-X显著提高了LLMs的推理能力，证明了结构化数据增强在提高LLMs对复杂输入上下文推理的有效性。</p>
<h4 id="_116">一句话总结：</h4>
<p>Struct-X通过有效利用结构化数据，显著提升了大型语言模型的推理能力。</p>
<hr />
<h2 id="explainable-biomedical-hypothesis-generation-via-retrieval-augmented-generation-enabled-large-language-models"><a href="http://arxiv.org/abs/2407.12888v1">Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models</a></h2>
<p>发布时间：2024-07-17</p>
<p>作者：Alexander R. Pelletier, Joseph Ramirez, Irsyad Adam, Simha Sankar, Yu Yan, Ding Wang, Dylan Steinecke, Wei Wang, Peipei Ping</p>
<h4 id="_117">中文摘要：</h4>
<p>当前可用的生物医学信息量巨大，给研究人员有效消化、处理和理解这些发现带来了重大挑战。大型语言模型（LLMs）已成为导航这一复杂且具有挑战性的数据景观的有力工具。然而，LLMs可能会导致幻觉性响应，这使得检索增强生成（RAG）对于实现准确信息至关重要。在本协议中，我们提出了RUGGED（基于图引导的可解释疾病区分检索），这是一个综合工作流程，旨在支持研究人员进行知识整合和假设生成，确定可行的路径。通过文本挖掘关联分析和疾病节点上的可解释图预测模型，对出版物和知识库中的相关生物医学信息进行审查、整合和提取，预测药物和疾病之间的潜在联系。这些分析以及生物医学文本被整合到一个框架中，该框架通过RAG启用的LLMs促进用户指导的机制阐明以及假设探索。一个临床用例展示了RUGGED评估和推荐心律失常性心肌病（ACM）和扩张型心肌病（DCM）治疗药物的能力，分析处方药物的分子相互作用和未探索的用途。该平台最小化了LLM的幻觉，提供了可操作的见解，并改善了新型治疗药物的调查研究。</p>
<h4 id="_118">一句话总结：</h4>
<p>RUGGED通过整合文本挖掘和可解释图预测模型，利用RAG和LLMs，为研究人员提供了一种评估和推荐新型治疗药物的方法，同时减少LLM的幻觉性响应。</p>
<hr />
<h2 id="conversational-query-reformulation-with-the-guidance-of-retrieved-documents"><a href="http://arxiv.org/abs/2407.12363v1">Conversational Query Reformulation with the Guidance of Retrieved Documents</a></h2>
<p>发布时间：2024-07-17</p>
<p>作者：Jeonghyun Park, Hwanhee Lee</p>
<h4 id="_119">中文摘要：</h4>
<p>对话式搜索旨在为对话式问答（ConvQA）中的给定问题检索相关段落。ConvQA中的问题面临诸如省略和指代等挑战，这使得获得期望的搜索结果变得困难。对话式查询重写（CQR）将这些当前查询转换为去上下文形式以解决这些问题。然而，现有的CQR方法主要关注重写对人类友好的查询，这并不总是能产生对检索器最优的搜索结果。为了克服这一挑战，我们引入了GuideCQR，这是一个利用引导文档来细化查询的框架，确保查询对检索器是最优的。具体来说，我们增强了关键词，从重新排序的文档中生成预期答案，并将它们与过滤过程统一。实验结果表明，由引导文档增强的查询优于之前的CQR方法。特别是，GuideCQR超越了由大型语言模型（LLM）提示驱动的方法的性能，并证明了引导文档在跨不同设置制定检索器友好查询中的重要性。</p>
<h4 id="_120">一句话总结：</h4>
<p>GuideCQR通过使用引导文档来优化查询，显著提升了对话式问答中检索相关段落的准确性。</p>
<hr />
<h2 id="agentpoison-red-teaming-llm-agents-via-poisoning-memory-or-knowledge-bases"><a href="http://arxiv.org/abs/2407.12784v1">AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases</a></h2>
<p>发布时间：2024-07-17</p>
<p>作者：Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, Bo Li</p>
<h4 id="_121">中文摘要：</h4>
<p>大型语言模型（LLM）代理在各种应用中表现出卓越的性能，这主要归功于它们在推理、利用外部知识和工具、调用API以及与环境交互执行动作方面的先进能力。当前的代理通常使用一个记忆模块或检索增强生成（RAG）机制，从知识库中检索过去的知识和具有相似嵌入的实例，以指导任务规划和执行。然而，对未经验证的知识库的依赖引发了关于它们的安全性和可信度的重大担忧。为了揭示这些漏洞，我们提出了一种新颖的红队攻击方法AgentPoison，这是第一个针对通用和基于RAG的LLM代理的后门攻击，通过污染其长期记忆或RAG知识库来实现。具体来说，我们将触发生成过程作为一个约束优化问题，通过将触发实例映射到一个独特的嵌入空间来优化后门触发器，以确保每当用户指令包含优化的后门触发器时，恶意演示以高概率从被污染的记忆或知识库中检索出来。与此同时，没有触发器的良性指令仍将保持正常性能。与传统的后门攻击不同，AgentPoison不需要额外的模型训练或微调，并且优化的后门触发器表现出优异的可迁移性、上下文一致性和隐蔽性。广泛的实验表明，AgentPoison在攻击三种类型的真实世界LLM代理（基于RAG的自动驾驶代理、知识密集型问答代理和医疗保健EHRAgent）方面非常有效。在每个代理上，AgentPoison的平均攻击成功率超过80%，对良性性能的影响最小（小于1%），污染率低于0.1%。</p>
<h4 id="_122">一句话总结：</h4>
<p>AgentPoison是一种针对LLM代理的后门攻击方法，通过污染其长期记忆或知识库来提高恶意演示的检索概率，同时保持良性指令的正常性能。</p>
<hr />
<h2 id="echosight-advancing-visual-language-models-with-wiki-knowledge"><a href="http://arxiv.org/abs/2407.12735v1">EchoSight: Advancing Visual-Language Models with Wiki Knowledge</a></h2>
<p>发布时间：2024-07-17</p>
<p>作者：Yibin Yan, Weidi Xie</p>
<h4 id="_123">中文摘要：</h4>
<p>本文介绍了一种名为EchoSight的新型多模态检索增强生成（RAG）框架，该框架能够使大型语言模型（LLMs）利用广泛的百科知识来回答视觉问题。EchoSight通过仅使用视觉信息搜索维基百科文章，然后根据候选文章与文本-图像查询的相关性进行进一步排序，以实现高性能的检索。这种方法显著提高了多模态知识的整合，从而提高了检索结果和视觉问答（VQA）响应的准确性。在Encyclopedic VQA和InfoSeek数据集上的实验结果表明，EchoSight在基于知识的VQA中建立了新的最先进结果，在Encyclopedic VQA上达到了41.8%的准确率，在InfoSeek上达到了31.3%的准确率。</p>
<h4 id="_124">一句话总结：</h4>
<p>EchoSight通过结合视觉信息和百科知识，显著提升了基于知识的视觉问答系统的性能。</p>
<hr />
<h2 id="optimizing-query-generation-for-enhanced-document-retrieval-in-rag"><a href="http://arxiv.org/abs/2407.12325v1">Optimizing Query Generation for Enhanced Document Retrieval in RAG</a></h2>
<p>发布时间：2024-07-17</p>
<p>作者：Hamin Koo, Minseon Kim, Sung Ju Hwang</p>
<h4 id="_125">中文摘要：</h4>
<p>大型语言模型（LLMs）在多种语言任务上表现出色，但它们经常生成错误信息，这种现象被称为“幻觉”。检索增强生成（RAG）旨在通过使用文档检索来减轻这一问题，以获得准确的回答。然而，由于查询模糊，RAG仍然面临幻觉问题。本研究旨在通过优化查询生成，使用查询-文档对齐分数，以及利用LLMs来细化查询，以提高文档检索的精确性和效率，从而改进RAG。实验表明，我们的方法提高了文档检索的准确性，平均准确率提高了1.6%。</p>
<h4 id="_126">一句话总结：</h4>
<p>本研究通过优化查询生成和利用LLMs细化查询，有效提高了检索增强生成（RAG）的文档检索准确性。</p>
<hr />
<h2 id="better-rag-using-relevant-information-gain"><a href="http://arxiv.org/abs/2407.12101v1">Better RAG using Relevant Information Gain</a></h2>
<p>发布时间：2024-07-16</p>
<p>作者：Marc Pickett, Jeremy Hartman, Ayan Kumar Bhowmick, Raquib-ul Alam, Aditya Vempaty</p>
<h4 id="_127">中文摘要：</h4>
<p>一种扩展大型语言模型（LLMs）记忆的常见方法是检索增强生成（RAG），它将来自更大记忆库中的文本插入到LLM的上下文窗口中。然而，上下文窗口通常限制在几千个标记内，这限制了可以告知模型响应的检索段落数量。因此，通过确保检索段落之间的多样性来避免占用上下文窗口空间中的冗余信息是很重要的。同时，信息也应与当前任务相关。大多数先前鼓励检索结果多样性的方法，如最大边际相关度（MMR），是通过引入一个明确权衡多样性和相关性的目标来实现的。我们提出了一种基于相关信息增益的新型简单优化指标，这是一种针对一组检索结果的查询相关总信息的概率度量。通过优化此指标，多样性从我们的系统中自然产生。当用作RAG系统的检索组件的替代品时，该方法在检索增强生成基准（RGB）的问答任务上实现了最先进的性能，超过了直接优化相关性和多样性的现有指标。</p>
<h4 id="_128">一句话总结：</h4>
<p>该研究提出了一种基于相关信息增益的优化方法，通过提高检索结果的相关性和多样性，显著提升了RAG系统的问答任务性能。</p>
<hr />
<h2 id="bright-a-realistic-and-challenging-benchmark-for-reasoning-intensive-retrieval"><a href="http://arxiv.org/abs/2407.12883v1">BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval</a></h2>
<p>发布时间：2024-07-16</p>
<p>作者：Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, Tao Yu</p>
<h4 id="_129">中文摘要：</h4>
<p>现有的检索基准主要包含信息检索查询（例如，来自搜索引擎的聚合问题），其中基于关键词或语义的检索通常足够。然而，许多复杂的现实世界查询需要深入推理来识别相关文档，而不仅仅是表面形式的匹配。例如，寻找编码问题的文档需要理解涉及函数的逻辑和语法。为了更好地评估此类具有挑战性的查询的检索效果，我们引入了BRIGHT，这是第一个要求进行密集推理以检索相关文档的文本检索基准。BRIGHT由从不同领域（如经济学、心理学、机器人学、软件工程、地球科学等）收集的1,398个现实世界查询构建而成，这些查询来源于自然发生或精心整理的人类数据。广泛的评估表明，即使是最先进的检索模型在BRIGHT上的表现也较差。在MTEB排行榜上领先的模型[38]，其得分达到59.0 nDCG@10，在BRIGHT上的nDCG@10得分仅为18.0。我们进一步证明，通过添加由大型语言模型（LLMs）生成的思维链推理来增强查询，可以提高性能高达12.2分。此外，BRIGHT对在基准模型预训练期间的数据泄露具有鲁棒性，因为我们通过即使在基准文档包含在训练数据中也表现出相似性能来验证这一点。我们相信，BRIGHT为在更现实和具有挑战性的环境中进行检索系统研究铺平了道路。我们的代码和数据可在https://brightbenchmark.github.io获取。</p>
<h4 id="_130">一句话总结：</h4>
<p>BRIGHT是首个要求进行密集推理以检索相关文档的文本检索基准，揭示了现有检索模型在复杂查询上的不足，并通过引入LLMs生成的思维链推理显著提升了检索性能。</p>
<hr />
<h2 id="a-comprehensive-evaluation-of-large-language-models-on-temporal-event-forecasting"><a href="http://arxiv.org/abs/2407.11638v1">A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting</a></h2>
<p>发布时间：2024-07-16</p>
<p>作者：He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua</p>
<h4 id="_131">中文摘要：</h4>
<p>最近，大型语言模型（LLMs）在知识问答、数学推理和常识推理等数据挖掘任务中展现出巨大的潜力。然而，LLMs在时间事件预测方面的推理能力尚未得到充分探索。为了系统地研究它们在时间事件预测方面的能力，我们全面评估了基于LLMs的时间事件预测方法。由于缺乏同时涉及图和文本数据的高质量数据集，我们首先构建了一个名为MidEast-TE-mini的基准数据集。基于这个数据集，我们设计了一系列基线方法，这些方法以不同的输入格式和检索增强生成（RAG）模块为特征。从广泛的实验中，我们发现直接将原始文本集成到LLMs的输入中并不能增强零样本外推性能。相反，将原始文本融入特定复杂事件并微调LLMs可以显著提高性能。此外，增强检索模块后，LLM可以有效地捕捉隐藏在历史事件中的时间关系模式。同时，在LLMs中，如流行度偏差和长尾问题等问题仍然存在，尤其是在基于RAG的方法中。这些发现不仅加深了我们对于基于LLMs的事件预测方法的理解，也突出了几个有希望的研究方向。我们认为，这次全面评估以及确定的研究机会将对未来通过LLMs进行时间事件预测的研究产生重大贡献。</p>
<h4 id="_132">一句话总结：</h4>
<p>本研究通过构建基准数据集和设计基线方法，全面评估了LLMs在时间事件预测中的能力，揭示了LLMs在捕捉时间关系模式方面的潜力及其存在的问题。</p>
<hr />
<h2 id="scientific-qa-system-with-verifiable-answers"><a href="http://arxiv.org/abs/2407.11485v1">Scientific QA System with Verifiable Answers</a></h2>
<p>发布时间：2024-07-16</p>
<p>作者：Adela Ljajić, Miloš Košprdić, Bojana Bašaragin, Darija Medvecki, Lorenzo Cassano, Nikola Milošević</p>
<h4 id="_133">中文摘要：</h4>
<p>本文介绍了VerifAI项目，这是一个开创性的开源科学问答系统，旨在提供不仅基于参考文献，而且经过自动审核和可验证的答案。该系统的组成部分包括：（1）一个结合语义和词汇搜索技术，在科学论文（PubMed）中进行信息检索的系统；（2）一个检索增强生成（RAG）模块，使用微调的生成模型（Mistral 7B）和检索到的文章来生成带有参考文献的声明；（3）一个基于微调的DeBERTa和XLM-RoBERTa模型，在自然语言推理任务中使用SciFACT数据集的验证引擎。验证引擎会交叉检查生成的声明及其来源的文章，以验证在生成声明过程中是否可能存在任何幻觉。通过利用信息检索和RAG模块，Verif.ai在从大量科学来源生成事实信息方面表现出色。同时，验证引擎严格双重检查此输出，确保其准确性和可靠性。这一双阶段过程在获取和确认事实信息方面发挥着至关重要的作用，显著提升了信息环境。我们的方法可以显著提高科学家的生产力，同时促进在科学领域应用生成语言模型的信任，在这些领域中，幻觉和错误信息是不可接受的。</p>
<h4 id="_134">一句话总结：</h4>
<p>VerifAI项目通过结合信息检索、检索增强生成和验证引擎，提供了一种基于事实的科学问答系统，旨在提高科学信息准确性和可靠性。</p>
<hr />
<h2 id="mindful-rag-a-study-of-points-of-failure-in-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.12216v1">Mindful-RAG: A Study of Points of Failure in Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-07-16</p>
<p>作者：Garima Agrawal, Tharindu Kumarage, Zeyad Alghamdi, Huan Liu</p>
<h4 id="_135">中文摘要：</h4>
<p>大型语言模型（LLMs）擅长生成连贯且与上下文相关的文本，但在处理特定领域和事实问答任务中的知识密集型查询时面临挑战。检索增强生成（RAG）系统通过结合外部知识源，如结构化知识图谱（KGs），来缓解这一问题。然而，尽管LLMs可以访问包含必要事实的KG提取信息，但它们往往难以产生准确的答案。我们的研究通过分析现有基于KG的RAG方法的错误模式，并识别出八个关键失败点来探讨这一困境。我们观察到，这些错误主要由于对区分问题的意图和充分从知识图谱事实中收集相关上下文关注不足。基于这一分析，我们提出了Mindful-RAG方法，这是一种旨在基于意图和上下文对齐的知识检索框架。该方法明确针对已识别的失败点，并提高了LLMs提供的答案的正确性和相关性，代表了相对于现有方法的一大进步。</p>
<h4 id="_136">一句话总结：</h4>
<p>本研究提出了一种名为Mindful-RAG的方法，旨在通过提高LLMs在知识密集型问答任务中的正确性和相关性，解决现有基于知识图谱的RAG方法中的关键失败点。</p>
<hr />
<h2 id="evaluation-of-rag-metrics-for-question-answering-in-the-telecom-domain"><a href="http://arxiv.org/abs/2407.12873v1">Evaluation of RAG Metrics for Question Answering in the Telecom Domain</a></h2>
<p>发布时间：2024-07-15</p>
<p>作者：Sujoy Roychowdhury, Sumit Soman, H G Ranjani, Neeraj Gunda, Vansh Chhabra, Sai Krishna Bala</p>
<h4 id="_137">中文摘要：</h4>
<p>检索增强生成（RAG）被广泛用于使大型语言模型（LLMs）在各种领域执行问答（QA）任务。然而，基于开源LLM的RAG在特定领域面临评估生成响应的挑战。文献中一个流行的框架是RAG评估（RAGAS），这是一个公开可用的库，它使用LLMs进行评估。RAGAS的一个缺点是缺乏评估指标数值推导的细节。这项工作的一个成果是对该包的修改版本，用于少数指标（忠实度、上下文相关性、答案相关性、答案正确性、答案相似性和事实正确性），通过它我们使用任何LLMs提供提示的中间输出。接下来，我们分析了修改后的RAGAS包输出的专家评估，并观察到在电信领域使用它的挑战。我们还研究了正确与错误检索下指标的影响，并观察到少数指标在正确检索时具有更高的值。我们还研究了基嵌入和通过预训练和微调领域适应的嵌入之间的指标差异。最后，我们评论了使用这些指标进行野外电信问答任务的适用性和挑战。</p>
<h4 id="_138">一句话总结：</h4>
<p>本研究改进了RAGAS包，分析了其在电信领域的应用挑战，并探讨了不同检索结果下评估指标的变化。</p>
<hr />
<h2 id="think-on-graph-20-deep-and-interpretable-large-language-model-reasoning-with-knowledge-graph-guided-retrieval"><a href="http://arxiv.org/abs/2407.10805v2">Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval</a></h2>
<p>发布时间：2024-07-15</p>
<p>作者：Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo</p>
<h4 id="_139">中文摘要：</h4>
<p>检索增强生成（RAG）通过实现动态信息检索，有效地缓解了大型语言模型（LLMs）在生成内容中存在的知识空白和幻觉问题。然而，这些系统在复杂推理和跨不同查询的一致性方面往往表现不佳。在本研究中，我们提出了Think-on-Graph 2.0，这是一个增强的RAG框架，它将问题与知识图谱（KG）对齐，并利用其作为导航工具，从而深化和细化了RAG范式在信息收集和整合方面的应用。KG引导的导航促进了深度和长距离的关联，以维护逻辑一致性并优化检索范围，以提高精确性和互操作性。同时，通过精确指令引导的语义相似性，可以更好地确保事实一致性。ToG${2.0}$不仅提高了LLMs响应的准确性和可靠性，还展示了混合结构化知识系统在显著提升LLM推理能力方面的潜力，使其更接近人类的表现。我们在四个公开数据集上进行了广泛的实验，以展示我们的方法与基线相比的优势。</p>
<h4 id="_140">一句话总结：</h4>
<p>Think-on-Graph 2.0通过知识图谱导航和语义相似性指导，显著提升了大型语言模型的推理准确性和可靠性。</p>
<hr />
<h2 id="enhancing-retrieval-and-managing-retrieval-a-four-module-synergy-for-improved-quality-and-efficiency-in-rag-systems"><a href="http://arxiv.org/abs/2407.10670v1">Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems</a></h2>
<p>发布时间：2024-07-15</p>
<p>作者：Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, Min Xu</p>
<h4 id="_141">中文摘要：</h4>
<p>检索增强生成（RAG）技术利用大型语言模型（LLMs）的上下文学习能力，以产生更准确和相关的响应。起源于简单的“检索后阅读”方法，RAG框架已经发展成为一个高度灵活和模块化的范例。一个关键组件，查询重写模块，通过生成一个便于搜索的查询来增强知识检索，这种方法使输入问题与知识库更加紧密地对应。我们的研究确定了通过生成多个查询来克服与单个查询相关的信息平台，并通过重写问题来消除歧义，从而阐明潜在意图的机会，以增强查询重写模块到查询重写+。我们还发现，当前的RAG系统存在与无关知识相关的问题；为了克服这一点，我们提出了知识过滤器。这两个模块都基于指令微调的Gemma-2B模型，共同提高了响应质量。最终确定的问题是有冗余的检索；我们引入了记忆知识库和检索触发器来解决这一问题。前者以参数无关的方式支持RAG系统知识库的动态扩展，而后者优化了访问外部知识的成本，从而提高了资源利用和响应效率。这四个RAG模块协同提高RAG系统的响应质量和效率。这些模块的有效性已经通过在六个常见的问答数据集上的实验和消融研究得到验证。源代码可在https://github.com/Ancientshi/ERM4获取。</p>
<h4 id="_142">一句话总结：</h4>
<p>本研究提出了一种基于RAG的问答系统，通过改进查询重写、知识过滤、记忆知识库和检索触发器等模块，显著提升了问答系统的响应质量和效率。</p>
<hr />
<h2 id="think-on-graph-20-deep-and-interpretable-large-language-model-reasoning-with-knowledge-graph-guided-retrieval_1"><a href="http://arxiv.org/abs/2407.10805v3">Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval</a></h2>
<p>发布时间：2024-07-15</p>
<p>作者：Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo</p>
<h4 id="_143">中文摘要：</h4>
<p>检索增强生成（RAG）通过实现动态信息检索，有效地缓解了生成内容中的知识空白和幻觉，从而显著提升了大型语言模型（LLMs）。然而，这些系统在复杂推理和跨不同查询的一致性方面往往表现不佳。在本研究中，我们提出了Think-on-Graph 2.0，这是一个增强的RAG框架，它将问题与知识图谱（KG）对齐，并将其用作导航工具，从而深化和细化了RAG的信息收集和整合范式。KG引导的导航促进了深度和长距离关联，以维护逻辑一致性并优化检索范围，以实现精确性和互操作性。同时，通过精确指令引导的语义相似性，可以更好地确保事实一致性。ToG${2.0}$不仅提高了LLMs响应的准确性和可靠性，还展示了混合结构化知识系统在显著提升LLM推理能力方面的潜力，使其更接近人类的表现。我们在四个公开数据集上进行了广泛的实验，以证明我们的方法与基线相比的优势。</p>
<h4 id="_144">一句话总结：</h4>
<p>Think-on-Graph 2.0通过知识图谱导航增强了检索增强生成，显著提升了大型语言模型的推理准确性和可靠性。</p>
<hr />
<h2 id="gensco-can-question-decomposition-based-passage-alignment-improve-question-answering"><a href="http://arxiv.org/abs/2407.10245v1">GenSco: Can Question Decomposition based Passage Alignment improve Question Answering?</a></h2>
<p>发布时间：2024-07-14</p>
<p>作者：Barah Fazili, Koustava Goswami, Natwar Modani, Inderjeet Nair</p>
<h4 id="_145">中文摘要：</h4>
<p>本文研究了通过在提示中提供经过精心选择的段落序列来提供对齐的上下文，是否能够通过大型语言模型（LLMs）在多跳问答（multi-hop QA）中生成更好的答案。我们引入了“GenSco”，这是一种基于多跳问题的预测分解来选择段落的创新方法。该框架包含两个不同的LLMs：（i）生成器LLM，用于问题分解和最终答案生成；（ii）一个辅助的开源LLM，用作评分器，以语义引导生成器进行段落选择。生成器仅调用一次进行答案生成，从而实现了一种成本效益高且高效的方法。我们在三个广泛使用的多跳问答数据集上进行了评估：2WikiMultiHop、Adversarial HotPotQA和MuSiQue，与各自表现最佳的基线相比，在MuSiQue和2WikiMultiHop上的精确匹配得分分别提高了15.1和5.9分。</p>
<h4 id="_146">一句话总结：</h4>
<p>本文提出了一种基于预测分解选择段落的“GenSco”方法，通过优化上下文提供，显著提升了大型语言模型在多跳问答任务中的答案生成质量。</p>
<hr />
<h2 id="integrating-ai-tutors-in-a-programming-course"><a href="http://arxiv.org/abs/2407.15718v1">Integrating AI Tutors in a Programming Course</a></h2>
<p>发布时间：2024-07-14</p>
<p>作者：Iris Ma, Alberto Krone Martins, Cristina Videira Lopes</p>
<h4 id="_147">中文摘要：</h4>
<p>RAGMan是一个由大型语言模型（LLM）驱动的辅导系统，能够支持多种特定课程和作业的AI辅导师。RAGMan利用检索增强生成（RAG）技术以及严格的指令，确保AI辅导师的回答与问题保持一致。通过使用RAGMan的AI辅导师，学生可以在不直接获得答案的情况下获得特定作业的协助，同时也有能力提出与编程相关的一般性问题。RAGMan作为一个可选资源部署在一门有455名学生报名的入门编程课程中，被配置为五个特定作业的AI辅导师。本文描述了学生与AI辅导师的互动、学生的反馈以及成绩的比较分析。总体而言，大约一半的学生与AI辅导师进行了互动，绝大多数的互动都是合法的作业问题。当学生在预期范围内提出问题时，AI辅导师98%的时间都能提供准确的回答。在使用了AI辅导师的学生中，78%的人表示辅导师帮助他们学习了。除了AI辅导师提供有价值的建议之外，学生还表示他们欣赏辅导师营造了一个没有评判的安全学习环境。</p>
<h4 id="_148">一句话总结：</h4>
<p>RAGMan辅导系统通过使用RAG技术，有效提升了学生在编程课程中的学习体验和作业完成质量。</p>
<hr />
<h2 id="document-level-clinical-entity-and-relation-extraction-via-knowledge-base-guided-generation"><a href="http://arxiv.org/abs/2407.10021v1">Document-level Clinical Entity and Relation Extraction via Knowledge Base-Guided Generation</a></h2>
<p>发布时间：2024-07-13</p>
<p>作者：Kriti Bhattarai, Inez Y. Oh, Zachary B. Abrams, Albert M. Lai</p>
<h4 id="_149">中文摘要：</h4>
<p>生成式预训练转换器（GPT）模型因其精确的提取和上下文理解能力，在临床实体和关系提取任务中展现出巨大潜力。在本研究中，我们进一步利用统一医学语言系统（UMLS）知识库，以准确识别医疗概念并提高文档级别的临床实体和关系提取。我们的框架选择与文本相关的UMLS概念，并将它们与提示结合，以引导语言模型提取实体。我们的实验表明，这种初始概念映射以及将这些映射概念包含在提示中，与不利用UMLS的通用语言模型的少样本提取任务相比，可以改善提取结果。此外，我们的结果表明，这种方法比标准的检索增强生成（RAG）技术更有效，在RAG技术中，检索到的数据与提示嵌入进行比较以生成结果。总体而言，我们发现将UMLS概念与GPT模型相结合，显著提高了实体和关系的识别，优于基线模型和RAG模型。通过结合UMLS等知识方法精确的概念映射能力与GPT的上下文理解能力，我们的方法突显了这些方法在医疗保健等特定领域的潜力。</p>
<h4 id="_150">一句话总结：</h4>
<p>本研究通过结合UMLS知识库与GPT模型，显著提升了临床实体和关系的提取效果，为医疗保健等特定领域的自然语言处理提供了新的解决方案。</p>
<hr />
<h2 id="fine-grained-analysis-of-in-context-linear-estimation-data-architecture-and-beyond"><a href="http://arxiv.org/abs/2407.10005v1">Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond</a></h2>
<p>发布时间：2024-07-13</p>
<p>作者：Yingcong Li, Ankit Singh Rawat, Samet Oymak</p>
<h4 id="_151">中文摘要：</h4>
<p>最近的研究表明，通过梯度下降步骤实现线性估计器，具有线性注意力的Transformer能够进行情境学习（ICL）。然而，现有关于优化景观的结果是在假设任务和特征向量是独立同分布（IID）以及注意力权重完全参数化的风格化设置下得出的。在这项工作中，我们通过对架构、低秩参数化和相关设计方面的贡献，对ICL的优化和泛化景观进行了更深入的分析：（1）我们研究了单层线性注意力和单层H3（一个状态空间模型）的景观。在合适的关联设计假设下，我们证明了它们都能实现一步预条件梯度下降。我们展示了H3由于其本地的卷积滤波器，在适当的设置下还具有实现样本加权和优于线性注意力的优势。（2）通过研究关联设计，我们为检索增强生成（RAG）和任务-特征对齐提供了新的风险界限，揭示了ICL样本复杂度如何从分布对齐中受益。（3）我们通过协方差谱推导了低秩参数化注意力权重的最优风险。通过这种方式，我们也阐明了LoRA如何通过捕捉任务协方差之间的变化来适应新的分布。实验结果证实了我们的理论发现。总的来说，这项工作在具有实际意义的设置中探索了ICL的优化和风险景观，并有助于更深入地理解其机制。</p>
<h4 id="_152">一句话总结：</h4>
<p>本研究通过分析架构、低秩参数化和相关设计，深入探讨了情境学习（ICL）的优化和风险景观，为理解其机制提供了新的见解。</p>
<hr />
<h2 id="mitigating-entity-level-hallucination-in-large-language-models"><a href="http://arxiv.org/abs/2407.09417v2">Mitigating Entity-Level Hallucination in Large Language Models</a></h2>
<p>发布时间：2024-07-12</p>
<p>作者：Weihang Su, Yichen Tang, Qingyao Ai, Changyue Wang, Zhijing Wu, Yiqun Liu</p>
<p>：</p>
<h4 id="_153">中文摘要：</h4>
<p>大型语言模型（LLMs）的兴起改变了用户获取信息的方式，从传统的搜索引擎转向与LLMs的直接问答交互。然而，LLMs的广泛应用揭示了一个被称为幻觉的重大挑战，即LLMs生成连贯但事实不准确的结果。这种幻觉现象导致了用户对基于LLMs的信息检索系统的信任度下降。为了应对这一挑战，本文提出了一种基于幻觉检测的动态检索增强（DRAD）作为检测和减轻LLMs中幻觉现象的新方法。DRAD通过根据实时幻觉检测动态调整检索过程来改进传统的检索增强。它包含两个主要组件：实时幻觉检测（RHD），用于在不使用外部模型的情况下识别潜在的幻觉，以及基于外部知识的自我校正（SEK），用于使用外部知识纠正这些错误。实验结果表明，DRAD在检测和减轻LLMs中的幻觉方面表现出优异的性能。我们所有的代码和数据都已在https://github.com/oneal2000/EntityHallucination上开源。</p>
<h4 id="_154">一句话总结：</h4>
<p>本文提出了一种基于幻觉检测的动态检索增强方法，有效检测和减轻大型语言模型中的幻觉现象。</p>
<hr />
<h2 id="context-embeddings-for-efficient-answer-generation-in-rag"><a href="http://arxiv.org/abs/2407.09252v2">Context Embeddings for Efficient Answer Generation in RAG</a></h2>
<p>发布时间：2024-07-12</p>
<p>作者：David Rau, Shuai Wang, Hervé Déjean, Stéphane Clinchant</p>
<h4 id="_155">中文摘要：</h4>
<p>检索增强生成（RAG）通过扩展输入以外部信息克服了大型语言模型（LLMs）的知识局限性。因此，模型接收到的上下文输入变得非常长，这直接导致解码时间变长，从而增加了用户等待答案的时间。为了解决这一挑战，我们提出了COCOM，一种有效的上下文压缩方法，将长上下文压缩成仅几个上下文嵌入，大幅加快了生成时间。我们的方法允许以不同的压缩率进行权衡，以解码时间换取答案质量。与早期方法相比，COCOM能够更有效地处理多个上下文，显著减少了长输入的解码时间。我们的方法在性能上优于现有的高效上下文压缩方法，实现了高达5.69倍的加速。</p>
<h4 id="_156">一句话总结：</h4>
<p>COCOM是一种有效的上下文压缩方法，通过将长上下文压缩成少量嵌入来加速检索增强生成过程，同时保持较高的答案质量。</p>
<hr />
<h2 id="compact-compressing-retrieved-documents-actively-for-question-answering"><a href="http://arxiv.org/abs/2407.09014v2">CompAct: Compressing Retrieved Documents Actively for Question Answering</a></h2>
<p>发布时间：2024-07-12</p>
<p>作者：Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, Jaewoo Kang</p>
<h4 id="_157">中文摘要：</h4>
<p>检索增强生成通过提供外部上下文来支持语言模型加强其事实基础。然而，当语言模型面对大量信息时，它们常常面临挑战，这降低了它们解决问题的有效性。上下文压缩通过过滤掉无关信息来解决这个问题，但当前的方法在无法通过单步方法捕捉关键信息的现实场景中仍然存在困难。为了克服这一限制，我们引入了CompAct，这是一个新颖的框架，它采用主动策略来压缩大量文档而不丢失关键信息。我们的实验表明，CompAct在多跳问答（QA）基准测试中在性能和压缩率方面都取得了显著的提升。CompAct作为一个灵活的成本效益插件模块，可以与各种现成的检索器或阅读器一起使用，实现了异常高的压缩率（47倍）。</p>
<h4 id="_158">一句话总结：</h4>
<p>CompAct通过主动策略压缩大量文档，显著提升了多跳问答任务中的性能和压缩率。</p>
<hr />
<h2 id="personarag-enhancing-retrieval-augmented-generation-systems-with-user-centric-agents"><a href="http://arxiv.org/abs/2407.09394v1">PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents</a></h2>
<p>发布时间：2024-07-12</p>
<p>作者：Saber Zerhoudi, Michael Granitzer</p>
<h4 id="_159">中文摘要：</h4>
<p>大型语言模型（LLMs）由于知识过时和幻觉问题，在生成可靠输出方面存在困难。检索增强生成（RAG）模型通过增强LLMs的外部知识来解决这一问题，但往往未能个性化检索过程。本文提出了一种名为PersonaRAG的新框架，该框架通过整合以用户为中心的代理，根据实时用户数据和交互来调整检索和生成。在多个问答数据集上进行了评估，PersonaRAG显示出相较于基线模型的优越性，能够为用户需求提供定制化的答案。结果表明，为用户适应的信息检索系统提供了有前景的发展方向。</p>
<h4 id="_160">一句话总结：</h4>
<p>PersonaRAG通过结合用户实时数据和交互，为大型语言模型提供个性化检索和生成，从而在问答任务中优于基线模型。</p>
<hr />
<h2 id="investigating-llms-as-voting-assistants-via-contextual-augmentation-a-case-study-on-the-european-parliament-elections-2024"><a href="http://arxiv.org/abs/2407.08495v1">Investigating LLMs as Voting Assistants via Contextual Augmentation: A Case Study on the European Parliament Elections 2024</a></h2>
<p>发布时间：2024-07-11</p>
<p>作者：Ilias Chalkidis</p>
<h4 id="_161">中文摘要：</h4>
<p>大型语言模型在经过指令微调后展现出前所未有的自然语言理解能力。近期的研究主要探讨了LLMs中的政治偏见和政治推理能力，主要聚焦于美国背景。鉴于2024年欧洲议会选举的近期发生，我们研究了LLMs是否可以作为投票建议应用（VAAs）。我们对MISTRAL和MIXTRAL模型进行了审计，并评估了它们根据最新的“欧盟与我国”投票辅助问卷预测政党立场的准确性。此外，我们通过检索增强生成（RAG）和基于网络搜索的输入上下文增强，以及通过分阶段对话进行自我反思的方法，探索了提高模型性能的替代方案，旨在从模型的内部记忆中重新收集相关内容。我们发现MIXTRAL的平均准确率高达82%。通过添加专家编纂的信息，可以提高模型性能约9%，这仍然是自动化方法的一个开放挑战。</p>
<h4 id="_162">一句话总结：</h4>
<p>本研究评估了LLMs作为投票建议应用在预测政党立场方面的准确性，并探索了通过增强输入上下文和自我反思来提高模型性能的方法。</p>
<hr />
<h2 id="lynx-an-open-source-hallucination-evaluation-model"><a href="http://arxiv.org/abs/2407.08488v2">Lynx: An Open Source Hallucination Evaluation Model</a></h2>
<p>发布时间：2024-07-11</p>
<p>作者：Selvan Sunitha Ravi, Bartosz Mielczarek, Anand Kannappan, Douwe Kiela, Rebecca Qian</p>
<h4 id="_163">中文摘要：</h4>
<p>LYNX是一种先进的幻觉检测大型语言模型（LLM），旨在减轻LLM在生成文本时产生的幻觉。然而，LLM仍然可能生成与检索到的上下文不符或相互矛盾的信息。为了评估LYNX，我们提出了HaluBench，这是一个全面的幻觉评估基准，包含来自各种现实世界领域的15k个样本。我们的实验结果表明，LYNX在HaluBench上优于GPT-4o、Claude-3-Sonnet以及封闭和开源的LLM作为裁判的模型。我们发布了LYNX、HaluBench以及我们的评估代码，供公众访问。</p>
<h4 id="_164">一句话总结：</h4>
<p>LYNX是一种能够有效检测大型语言模型幻觉的新技术，通过HaluBench基准测试展现出优于现有模型的表现。</p>
<hr />
<h2 id="speculative-rag-enhancing-retrieval-augmented-generation-through-drafting"><a href="http://arxiv.org/abs/2407.08223v1">Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting</a></h2>
<p>发布时间：2024-07-11</p>
<p>作者：Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, Chen-Yu Lee, Tomas Pfister</p>
<h4 id="_165">中文摘要：</h4>
<p>检索增强生成（RAG）结合了大型语言模型（LLMs）的生成能力和外部知识源，以提供更准确和更新的响应。最近的RAG进展集中在通过迭代LLMs的细化或通过额外的指令调整获得的自我批评能力来改善检索结果。在这项工作中，我们引入了推测性RAG——一个利用更大通用LLM来高效验证由较小、蒸馏的专业LLM并行产生的多个RAG草稿的框架。每个草稿都是从检索到的文档的不同子集中生成的，提供了对证据的多样化视角，同时减少了每个草稿的输入令牌计数。这种方法增强了每个子集的理解，并减轻了潜在的长上下文位置偏差。我们的方法通过将起草工作委托给较小的专业LLM来加速RAG，而较大的通用LLM则对草稿执行单次验证过程。广泛的实验表明，Speculative RAG在TriviaQA、MuSiQue、PubHealth和ARC-Challenge基准测试中实现了最先进的性能，同时降低了延迟。与传统的RAG系统相比，它在PubHealth上显著提高了准确率高达12.97%，同时将延迟降低了51%。</p>
<h4 id="_166">一句话总结：</h4>
<p>Speculative RAG通过利用大语言模型验证小语言模型生成的多个草稿，实现了在多个基准测试中提高RAG准确率并降低延迟的效果。</p>
<hr />
<h2 id="ninjallm-fast-scalable-and-cost-effective-rag-using-amazon-sagemaker-and-aws-trainium-and-inferentia2"><a href="http://arxiv.org/abs/2407.12057v1">NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker and AWS Trainium and Inferentia2</a></h2>
<p>发布时间：2024-07-11</p>
<p>作者：Tengfei Xue, Xuefeng Li, Roman Smirnov, Tahir Azim, Arash Sadrieh, Babak Pahlavan</p>
<h4 id="_167">中文摘要：</h4>
<p>检索增强生成（RAG）技术在当今被广泛用于以对话形式检索和呈现信息。本文提出了一系列对传统RAG技术的改进，重点关注在AWS Trainium和Inferentia2 AI芯片上微调和托管的大型语言模型（LLMs），这些芯片以其弹性、成本效益和高效的人工智能计算任务性能而著称。除了在这些芯片上实现部署外，这项工作旨在提高工具的使用效率、添加引用功能，并减轻由于上下文偏差导致的幻觉和危险响应的风险。我们在Natural Questions和HotPotQA数据集上对RAG系统的性能进行了基准测试，分别达到了62%和59%的准确率，超过了DBRX和Mixtral Instruct等模型。</p>
<h4 id="_168">一句话总结：</h4>
<p>本文提出了一种基于AWS Trainium和Inferentia2芯片的RAG技术改进方案，有效提升了信息检索和生成的准确性和安全性。</p>
<hr />
<h2 id="beyond-benchmarks-evaluating-embedding-model-similarity-for-retrieval-augmented-generation-systems"><a href="http://arxiv.org/abs/2407.08275v1">Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems</a></h2>
<p>发布时间：2024-07-11</p>
<p>作者：Laura Caspari, Kanishka Ghosh Dastidar, Saber Zerhoudi, Jelena Mitrovic, Michael Granitzer</p>
<h4 id="_169">中文摘要：</h4>
<p>在检索增强生成（RAG）系统的设计中，嵌入模型的选取是一个关键步骤。鉴于可供选择的模型数量庞大，识别相似模型的集群可以简化这一模型选择过程。仅仅依赖基准性能分数只能对模型相似性进行弱评估。因此，在本研究中，我们评估了RAG系统中嵌入模型的相似性。我们的评估分为两个方面：我们使用中心核对齐（Centered Kernel Alignment）在成对水平上比较嵌入。此外，鉴于其对RAG系统的特别相关性，我们还使用Jaccard和排名相似性来评估这些模型之间的检索结果相似性。我们比较了包括专有模型在内的不同嵌入模型家族，跨越了来自流行基准信息检索（BEIR）的五个数据集。通过我们的实验，我们确定了对应于模型家族的模型集群，但有趣的是，还有一些跨家族的集群。此外，我们对top-k检索相似性的分析揭示了在低k值时的高变异性。我们还确定了专有模型的可能开源替代品，其中Mistral与OpenAI模型的相似性最高。</p>
<h4 id="_170">一句话总结：</h4>
<p>本研究通过比较嵌入模型和检索结果相似性，评估了RAG系统中不同嵌入模型的相似性，并发现了跨家族的模型集群。</p>
<hr />
<h2 id="facts-about-building-retrieval-augmented-generation-based-chatbots"><a href="http://arxiv.org/abs/2407.07858v1">FACTS About Building Retrieval Augmented Generation-based Chatbots</a></h2>
<p>发布时间：2024-07-10</p>
<p>作者：Rama Akkiraju, Anbang Xu, Deepak Bora, Tan Yu, Lu An, Vishal Seth, Aaditya Shukla, Pritam Gundecha, Hridhay Mehta, Ashwin Jha, Prithvi Raj, Abhinav Balasubramanian, Murali Maram, Guru Muthusamy, Shivakesh Reddy Annepally, Sidney Knowles, Min Du, Nick Burnett, Sean Javiya, Ashok Marannan, Mamta Kumari, Surbhi Jha, Ethan Dereszenski, Anupam Chakraborty, Subhash Ranjan, Amina Terfai, Anoop Surya, Tracey Mercer, Vinodh Kumar Thanigachalam, Tamar Bar, Sanjana Krishnan, Samy Kilaru, Jasmine Jaksic, Nave Algarici, Jacob Liberman, Joey Conway, Sonu Nayyar, Justin Boitano</p>
<h4 id="_171">中文摘要：</h4>
<p>企业聊天机器人，由生成式人工智能驱动，正成为提高员工生产力的关键应用。检索增强生成（RAG）、大型语言模型（LLMs）以及Langchain和Llamaindex等编排框架对于构建这些聊天机器人至关重要。然而，创建有效的企业聊天机器人具有挑战性，需要细致的RAG管道工程。这包括微调嵌入和LLMs、从矢量数据库中提取文档、重述查询、重新排序结果、设计提示、遵守文档访问控制、提供简洁的响应、包括参考文献、保护个人信息以及构建编排代理。我们基于与三个NVIDIA聊天机器人的经验（用于IT/HR福利、财务收益和一般内容）提出了一种构建基于RAG的聊天机器人的框架。我们的贡献有三点：引入了FACTS框架（Freshness，Architectures，Cost，Testing，Security），提出了十五个RAG管道控制点，并提供了关于大型和小型LLMs之间准确度-延迟权衡的实证结果。据我们所知，这是第一篇提供构建安全企业级聊天机器人因素及解决方案的整体视角的论文。</p>
<h4 id="_172">一句话总结：</h4>
<p>本文提出了一种构建安全企业级聊天机器人的框架，通过引入FACTS框架和实证结果，为提高员工生产力的关键应用提供了全面解决方案。</p>
<hr />
<h2 id="flooding-spread-of-manipulated-knowledge-in-llm-based-multi-agent-communities"><a href="http://arxiv.org/abs/2407.07791v2">Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities</a></h2>
<p>发布时间：2024-07-10</p>
<p>作者：Tianjie Ju, Yiting Wang, Xinbei Ma, Pengzhou Cheng, Haodong Zhao, Yulong Wang, Lifeng Liu, Jian Xie, Zhuosheng Zhang, Gongshen Liu</p>
<h4 id="_173">中文摘要：</h4>
<p>本文研究了基于大型语言模型（LLMs）的多智能体系统在处理世界知识时存在的固有漏洞，这些漏洞可能被攻击者利用来无意识地传播虚假信息。通过构建详细的威胁模型和模拟环境，本文提出了一种新颖的两阶段攻击方法，包括说服性注入和操纵性知识注入，以系统地探索在无需显式提示操纵的情况下，操纵性知识（即反事实和有害知识）传播的可能性。实验结果表明，该攻击方法能够成功诱导基于LLMs的智能体在通信过程中传播反事实和有害知识，而不会降低其基本能力。此外，研究还发现，这些操纵可以通过流行的检索增强生成框架持续存在，其中一些良性智能体存储和检索被操纵的聊天历史以供未来交互使用。这一持续性表明，即使在交互结束后，良性智能体也可能继续受到操纵性知识的影响。本文揭示了基于LLMs的多智能体系统中的重大安全风险，强调了对抗操纵性知识传播的强大防御措施的重要性，例如引入“守护者”智能体和高级事实核查工具。</p>
<h4 id="_174">一句话总结：</h4>
<p>本文揭示了基于大型语言模型的多智能体系统中操纵性知识传播的安全风险，并提出了相应的防御策略。</p>
<hr />
<h2 id="internet-of-agents-weaving-a-web-of-heterogeneous-agents-for-collaborative-intelligence"><a href="http://arxiv.org/abs/2407.07061v2">Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</a></h2>
<p>发布时间：2024-07-09</p>
<p>作者：Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun</p>
<h4 id="_175">中文摘要：</h4>
<p>随着大型语言模型（LLMs）的快速发展，为高度智能的自主代理的发展铺平了道路。然而，现有的多代理框架往往难以整合多样化的第三方代理，因为它们依赖于自身生态系统中定义的代理。此外，由于大多数框架仅限于单设备设置，这些框架在模拟分布式环境方面也面临挑战。受互联网概念的启发，我们提出了代理互联网（IoA），这是一个新型框架，通过提供一个灵活且可扩展的平台，以支持基于LLM的多代理协作来解决这些限制。IoA引入了一种代理集成协议、类似即时通讯的架构设计，以及用于代理团队协作和对话流程控制的动态机制。通过在通用助手任务、具身人工智能任务和检索增强生成基准上的广泛实验，我们证明了IoA在一致性上优于最先进的基线，展示了其促进异构代理有效协作的能力。IoA代表了在类似互联网的环境中连接各种代理的一步，其中代理可以无缝协作以实现更高的智能和能力。我们的代码库已发布在\url{https://github.com/OpenBMB/IoA}。</p>
<h4 id="_176">一句话总结：</h4>
<p>IoA通过提供一个灵活且可扩展的平台，实现了基于LLM的多代理协作，从而促进了异构代理之间的有效协作。</p>
<hr />
<h2 id="peer-expertizing-domain-specific-tasks-with-a-multi-agent-framework-and-tuning-methods"><a href="http://arxiv.org/abs/2407.06985v2">PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods</a></h2>
<p>发布时间：2024-07-09</p>
<p>作者：Yiying Wang, Xiaojing Li, Binzhu Wang, Yueyang Zhou, Han Ji, Hong Chen, Jinshi Zhang, Fei Yu, Zewei Zhao, Song Jin, Renji Gong, Wanqing Xu</p>
<h4 id="_177">中文摘要：</h4>
<p>在特定领域的应用中，GPT-4（通过精确提示或检索增强生成）展现出显著潜力，但面临着性能、成本和数据隐私的三大关键问题。高性能需要复杂的处理技术，然而在复杂的工作流程中管理多个代理通常既昂贵又具有挑战性。为了解决这个问题，我们引入了PEER（计划、执行、表达、审查）多代理框架。该框架通过整合精确的问题分解、高级的信息检索、全面的总结和严格的自我评估，系统化了特定领域的任务。鉴于成本和数据隐私的担忧，企业正从GPT-4等专有模型转向定制模型，在成本、安全和性能之间寻求平衡。我们开发了利用在线数据和用户反馈进行高效模型调优的工业实践。本研究为在特定领域问题解决中应用多代理系统以及实施有效的代理调优策略提供了最佳实践指南。我们的实证研究，特别是在金融问答领域，表明我们的方法达到了GPT-4的95.0%的性能，同时有效地管理成本并确保数据隐私。</p>
<h4 id="_178">一句话总结：</h4>
<p>本研究提出了一种基于PEER框架的多代理系统，在确保数据隐私的同时，实现了与GPT-4相当的性能，并有效管理了成本。</p>
<hr />
<h2 id="a-simple-architecture-for-enterprise-large-language-model-applications-based-on-role-based-security-and-clearance-levels-using-retrieval-augmented-generation-or-mixture-of-experts"><a href="http://arxiv.org/abs/2407.06718v1">A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts</a></h2>
<p>发布时间：2024-07-09</p>
<p>作者：Atilla Özgür, Yılmaz Uygun</p>
<h4 id="_179">中文摘要：</h4>
<p>本研究提出了一种用于大型语言模型（LLMs）的企业应用架构，旨在解决基于角色的安全和北约清级问题。该提案旨在解决当前LLMs在处理安全和信息访问方面的局限性。所提出的架构可以在使用检索增强生成（RAG）和专家混合模型（MoE）微调的同时使用。它可以仅与RAG一起使用，或仅与MoE一起使用，或同时使用两者。通过使用用户的角色和安全性清级，对RAG中的文档和MoE中的专家进行筛选，从而防止信息泄露。</p>
<h4 id="_180">一句话总结：</h4>
<p>本研究提出了一种基于角色和北约清级的大型语言模型企业应用架构，通过RAG和MoE模型微调，实现安全信息访问控制，防止信息泄露。</p>
<hr />
<h2 id="vimi-grounding-video-generation-through-multi-modal-instruction"><a href="http://arxiv.org/abs/2407.06304v1">VIMI: Grounding Video Generation through Multi-modal Instruction</a></h2>
<p>发布时间：2024-07-08</p>
<p>作者：Yuwei Fang, Willi Menapace, Aliaksandr Siarohin, Tsai-Shien Chen, Kuan-Chien Wang, Ivan Skorokhodov, Graham Neubig, Sergey Tulyakov</p>
<h4 id="_181">中文摘要：</h4>
<p>现有的文本到视频扩散模型仅依赖于纯文本编码器进行预训练。这种限制源于缺乏大规模的多模态提示视频数据集，导致视觉基础知识的缺乏，限制了其在多模态集成中的多样性和应用。为了解决这个问题，我们通过检索方法将上下文示例与给定的文本提示配对，构建了一个大规模的多模态提示数据集，并采用两阶段训练策略，使同一模型能够执行多种视频生成任务。在第一阶段，我们提出一个多模态条件视频生成框架，用于在这些增强数据集上进行预训练，建立一个基于视觉基础的视频生成的基础模型。其次，我们在三个视频生成任务上对第一阶段训练的模型进行微调，并融入多模态指令。这一过程进一步提升了模型处理多样输入和任务的能力，确保了多模态信息的无缝集成。经过这两阶段训练后，VIMI展示了多模态理解能力，如图1所示，能够根据提供的输入生成上下文丰富且个性化的视频。与之前基于视觉基础的视频生成方法相比，VIMI能够在保持语义控制的同时，合成一致且时间上连贯的视频，并具有大范围的动态变化。最后，VIMI在UCF101基准测试中也实现了最先进的文本到视频生成结果。</p>
<h4 id="_182">一句话总结：</h4>
<p>VIMI通过构建大规模多模态提示数据集和两阶段训练策略，实现了基于文本的视觉基础视频生成，并在多个任务上取得了最先进的性能。</p>
<hr />
<h2 id="oran-bench-13k-an-open-source-benchmark-for-assessing-llms-in-open-radio-access-networks"><a href="http://arxiv.org/abs/2407.06245v2">ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open Radio Access Networks</a></h2>
<p>发布时间：2024-07-08</p>
<p>作者：Pranshav Gajjar, Vijay K. Shah</p>
<h4 id="_183">中文摘要：</h4>
<p>大型语言模型（LLMs）可以通过增强网络分析、异常检测和代码生成，显著提高开放无线接入网络（O-RAN）各项任务的效率和可靠性，从而革新我们部署和运营O-RAN的方式。本文介绍了ORAN-Bench-13K，这是第一个旨在评估O-RAN环境中大型语言模型（LLMs）性能的全面基准。我们的基准包括从116份O-RAN规范文档中精心制作的13,952个多项选择题。我们利用一个新颖的三阶段LLM框架，并将问题分为三个不同难度级别，以覆盖广泛的ORAN相关知识。我们全面评估了包括Gemini、Chat-GPT和Mistral在内的几个最先进的LLMs的性能。此外，我们提出了基于检索增强生成（RAG）的ORANSight管道，它在ORAN-Bench-13K上的表现优于其他测试的闭源模型。我们的发现表明，当前流行的LLM模型在O-RAN方面并不擅长，突显了专用模型的需求。我们观察到，当引入基于RAG的ORANSight管道时，性能有显著提升，宏观准确率为0.784，加权准确率为0.776，平均比其他测试的LLMs高出21.55%和22.59%。</p>
<h4 id="_184">一句话总结：</h4>
<p>本文提出了一种基于RAG的ORANSight管道，显著提升了大型语言模型在O-RAN基准测试中的性能，揭示了当前LLMs在O-RAN领域的不足，并强调了专用模型的重要性。</p>
<hr />
<h2 id="faux-polyglot-a-study-on-information-disparity-in-multilingual-large-language-models"><a href="http://arxiv.org/abs/2407.05502v2">Faux Polyglot: A Study on Information Disparity in Multilingual Large Language Models</a></h2>
<p>发布时间：2024-07-07</p>
<p>作者：Nikhil Sharma, Kenton Murray, Ziang Xiao</p>
<h4 id="_185">中文摘要：</h4>
<p>利用检索增强生成（RAG）技术，大型语言模型（LLMs）在信息搜索中扮演着关键角色，并被全球范围内采用。尽管LLMs的多语言能力为跨越语言障碍提供了新的机遇，但这些能力是否能在现实生活中转化为解决多语言来源之间的语言差异和知识冲突的实际场景？在本文中，我们研究了LLMs在基于RAG的信息搜索设置中的语言偏好。我们发现，在信息检索和答案生成过程中，LLMs都表现出对与查询语言相同语言的信息的系统偏见。此外，在查询语言信息较少的情况下，LLMs倾向于选择高资源语言的文档，从而强化了主导观点。这种偏见存在于事实性和基于观点的查询中。我们的结果突显了多语言LLMs在信息搜索系统中的语言差异。LLMs看似有益的多语言能力可能会通过强化语言特定的信息茧房或过滤气泡，进一步边缘化低资源观点，从而对信息平等产生反效果。</p>
<h4 id="_186">一句话总结：</h4>
<p>本文揭示了大型语言模型在信息搜索中存在的语言偏好问题，指出其多语言能力可能加剧信息不平等。</p>
<hr />
<h2 id="rule-reliable-multimodal-rag-for-factuality-in-medical-vision-language-models"><a href="http://arxiv.org/abs/2407.05131v1">RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models</a></h2>
<p>发布时间：2024-07-06</p>
<p>作者：Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, Huaxiu Yao</p>
<h4 id="_187">中文摘要：</h4>
<p>近年来，医学大视觉语言模型（Med-LVLMs）的出现提升了医学诊断的效率。然而，当前的Med-LVLMs常常遇到事实性问题，经常生成与既定医学事实不符的回应。检索增强生成（RAG）通过利用外部知识可以提高这些模型的事实准确性，但同时也引入了两个主要挑战。首先，有限的检索上下文可能无法涵盖所有必要信息，而过度的检索可能会引入无关和不准确的信息，干扰模型的生成。其次，在模型原本回答正确的情况下，应用RAG可能导致过度依赖检索上下文，从而得出错误答案。为了解决这些问题，我们提出了RULE，它由两个组件组成。首先，我们引入了一种通过校准选择检索上下文数量来控制事实风险的有效策略。其次，基于那些因过度依赖检索上下文而导致错误的样本，我们精心制作了一个偏好数据集以微调模型，平衡其在生成过程中对固有知识和检索上下文的依赖。我们在三个医学视觉问答（VQA）数据集上展示了RULE的有效性，实现了事实准确性的平均提升20.8%。我们已在https://github.com/richard-peng-xia/RULE上公开我们的基准和代码。</p>
<h4 id="_188">一句话总结：</h4>
<p>RULE通过校准检索上下文数量和微调模型依赖，有效提升了医学大视觉语言模型的事实准确性。</p>
<hr />
<h2 id="how-do-you-know-that-teaching-generative-language-models-to-reference-answers-to-biomedical-questions"><a href="http://arxiv.org/abs/2407.05015v1">How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions</a></h2>
<p>发布时间：2024-07-06</p>
<p>作者：Bojana Bašaragin, Adela Ljajić, Darija Medvecki, Lorenzo Cassano, Miloš Košprdić, Nikola Milošević</p>
<h4 id="_189">中文摘要：</h4>
<p>大型语言模型（LLMs）最近已成为用户在线提问的主要答案来源。尽管它们能够提供流畅的回答，但它们的准确性和可靠性可能带来重大挑战。这在敏感领域如生物医药中尤为明显，因为这些领域对事实性回答的需求更高。本文介绍了一种生物医药检索增强生成（RAG）系统，旨在提高生成回答的可靠性。该系统基于一个针对参考问答进行微调的LLM，通过提示将从PubMed检索到的相关摘要作为输入传递给LLM的上下文。其输出是基于PubMed摘要的回答，其中每个陈述都有相应的参考文献，使用户能够验证答案。我们的检索系统与PubMed搜索引擎相比，绝对改进了23%。基于对一小部分样本的手动评估，我们的微调LLM组件在引用相关摘要方面取得了与GPT-4 Turbo相当的结果。我们将用于微调模型的dataset以及基于Mistral-7B-instruct-v0.1和v0.2的微调模型公开。</p>
<h4 id="_190">一句话总结：</h4>
<p>本文提出了一种基于LLM的生物医药RAG系统，通过PubMed摘要检索增强生成回答的可靠性，显著提升了答案的准确性和可验证性。</p>
<hr />
<h2 id="vortex-under-ripplet-an-empirical-study-of-rag-enabled-applications"><a href="http://arxiv.org/abs/2407.05138v1">Vortex under Ripplet: An Empirical Study of RAG-enabled Applications</a></h2>
<p>发布时间：2024-07-06</p>
<p>作者：Yuchen Shao, Yuheng Huang, Jiawei Shen, Lei Ma, Ting Su, Chengcheng Wan</p>
<h4 id="_191">中文摘要：</h4>
<p>大型语言模型（LLMs）通过检索增强生成（RAG）得到了增强，在多种应用场景中提供了有效的解决方案。然而，由于缺乏接口规范、软件环境的要求以及复杂的系统管理，开发者将RAG增强的LLMs集成到软件系统中面临挑战。在本文中，我们手动研究了100个包含RAG增强LLMs的开源应用程序及其问题报告。我们发现，超过98%的应用程序包含多个影响软件功能、效率和安全的集成缺陷。我们还归纳了19种缺陷模式并提出了应对这些缺陷的指南。我们希望这项工作能够帮助LLM驱动的软件开发并激励未来的研究。</p>
<h4 id="_192">一句话总结：</h4>
<p>本文通过分析开源应用程序，揭示了RAG增强LLMs集成中的缺陷模式，并提出了相应的解决指南。</p>
<hr />
<h2 id="ramo-retrieval-augmented-generation-for-enhancing-moocs-recommendations"><a href="http://arxiv.org/abs/2407.04925v1">RAMO: Retrieval-Augmented Generation for Enhancing MOOCs Recommendations</a></h2>
<p>发布时间：2024-07-06</p>
<p>作者：Jiarui Rao, Jionghao Lin</p>
<h4 id="_193">中文摘要：</h4>
<p>大规模开放在线课程（MOOCs）通过提供多样化的课程，打破了与地理、财务和时间相关的传统障碍，显著提高了教育的可及性。然而，学生往往难以在众多课程中进行选择，尤其是在探索新的研究领域时。受到这一挑战的驱动，研究人员一直在探索课程推荐系统，以提供符合个人学习偏好和职业抱负的定制化指导。这些系统在有效解决新用户的“冷启动”问题方面面临特别挑战。推荐系统领域的最新进展表明，将大型语言模型（LLMs）整合到推荐过程中可以增强个性化推荐并解决“冷启动”问题。受这些进展的启发，我们的研究引入了RAMO（针对MOOCs的检索增强生成），这是一个专门设计来克服传统课程推荐系统“冷启动”挑战的系统。RAMO系统利用LLMs的能力，以及通过检索增强生成（RAG）促进的上下文理解，通过对话界面提供课程推荐，旨在提升在线学习体验。</p>
<h4 id="_194">一句话总结：</h4>
<p>RAMO系统通过整合大型语言模型和检索增强生成技术，旨在克服MOOCs推荐系统中的“冷启动”问题，为用户提供个性化的课程推荐。</p>
<hr />
<h2 id="rethinking-visual-prompting-for-multimodal-large-language-models-with-external-knowledge"><a href="http://arxiv.org/abs/2407.04681v1">Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge</a></h2>
<p>发布时间：2024-07-05</p>
<p>作者：Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, Lu Yuan</p>
<h4 id="_195">中文摘要：</h4>
<p>近年来，多模态大型语言模型（MLLMs）通过在大量高质量的图像-文本数据集上进行训练，取得了显著的进步，使它们能够较好地理解图像。然而，在文本中明确传达细粒度或空间密集信息（如掩码）的固有困难，对MLLMs构成了挑战，限制了它们回答需要理解详细或局部视觉元素的问题的能力。受检索增强生成（RAG）概念的启发，本文提出了一种新的视觉提示方法，将来自专业视觉模型（例如，实例分割/OCR模型）获取的细粒度外部知识整合到MLLMs中。这是增强MLLMs性能的一个有希望但尚未充分探索的方向。我们的方法与现有工作不同，后者将外部知识转化为额外的文本提示，需要模型间接学习视觉内容和文本坐标之间的对应关系。相反，我们提出将细粒度知识信息直接嵌入到空间嵌入图中作为视觉提示。这种设计可以轻松地集成到各种MLLMs中，如LLaVA和Mipha，显著提高它们的视觉理解性能。通过严格的实验，我们证明了我们的方法可以增强MLLMs在九个基准上的性能，增强了它们的细粒度上下文感知能力。</p>
<h4 id="_196">一句话总结：</h4>
<p>本文提出了一种将细粒度外部知识直接嵌入到空间嵌入图中作为视觉提示的方法，以增强MLLMs的视觉理解能力。</p>
<hr />
<h2 id="gpt-vs-retro-exploring-the-intersection-of-retrieval-and-parameter-efficient-fine-tuning"><a href="http://arxiv.org/abs/2407.04528v1">GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning</a></h2>
<p>发布时间：2024-07-05</p>
<p>作者：Aleksander Ficek, Jiaqi Zeng, Oleksii Kuchaiev</p>
<h4 id="_197">中文摘要：</h4>
<p>参数高效微调（PEFT）和检索增强生成（RAG）已成为在最小化计算需求的同时适应大型语言模型的方法。在本文中，我们将PEFT方法（P-tuning、Adapters和LoRA）应用于修改后的检索增强Transformer（RETRO）和基线GPT模型，这些模型的大小从8.23亿参数到480亿参数不等。我们发现，由于独特的预训练过程，RETRO模型在零样本设置中优于GPT模型，但GPT模型在PEFT下具有更高的性能潜力。此外，我们的研究指出，80亿参数模型在成本和性能之间达到了最佳平衡，而P-tuning在PEFT技术中表现稍逊。我们进一步提供了将PEFT应用于指令调整的RETRO模型和基线RETRO模型之间的比较分析。这项工作展示了将各种PEFT方法与RAG相结合的首次全面比较，应用于GPT和RETRO模型，突出了它们的相对性能。</p>
<h4 id="_198">一句话总结：</h4>
<p>本文通过对比分析PEFT方法在GPT和RETRO模型中的应用，揭示了不同方法在检索增强生成任务中的性能差异。</p>
<hr />
<h2 id="eventchat-implementation-and-user-centric-evaluation-of-a-large-language-model-driven-conversational-recommender-system-for-exploring-leisure-events-in-an-sme-context"><a href="http://arxiv.org/abs/2407.04472v3">EventChat: Implementation and user-centric evaluation of a large language model-driven conversational recommender system for exploring leisure events in an SME context</a></h2>
<p>发布时间：2024-07-05</p>
<p>作者：Hannes Kunstmann, Joseph Ollier, Joel Persson, Florian von Wangenheim</p>
<h4 id="_199">中文摘要：</h4>
<p>大型语言模型（LLMs）在对话推荐系统（CRS）的战略潜力方面带来了巨大的进步。然而，迄今为止，研究主要集中在实现LLM驱动的CRS的技术框架上，而不是对最终用户的评估或对企业（尤其是构成全球经济基石的小型和中型企业（SME））的战略影响。在本文中，我们详细介绍了在一个SME环境中设计的LLM驱动的CRS，并使用客观系统指标和主观用户评估来衡量其在实际应用中的表现。在此过程中，我们还概述了一个简短的改进ResQue模型，用于评估LLM驱动的CRS，以促进在快速发展的领域中的可重复性。我们的结果表明，从用户体验角度来看，系统表现良好（85.5%的推荐准确率），但同时也突显了延迟、成本和质量问题，这些挑战着商业的可行性。值得注意的是，每交互的平均成本为0.04美元，延迟为5.7秒，成本效益和响应时间成为实现更用户友好和具有经济可行性的LLM驱动的CRS在SME环境中的关键领域。这些成本的主要驱动因素是使用高级LLM作为检索增强生成（RAG）技术中的排序器。我们的结果还表明，仅依靠如以ChatGPT作为基础LLM的提示学习等方法的挑战在于，在生产环境中难以达到令人满意的质量。我们还概述了SME部署LLM驱动的CRS时的战略考虑，特别是考虑到当前技术景观中的权衡。</p>
<h4 id="_200">一句话总结：</h4>
<p>本文研究了LLM驱动的对话推荐系统在SME环境中的应用，发现成本效益和响应时间是实现用户友好和经济可行的关键。</p>
<hr />
<h2 id="arigraph-learning-knowledge-graph-world-models-with-episodic-memory-for-llm-agents"><a href="http://arxiv.org/abs/2407.04363v1">AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents</a></h2>
<p>发布时间：2024-07-05</p>
<p>作者：Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, Evgeny Burnaev</p>
<h4 id="_201">中文摘要：</h4>
<p>随着生成式人工智能的进步，大型语言模型（LLMs）在自主代理开发中的应用潜力得到了拓展。实现真正的自主性需要积累和更新从与环境交互中获得的知识，并有效地利用它。当前的基于LLM的方法通过使用观察的全历史记录、总结或检索增强来利用过去的经验。然而，这些非结构化的记忆表示并不便于推理和规划，这对于复杂决策是必要的。在我们的研究中，我们引入了AriGraph，这是一种新颖的方法，其中代理在探索环境的同时构建了一个记忆图，该图整合了语义和情景记忆。这种图结构促进了相关联的概念的关联检索，这些概念与代理的当前状态和目标相关，从而作为一个有效的环境模型，增强了代理的探索和规划能力。我们证明了我们的Ariadne LLM代理，配备了这个提议的记忆架构，并增加了规划和决策功能，在TextWorld环境中能够有效地处理复杂任务，无需任何先验知识。我们的方法在包括First TextWorld Problems竞赛中的烹饪挑战以及如家庭清洁和解谜寻宝等新任务在内的各种任务中，明显优于全历史记录、总结和检索增强生成等现有方法。</p>
<h4 id="_202">一句话总结：</h4>
<p>本研究提出的AriGraph方法通过构建记忆图，有效提升了LLM代理在复杂环境中的探索和规划能力。</p>
<hr />
<h2 id="an-interactive-multi-modal-query-answering-system-with-retrieval-augmented-large-language-models"><a href="http://arxiv.org/abs/2407.04217v1">An Interactive Multi-modal Query Answering System with Retrieval-Augmented Large Language Models</a></h2>
<p>发布时间：2024-07-05</p>
<p>作者：Mengzhao Wang, Haotian Wu, Xiangyu Ke, Yunjun Gao, Xiaoliang Xu, Lu Chen</p>
<h4 id="_203">中文摘要：</h4>
<p>本文提出了一种基于新开发的跨模态检索框架和导航图索引的交互式多模态查询回答（MQA）系统，该系统集成了先进的语言大模型（LLMs）。该系统包括五个核心组件：数据预处理、向量表示、索引构建、查询执行和答案生成，这些组件由一个专门的协调器协同工作，以确保从输入到答案生成的数据流畅。MQA的一个显著特点是利用对比学习来评估不同模态的重要性，从而精确测量多模态信息相似度。此外，系统通过使用计算剪枝技术优化的高级导航图索引实现了高效的检索。该系统的另一个亮点是其可插拔的处理框架，允许无缝集成嵌入模型、图索引和LLMs，为用户提供从多模态知识库中获取洞察力的多种选择。MQA的初步视频介绍可在https://youtu.be/xvUuo2ZIqWk查看。</p>
<h4 id="_204">一句话总结：</h4>
<p>本文提出了一种基于LLMs的交互式多模态查询回答系统，通过对比学习和高效的导航图索引，实现了对多模态信息的高效检索和精确相似度测量。</p>
<hr />
<h2 id="nutribench-a-dataset-for-evaluating-large-language-models-in-carbohydrate-estimation-from-meal-descriptions"><a href="http://arxiv.org/abs/2407.12843v1">NutriBench: A Dataset for Evaluating Large Language Models in Carbohydrate Estimation from Meal Descriptions</a></h2>
<p>发布时间：2024-07-04</p>
<p>作者：Andong Hua, Mehak Preet Dhaliwal, Ryan Burke, Yao Qin</p>
<h4 id="_205">中文摘要：</h4>
<p>准确的营养估算有助于人们做出明智的饮食选择，对于预防严重健康问题至关重要。我们提出了NutriBench，这是第一个公开可用的基于自然语言餐食描述的营养基准。NutriBench包含5,000条经过人工验证的餐食描述，带有宏量营养素标签，包括碳水化合物、蛋白质、脂肪和卡路里。数据根据餐食中食物项目的数量、份量和流行程度以及份量描述的特定性，分为15个子集，复杂度各异。我们对包括GPT-3.5、Llama-3以及具有标准、思维链和检索增强生成策略的医疗领域特定模型在内的七种流行的最先进的大型语言模型（LLMs）进行了广泛的评估，这些模型在我们的基准上对碳水化合物估算进行了评估。我们还进行了一项包括专家和非专家参与者的研究，发现LLMs可以在一系列复杂查询中提供更准确和更快的预测。我们对不同的LLMs进行了全面的分析和比较，突出了在现实场景中使用LLMs进行营养估算的机会和挑战。我们的基准可在以下网址公开获取：https://mehak126.github.io/nutribench.html</p>
<h4 id="_206">一句话总结：</h4>
<p>NutriBench是一个基于自然语言餐食描述的营养基准，通过评估多个大型语言模型，展示了其在营养估算中的潜力和挑战。</p>
<hr />
<h2 id="meta-prompting-optimized-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.03955v1">Meta-prompting Optimized Retrieval-augmented Generation</a></h2>
<p>发布时间：2024-07-04</p>
<p>作者：João Rodrigues, António Branco</p>
<h4 id="_207">中文摘要：</h4>
<p>检索增强生成利用从外部来源检索到的内容来提升大型语言模型在下游任务中的性能。然而，检索到的内容量过大、其部分可能分散，或者超出焦点范围，可能会最终产生负面影响而非增量效果。为了缓解这一问题并改进检索增强生成，我们提出了一种方法，通过元提示优化来在将其包含在提示之前对检索到的内容进行细化。通过在来自StrategyQA数据集的具有挑战性的多跳问答任务上进行实证测试，评估结果表明，这种方法比没有使用此方法的类似检索增强系统表现优异，超过30%。</p>
<h4 id="_208">一句话总结：</h4>
<p>该方法通过元提示优化对检索内容进行细化，显著提升了检索增强生成的性能。</p>
<hr />
<h2 id="tonggu-mastering-classical-chinese-understanding-with-knowledge-grounded-large-language-models"><a href="http://arxiv.org/abs/2407.03937v1">TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models</a></h2>
<p>发布时间：2024-07-04</p>
<p>作者：Jiahuan Cao, Dezhi Peng, Peirong Zhang, Yongxin Shi, Yang Liu, Kai Ding, Lianwen Jin</p>
<h4 id="_209">中文摘要：</h4>
<p>古典汉语是通往中国古代丰富遗产和智慧的大门，但其复杂性对大多数没有专业知识的人来说构成了巨大的理解障碍。尽管大型语言模型（LLMs）在自然语言处理（NLP）方面表现出惊人的能力，但在古典汉语理解（CCU）方面，尤其是在数据需求和知识密集型任务中，它们面临着挑战。为了应对这一困境，我们提出了\textbf{TongGu}（意为理解古今），这是第一个针对CCU的LLM，其基于三个核心贡献。首先，我们从丰富的古典汉语语料库中构建了一个两阶段指令微调数据集ACCN-INS，旨在释放LLMs在CCU方面的全部潜力。其次，我们提出了冗余感知调优（RAT）以防止灾难性遗忘，使TongGu能够在保留其基础知识的同时获得新的能力。第三，我们提出了基于知识归因的CCU检索增强生成（CCU-RAG）技术，以减少幻觉。在24个不同的CCU任务上的广泛实验验证了TongGu的优越能力，强调了RAT和CCU-RAG的有效性。该模型和数据集将公开可用。</p>
<h4 id="_210">一句话总结：</h4>
<p>TongGu是首个针对古典汉语理解的LLM，通过创新的数据集和调优技术，显著提升了古典汉语处理能力。</p>
<hr />
<h2 id="dslr-document-refinement-with-sentence-level-re-ranking-and-reconstruction-to-enhance-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.03627v2">DSLR: Document Refinement with Sentence-Level Re-ranking and Reconstruction to Enhance Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-07-04</p>
<p>作者：Taeho Hwang, Soyeong Jeong, Sukmin Cho, SeungYoon Han, Jong C. Park</p>
<h4 id="_211">中文摘要：</h4>
<p>近年来，大型语言模型（LLMs）在自然语言处理（NLP）的各种任务上的性能得到了显著提升。然而，由于参数记忆的限制，LLMs在生成非事实性回应方面仍然存在困难。检索增强生成（RAG）系统通过引入检索模块来整合外部知识，以解决这一问题。尽管如此，当前的RAG系统在检索失败和LLMs过滤无关信息的能力有限方面仍面临挑战。因此，在本研究中，我们提出了DSLR（基于句子级重排序和重构的文档细化），这是一个无监督框架，它将检索到的文档分解为句子，过滤掉无关的句子，并将它们重新组合成连贯的段落。我们在多个开放域问答数据集上对DSLR进行了实验验证，结果表明DSLR在传统固定大小段落的基础上显著提高了RAG的性能。此外，我们的DSLR在不进行额外训练的情况下，在特定但现实的应用场景中提升了性能，为RAG系统中检索文档的细化提供了一个有效且高效的解决方案。</p>
<h4 id="_212">一句话总结：</h4>
<p>本研究提出的DSLR框架通过句子级重排序和重构，有效提升了RAG系统在检索文档细化方面的性能。</p>
<hr />
<h2 id="automated-cc-program-repair-for-high-level-synthesis-via-large-language-models"><a href="http://arxiv.org/abs/2407.03889v1">Automated C/C++ Program Repair for High-Level Synthesis via Large Language Models</a></h2>
<p>发布时间：2024-07-04</p>
<p>作者：Kangwei Xu, Grace Li Zhang, Xunzhao Yin, Cheng Zhuo, Ulf Schlichtmann, Bing Li</p>
<h4 id="_213">中文摘要：</h4>
<p>在高级综合（HLS）中，将常规的C/C++程序转换为与其兼容的HLS版本（HLS-C）仍然需要大量的手动工作。已经引入了各种程序脚本来自动化这一过程。但是，生成的代码通常包含许多问题，需要开发者手动修复。由于大型语言模型（LLMs）具有自动化代码生成的能力，它们也可以用于HLS中的自动程序修复。然而，由于LLMs在考虑硬件和软件的同时训练有限，使用LLMs进行程序修复时可能会出现幻觉，导致编译失败。此外，使用LLMs进行迭代修复也会产生高昂的成本。为了解决这些挑战，我们提出了一种LLM驱动的程序修复框架，该框架以常规的C/C++代码作为输入，并自动生成相应的HLS-C代码以进行综合，同时最大限度地减少人工修复的工作量。为了减轻LLMs中的幻觉并提高提示质量，引入了检索增强生成（RAG）范式来引导LLMs进行正确的修复。此外，我们使用LLMs创建了一个静态位宽优化程序，以确定变量的优化位宽。此外，还引入了LLM驱动的HLS优化策略，用于在HLS-C程序中添加/调整pragma以进行电路优化。实验结果表明，与传统的脚本和直接应用LLMs进行程序修复相比，所提出的LLM驱动的自动化框架在24个真实世界应用中实现了更高的修复通过率。</p>
<h4 id="_214">一句话总结：</h4>
<p>该研究提出了一种基于LLM的自动化程序修复框架，用于HLS，通过引入RAG范式和静态位宽优化，显著提高了修复效率和成功率。</p>
<hr />
<h2 id="casegpt-a-case-reasoning-framework-based-on-language-models-and-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.07913v1">CaseGPT: a case reasoning framework based on language models and retrieval-augmented generation</a></h2>
<p>发布时间：2024-07-04</p>
<p>作者：Rui Yang</p>
<h4 id="_215">中文摘要：</h4>
<p>本文提出了一种名为CaseGPT的创新方法，该方法结合了大型语言模型（LLMs）和检索增强生成（RAG）技术，以增强医疗和法律领域中的基于案例推理。该系统通过允许基于不精确描述的模糊搜索来解决传统数据库查询的挑战，从而提高数据可搜索性和可用性。CaseGPT不仅检索相关案例数据，还能根据现有案例数据中识别出的模式生成有洞察力的建议和推荐。这一功能对于医疗诊断、法律先例研究和案例策略制定等任务特别有价值。论文深入讨论了该系统的方法论、在医疗和法律领域的性能以及其未来应用的潜力。我们的实验表明，CaseGPT在精确度、召回率和效率方面显著优于传统的基于关键词和简单LLM的系统。</p>
<h4 id="_216">一句话总结：</h4>
<p>CaseGPT通过结合大型语言模型和检索增强生成技术，显著提升了医疗和法律领域基于案例推理的效率和准确性。</p>
<hr />
<h2 id="improving-retrieval-augmented-text-to-sql-with-ast-based-ranking-and-schema-pruning"><a href="http://arxiv.org/abs/2407.03227v1">Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning</a></h2>
<p>发布时间：2024-07-03</p>
<p>作者：Zhili Shen, Pavlos Vougiouklis, Chenxin Diao, Kaustubh Vyas, Yuanyi Ji, Jeff Z. Pan</p>
<h4 id="_217">中文摘要：</h4>
<p>本文从大型语言模型的角度关注文本到SQL的语义解析。受商业数据库模式规模和商业智能解决方案部署性挑战的启发，我们提出了一种动态检索输入数据库信息并使用抽象语法树选择少量示例进行上下文学习的方法。此外，我们研究了并行语义解析在生成预期SQL查询的近似版本方面的应用程度，以支持我们的检索。我们将这种方法推向极致——我们调整了一个参数少于5亿个的模型，使其作为极其高效的近似器，并通过并行处理模式的能力对其进行增强。我们将我们的方法应用于单语种和跨语言语义解析基准测试，显示出优于现有基线的改进。全面的实验突出了涉及检索增强生成设置中模块的贡献，揭示了未来工作的有趣方向。</p>
<h4 id="_218">一句话总结：</h4>
<p>本文提出了一种基于大型语言模型的文本到SQL语义解析方法，通过动态检索数据库信息和并行处理模式，实现了高效的近似查询生成，并在语义解析基准测试中取得了显著改进。</p>
<hr />
<h2 id="a-comparative-study-of-dsl-code-generation-fine-tuning-vs-optimized-retrieval-augmentation"><a href="http://arxiv.org/abs/2407.02742v1">A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation</a></h2>
<p>发布时间：2024-07-03</p>
<p>作者：Nastaran Bassamzadeh, Chhaya Methani</p>
<h4 id="_219">中文摘要：</h4>
<p>近年来，随着大型语言模型（LLMs）的出现，自然语言到代码生成（Natural Language to Code Generation）取得了显著进展。尽管在通用语言如C、C++和Python的生成方面取得了显著改进，但LLMs在特定领域语言（DSLs）中的自定义函数名处理上仍然存在困难。这导致了更高的幻觉率和语法错误，尤其是在具有大量自定义函数名的DSLs中。此外，函数名的持续更新也增加了挑战，因为LLMs需要保持最新。在本文中，我们提出了使用检索增强生成（RAG）优化LLMs进行DSL生成的方案，并进行了比较这些策略的消融研究。我们使用一个DSL生成了一个训练集和测试集，以代表公共领域大约700个API的自动化任务。我们使用训练集来微调一个针对该DSL的Codex模型。我们的结果表明，微调模型在代码相似度指标上得分最高。通过我们的RAG优化，我们在相似度指标上实现了平局。然而，编译率显示，两个模型仍然多次出现语法错误，基于RAG的方法比基于LLMs的方法高出2分。相反，RAG模型的幻觉率在API名称上落后1分，在API参数键上落后2分。我们得出结论，优化的RAG模型可以匹配微调模型的质量，并为新的、未见过的API提供优势。</p>
<h4 id="_220">一句话总结：</h4>
<p>本文提出了一种基于RAG的优化方法，用于提高LLMs在特定领域语言中的代码生成质量，并证明了该方法在代码相似度和编译率上均优于直接微调模型。</p>
<hr />
<h2 id="rankrag-unifying-context-ranking-with-retrieval-augmented-generation-in-llms"><a href="http://arxiv.org/abs/2407.02485v1">RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs</a></h2>
<p>发布时间：2024-07-02</p>
<p>作者：Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, Bryan Catanzaro</p>
<h4 id="_221">中文摘要：</h4>
<p>大型语言模型（LLMs）通常在检索增强生成（RAG）中利用检索器的top-k上下文。在这项工作中，我们提出了一种新颖的指令微调框架RankRAG，该框架通过指令微调单个LLM，使其在RAG中同时实现上下文排序和答案生成。具体来说，通过将一小部分排序数据添加到训练混合中，指令微调的LLMs表现出惊人的效果，并优于现有的专家排序模型，包括仅在大量排序数据上专门微调的相同LLM。对于生成任务，我们将我们的模型与许多强大的基线进行了比较，包括GPT-4-0613、GPT-4-turbo-2024-0409和ChatQA-1.5，这是一个在RAG基准测试中具有最先进性能的开源模型。具体来说，我们的Llama3-RankRAG在九个知识密集型基准测试中显著优于Llama3-ChatQA-1.5和GPT-4模型。此外，在没有对生物医学数据进行指令微调的情况下，它在五个生物医学领域的RAG基准测试中也与GPT-4模型表现相当，这证明了其在新领域泛化方面的卓越能力。</p>
<h4 id="_222">一句话总结：</h4>
<p>RankRAG通过指令微调单个LLM，在RAG任务中实现了上下文排序和答案生成的双重优化，显著提升了模型在多个基准测试中的性能。</p>
<hr />
<h2 id="synthetic-multimodal-question-generation"><a href="http://arxiv.org/abs/2407.02233v1">Synthetic Multimodal Question Generation</a></h2>
<p>发布时间：2024-07-02</p>
<p>作者：Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosenberg, Sina Pakazad, Tongshuang Wu, Graham Neubig</p>
<h4 id="_223">中文摘要：</h4>
<p>多模态检索增强生成（Multimodal Retrieval Augmented Generation，MMRAG）是一种强大的跨多模态文档的问答方法。评估MMRAG的一个关键挑战是缺乏与感兴趣的问题风格和模态相匹配的高质量数据集。鉴于此，我们提出了SMMQG，一个合成数据生成框架。SMMQG利用检索器、大型语言模型（Large Language Model，LLM）和大型多模态模型（Large Multimodal Model，LMM）之间的相互作用，直接从多模态文档中生成问题和答案对，其中问题符合指定的风格和模态。我们使用SMMQG生成了一个包含1024个问题的MMRAG数据集，并使用它评估了最先进的模型，揭示了只有通过风格和模态特定的评估数据才能获得的模型性能见解。接下来，我们通过一项人类研究来衡量SMMQG生成数据的质量。我们发现，我们合成数据的质量与来自众包基准MMQA的质量相当，并且使用这两个数据集进行下游评估的结果高度一致。</p>
<h4 id="_224">一句话总结：</h4>
<p>SMMQG通过模拟多模态文档中的问答对，为MMRAG模型提供了高质量的数据集，并揭示了模型性能的见解。</p>
<hr />
<h2 id="why-does-in-context-learning-fail-sometimes-evaluating-in-context-learning-on-open-and-closed-questions"><a href="http://arxiv.org/abs/2407.02028v1">Why does in-context learning fail sometimes? Evaluating in-context learning on open and closed questions</a></h2>
<p>发布时间：2024-07-02</p>
<p>作者：Xiang Li, Haoran Tang, Siyu Chen, Ziwei Wang, Ryan Chen, Marcin Abram</p>
<h4 id="_225">中文摘要：</h4>
<p>本研究测量了情境学习性能作为任务新颖性和难度的函数，针对开放和封闭问题。为此，我们创建了一个新的基准，包括难以解答的科学问题，每个问题都与各种相关性的情境相匹配。我们发现，出人意料的是，与主题更一致的情境并不总是比不那么相关的情境更有帮助。这种效应在开放问题和高度难度或新颖性的问题中尤为明显。这一结果揭示了大型语言模型在处理封闭式和开放式问题时的根本差异，并表明需要对不同类型问题的情境学习进行更稳健的评估。此外，它还提出了一个新问题，即如何最优地选择大型语言模型的情境，特别是在检索增强生成（RAG）系统的背景下。我们的结果表明，这个问题的答案可能高度依赖于应用，并且可能取决于包括问题的格式、问题的感知难度水平以及我们寻求的信息的新颖性或流行度等因素。</p>
<h4 id="_226">一句话总结：</h4>
<p>本研究揭示了情境学习在处理不同类型问题时存在差异，并强调了选择合适情境对大型语言模型性能的重要性。</p>
<hr />
<h2 id="mememo-on-device-retrieval-augmentation-for-private-and-personalized-text-generation"><a href="http://arxiv.org/abs/2407.01972v1">MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation</a></h2>
<p>发布时间：2024-07-02</p>
<p>作者：Zijie J. Wang, Duen Horng Chau</p>
<h4 id="_227">中文摘要：</h4>
<p>检索增强文本生成（RAG）通过从可更新的外部知识库中检索信息来解决大型语言模型（LLMs）的常见局限性，如幻觉。然而，现有方法通常需要专门的后端服务器进行数据存储和检索，这限制了它们在需要严格数据隐私的使用场景中的应用，例如个人金融、教育和医疗。为了解决客户端密集检索的迫切需求，我们引入了MeMemo，这是第一个开源的JavaScript工具包，它将最先进的近似最近邻搜索技术HNSW适配到浏览器环境中。我们的工具包使用现代和原生的Web技术，如IndexedDB和Web Workers，利用客户端硬件能力，使研究人员和开发者能够高效地在浏览器中搜索数百万个高维向量。MeMemo为新的设计和研究机会提供了可能性，例如私有和个性化的内容创作以及交互式原型设计，如我们在示例应用RAG Playground中所展示的。回顾我们的工作，我们讨论了设备端密集检索的机会和挑战。MeMemo可在https://github.com/poloclub/mememo获取。</p>
<h4 id="_228">一句话总结：</h4>
<p>MeMemo是一个开源的JavaScript工具包，它将先进的近似最近邻搜索技术应用于浏览器环境，以实现客户端密集检索，从而为隐私敏感的应用场景提供了一种新的解决方案。</p>
<hr />
<h2 id="ground-every-sentence-improving-retrieval-augmented-llms-with-interleaved-reference-claim-generation"><a href="http://arxiv.org/abs/2407.01796v1">Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Sirui Xia, Xintao Wang, Jiaqing Liang, Yifei Zhang, Weikang Zhou, Jiaji Deng, Fei Yu, Yanghua Xiao</p>
<h4 id="_229">中文摘要：</h4>
<p>检索增强生成（RAG）已被广泛采用以增强大型语言模型（LLMs）在知识密集型任务中的性能。最近，属性文本生成（ATG）引起了越来越多的关注，它为RAG中的模型响应提供引用，以提高LLM生成内容的可信度并促进验证。先前的方法主要采用粗粒度属性，与段落级别的参考文献或提供段落级别的引用相联系。然而，这些方法在可验证性方面仍有不足，并且需要一定的时间成本进行事实核查。本文提出了一种名为ReClaim（Refer &amp; Claim）的细粒度ATG方法，该方法逐步交替生成引用和答案。与传统的粗粒度属性不同，ReClaim允许模型在长格式问答任务中对每个答案句子添加句子级别的细粒度引用。我们的实验涵盖了各种训练和推理方法以及多个LLMs，验证了我们的方法的有效性。</p>
<h4 id="_230">一句话总结：</h4>
<p>本文提出了一种名为ReClaim的细粒度属性文本生成方法，通过在长格式问答任务中为每个答案句子添加句子级别的引用，提高了LLM生成内容的可验证性和可信度。</p>
<hr />
<h2 id="retrieval-augmented-generation-in-multilingual-settings"><a href="http://arxiv.org/abs/2407.01463v1">Retrieval-augmented generation in multilingual settings</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Nadezhda Chirkova, David Rau, Hervé Déjean, Thibault Formal, Stéphane Clinchant, Vassilina Nikoulina</p>
<h4 id="_231">中文摘要：</h4>
<p>检索增强生成（RAG）最近作为一种将最新或特定领域知识融入大型语言模型（LLMs）并提高LLMs事实性的有前景解决方案出现，但主要在仅英语的设置中研究。在这项工作中，我们考虑了多语言设置下的RAG（mRAG），即用户查询和数据存储库在13种语言中，并研究了构建一个高性能的mRAG管道所需的组件和调整，该管道可以用作未来工作的强大基线。我们的发现强调，尽管有高质量现成的多语言检索器和生成器，但需要特定任务的提示工程来实现用户语言的生成。此外，当前的评估指标需要针对多语言设置进行调整，以考虑拼写命名实体的变化。未来工作中需要解决的主要限制包括非拉丁字母语言中的频繁代码转换、偶尔的流畅性错误、提供的文档的错误阅读或不相关的检索。我们在https://github.com/naver/bergen上发布了所得到的mRAG基线管道的代码。</p>
<h4 id="_232">一句话总结：</h4>
<p>本研究探讨了在多语言环境下构建高效检索增强生成（mRAG）管道所需的组件和调整，并强调了特定任务提示工程和评估指标调整的重要性。</p>
<hr />
<h2 id="first-place-solution-of-2023-global-artificial-intelligence-technology-innovation-competition-track-1"><a href="http://arxiv.org/abs/2407.01271v2">First Place Solution of 2023 Global Artificial Intelligence Technology Innovation Competition Track 1</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Xiangyu Wu, Hailiang Zhang, Yang Yang, Jianfeng Lu</p>
<h4 id="_233">中文摘要：</h4>
<p>本文提出了一种针对全球人工智能技术创新竞赛一阶段：医学影像诊断报告生成的冠军解决方案。我们选择CPT-BASE作为文本生成任务的基础模型。在预训练阶段，我们删除了CPT-BASE的掩码语言建模任务，并重新构建词汇表，采用跨度掩码策略，逐步增加掩码比例以执行去噪自编码器预训练任务。在微调阶段，我们设计了迭代检索增强和噪声感知相似桶提示策略。检索增强构建了一个微型知识库，丰富了模型的输入信息，而相似桶进一步感知微型知识库中的噪声信息，引导模型根据相似提示生成更高质量的诊断报告。令人惊讶的是，我们的单一模型在排行榜A上取得了2.321的分数，多模型融合的A和B排行榜分数分别为2.362和2.320，确保了排名第一的位置。</p>
<h4 id="_234">一句话总结：</h4>
<p>本文提出的基于CPT-BASE的医学影像诊断报告生成模型在竞赛中取得了优异成绩，实现了高质高效的诊断报告生成。</p>
<hr />
<h2 id="searching-for-best-practices-in-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.01219v1">Searching for Best Practices in Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang</p>
<h4 id="_235">中文摘要：</h4>
<p>检索增强生成（RAG）技术在整合最新信息、减轻幻觉和提升响应质量方面已被证明是有效的，尤其是在专业领域。尽管许多RAG方法被提出通过查询依赖的检索来增强大型语言模型，但这些方法仍然受到其复杂实现和延长响应时间的困扰。通常，一个RAG工作流程涉及多个处理步骤，每个步骤都可以以不同的方式执行。在这里，我们研究了现有的RAG方法及其潜在组合，以确定最佳的RAG实践。通过广泛的实验，我们提出了几种平衡性能和效率的RAG部署策略。此外，我们证明了多模态检索技术可以显著增强关于视觉输入的问答能力，并使用“检索即生成”策略加速多模态内容的生成。</p>
<h4 id="_236">一句话总结：</h4>
<p>本研究通过实验探讨了检索增强生成（RAG）技术的最佳实践，以提升大型语言模型在专业领域的性能和效率。</p>
<hr />
<h2 id="textmemory3-language-modeling-with-explicit-memory"><a href="http://arxiv.org/abs/2407.01178v1">$\text{Memory}^3$: Language Modeling with Explicit Memory</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, Weinan E</p>
<h4 id="_237">中文摘要：</h4>
<p>本文提出了一种通过为大型语言模型（LLMs）配备显式记忆来降低训练和推理成本的方法。这种显式记忆是一种比模型参数和文本检索增强生成（RAG）更经济的内存格式，灵感来源于人脑的记忆层次结构。通过将大部分知识外部化到显式记忆中，LLM可以享受更小的参数大小、训练成本和推理成本，这些成本与剩余的“抽象知识”量成正比。作为概念验证的初步证明，我们从头开始训练了一个24亿的LLM，其性能优于许多更大的LLM和RAG模型，并且解码速度高于RAG。该模型命名为$\text{Memory}^3$，因为显式记忆是LLM中继隐式记忆（模型参数）和工作记忆（上下文键值）之后的第三种记忆形式。我们引入了一种记忆电路理论来支持知识的对外化，并提出了包括使存储变得可处理的记忆稀疏化机制和促进记忆形成的两阶段预训练方案在内的创新技术。</p>
<h4 id="_238">一句话总结：</h4>
<p>本文提出了一种通过引入显式记忆来降低大型语言模型训练和推理成本的新方法，显著提高了模型性能和效率。</p>
<hr />
<h2 id="learning-to-explore-and-select-for-coverage-conditioned-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.01158v1">Learning to Explore and Select for Coverage-Conditioned Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Takyoung Kim, Kyungjae Lee, Young Rok Jang, Ji Yong Cho, Gangwoo Kim, Minseok Cho, Moontae Lee</p>
<h4 id="_239">中文摘要：</h4>
<p>本文研究了与亿规模大型语言模型交互时，由于模型广泛的参数能力和检索增强功能，通常会产生长篇回答的问题。尽管详细的回答提供了对特定主题的深入见解，但它们往往会产生重复且不够吸引人的内容，不符合用户兴趣。在本文中，我们专注于在用户请求特定范围信息的情况下（即覆盖条件（$C^2$）场景）查询概述（即选定的查询序列）的作用。为了模拟$C^2$场景，我们构建了QTree，这是10K组按特定主题的多种视角分解的信息寻求查询。通过利用QTree，我们训练了QPlanner，这是一个7B语言模型，能够生成遵循覆盖条件查询的定制查询概述。我们通过自动和人工评估分析了生成概述的有效性，针对检索增强生成（RAG）。此外，实验结果表明，经过对齐训练的QPlanner可以进一步提供满足不同用户兴趣的概述。我们的资源可在https://github.com/youngerous/qtree上获取。</p>
<h4 id="_240">一句话总结：</h4>
<p>本文提出了一种基于QTree的查询规划器QPlanner，通过生成符合覆盖条件查询的定制概述，有效提升了检索增强生成（RAG）的效果。</p>
<hr />
<h2 id="bergen-a-benchmarking-library-for-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2407.01102v1">BERGEN: A Benchmarking Library for Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, Stéphane Clinchant</p>
<h4 id="_241">中文摘要：</h4>
<p>检索增强生成（Retrieval-Augmented Generation）允许通过外部知识来增强大型语言模型。针对近年来生成式大型语言模型（generative LLMs）的流行，许多RAG方法被提出，这些方法涉及大量不同的配置，如评估数据集、集合、指标、检索器和LLMs。不一致的基准测试在比较方法和理解管道中每个组件的影响方面构成了一个主要挑战。在这项工作中，我们研究了RAG的最佳实践，为RAG的系统评估奠定了基础，并提出了BERGEN，一个用于可重复研究的端到端库，用于标准化RAG实验。在针对问答（QA）的广泛研究中，我们对不同的最先进检索器、重排器和LLMs进行了基准测试。此外，我们还分析了现有的RAG指标和数据集。我们的开源库BERGEN可在\url{https://github.com/naver/bergen}获取。</p>
<h4 id="_242">一句话总结：</h4>
<p>本研究提出了一种端到端的RAG库BERGEN，用于标准化和比较不同RAG方法，并通过基准测试和分析现有指标和数据集，为RAG的系统评估提供了基础。</p>
<hr />
<h2 id="eliminating-position-bias-of-language-models-a-mechanistic-approach"><a href="http://arxiv.org/abs/2407.01100v1">Eliminating Position Bias of Language Models: A Mechanistic Approach</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang, Chi Han, Shuiwang Ji, Sham M. Kakade, Hao Peng, Heng Ji</p>
<h4 id="_243">中文摘要：</h4>
<p>位置偏差已被证明是现代语言模型（LMs）的一个普遍问题，其中模型根据内容在给定上下文中的位置优先处理内容。这种偏差通常会导致模型出现意外的失败，并损害各种应用中的性能、鲁棒性和可靠性。我们的机制分析将位置偏差归因于几乎所有最先进的LMs中使用的两个组件：因果注意力（causal attention）和相对位置编码（relative positional encodings）。具体来说，我们发现因果注意力通常导致模型偏好远离内容，而基于检索增强问答（QA）的分析，相对位置编码如RoPE则偏好邻近内容。此外，我们的关于目标检测的实证研究揭示了位置偏差也存在于视觉语言模型（VLMs）中。基于上述分析，我们提出了一种无训练、零样本（TRAINING-FREE ZERO-SHOT）的方法来消除由不同输入段顺序（例如，LM作为裁判的选项，QA中检索到的文档）引起的位置偏差。我们的方法将因果注意力改为段之间的双向注意力，并利用模型注意力值来决定段的相对顺序，而不是使用输入提示中提供的顺序，从而在段级别实现位置不变推断（Position-INvariant inferencE，PINE）。通过消除位置偏差，模型在位置偏差普遍存在的下游任务中（如LM作为裁判和检索增强QA）实现了更好的性能和可靠性。值得注意的是，PINE在将LM用于评估推理对时特别有用：它通常在大多数情况下提供8到10个百分点的性能提升，并使Llama-3-70B-Instruct在RewardBench推理子集上的表现甚至优于GPT-4-0125-preview。</p>
<h4 id="_244">一句话总结：</h4>
<p>本研究提出了一种消除语言模型中位置偏差的方法，通过改变注意力机制和利用模型注意力值来决定段顺序，从而提高模型在下游任务中的性能和可靠性。</p>
<hr />
<h2 id="face4rag-factual-consistency-evaluation-for-retrieval-augmented-generation-in-chinese"><a href="http://arxiv.org/abs/2407.01080v2">Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Yunqi Xu, Tianchi Cai, Jiyan Jiang, Xierui Song</p>
<h4 id="_245">中文摘要：</h4>
<p>当前传统检索增强生成（RAG）中普遍存在的实际不一致错误问题促使了对事实一致性评估（FCE）的研究。尽管之前提出了各种FCE方法，但这些方法都是在由特定大型语言模型（LLM）生成的数据集上进行的评估。由于缺乏一个全面的基准，这些FCE方法在其他具有不同错误分布或甚至未见过的错误类型的LLM上的表现仍然未被探索，因为这些方法可能无法检测由其他LLM生成的错误类型。为了填补这一空白，在本文中，我们提出了第一个独立的RAG事实一致性评估基准\emph{Face4RAG}。我们的基准包括一个基于精心设计的关于事实不一致错误类型的合成数据集，以及从六个常用LLM构建的真实世界数据集，这使得可以对特定错误类型或真实世界的错误分布进行FCE方法的评估。在提出的基准上，我们发现现有的FCE方法未能检测到逻辑谬误，这指的是答案和检索到的参考之间的逻辑结构不匹配。为了解决这个问题，我们进一步提出了一种名为\emph{L-Face4RAG}的新方法，该方法具有两种新颖的设计：逻辑保留答案分解和事实-逻辑FCE。大量的实验表明，L-Face4RAG在广泛的任务上显著优于先前的事实不一致检测方法，特别是在其最初激发的RAG任务之外。基准和我们的提出的方法都是公开可用的。\footnote{\url{https://huggingface.co/datasets/yq27/Face4RAG}\label{link_face4rag}}</p>
<h4 id="_246">一句话总结：</h4>
<p>本文提出了首个独立的RAG事实一致性评估基准Face4RAG，并提出了L-Face4RAG方法，显著提升了事实不一致检测的性能。</p>
<hr />
<h2 id="exploring-advanced-large-language-models-with-llmsuite"><a href="http://arxiv.org/abs/2407.12036v1">Exploring Advanced Large Language Models with LLMsuite</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Giorgio Roffo</p>
<h4 id="_247">中文摘要：</h4>
<p>本教程探讨了大型语言模型（LLMs）如ChatGPT和Gemini的发展进展和挑战。它指出了固有的局限性，如时间知识截止、数学不准确性和生成错误信息等问题，并提出了解决方案，如检索增强生成（RAG）、程序辅助语言模型（PAL）以及ReAct和LangChain等框架。这些技术的集成提高了LLMs的性能和可靠性，尤其是在多步推理和复杂任务执行方面。论文还涵盖了微调策略，包括指令微调、参数高效方法如LoRA以及基于人类反馈的强化学习（RLHF）和强化自训练（ReST）。此外，它还提供了关于LLMs的Transformer架构和训练技术的全面调查。实现这些技术的工具箱可在https://github.com/giorgioroffo/large_language_models_open_suite公开获取。</p>
<h4 id="_248">一句话总结：</h4>
<p>本文综述了大型语言模型的发展、挑战及其改进策略，包括技术集成、微调方法和架构设计。</p>
<hr />
<h2 id="secgenai-enhancing-security-of-cloud-based-generative-ai-applications-within-australian-critical-technologies-of-national-interest"><a href="http://arxiv.org/abs/2407.01110v1">SecGenAI: Enhancing Security of Cloud-based Generative AI Applications within Australian Critical Technologies of National Interest</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Christoforus Yoga Haryanto, Minh Hieu Vu, Trung Duc Nguyen, Emily Lomempow, Yulia Nurliana, Sona Taheri</p>
<h4 id="_249">中文摘要：</h4>
<p>本文介绍了SecGenAI，这是一个针对基于云的生成式人工智能（GenAI）应用的全面安全框架，重点关注检索增强生成（RAG）系统。SecGenAI解决了功能、基础设施和治理需求，通过整合端到端安全分析来生成强调数据隐私、安全部署和共享责任模型的规范。SecGenAI与澳大利亚隐私原则、人工智能伦理原则以及澳大利亚网络安全中心和数字转型机构的指南保持一致，减轻了数据泄露、对抗性攻击和模型反演等威胁。该框架的创新方法结合了先进的机器学习技术与强大的安全措施，确保符合澳大利亚法规，同时增强生成式人工智能系统的可靠性和可信度。这项研究通过为行业提供安全的GenAI实施策略，促进人工智能应用的创新，并保护国家利益，为智能系统领域做出了贡献。</p>
<h4 id="_250">一句话总结：</h4>
<p>SecGenAI是一个旨在确保基于云的生成式人工智能应用安全性的全面框架，通过结合先进的机器学习技术与严格的安全措施，以保护澳大利亚的国家利益。</p>
<hr />
<h2 id="hybrid-rag-empowered-multi-modal-llm-for-secure-healthcare-data-management-a-diffusion-based-contract-theory-approach"><a href="http://arxiv.org/abs/2407.00978v1">Hybrid RAG-empowered Multi-modal LLM for Secure Healthcare Data Management: A Diffusion-based Contract Theory Approach</a></h2>
<p>发布时间：2024-07-01</p>
<p>作者：Cheng Su, Jinbo Wen, Jiawen Kang, Yonghua Wang, Hudan Pan, M. Shamim Hossain</p>
<h4 id="_251">中文摘要：</h4>
<p>在快速发展的医疗保健领域，安全的数据管理和有效的数据共享变得至关重要。生成式人工智能的进步使得多模态大型语言模型（MLLMs）成为管理医疗保健数据的关键工具。MLLMs通过在大规模多模态数据上的训练，能够支持多模态输入并生成多种类型的内容。然而，在开发医疗MLLMs时仍存在一些关键挑战，包括医疗保健数据的安全性和新鲜度问题，这些问题会影响MLLMs的输出质量。在本文中，我们提出了一种混合检索增强生成（RAG）赋能的医疗MLLMs框架，用于医疗保健数据管理。该框架利用分层跨链架构来促进安全的数据训练。此外，它通过混合RAG增强了MLLMs的输出质量，混合RAG采用多模态指标来过滤各种单模态RAG结果，并将这些检索结果作为额外的输入提供给MLLMs。此外，我们使用信息年龄间接评估MLLMs对数据新鲜度的影响，并利用契约理论激励医疗数据持有者共享新鲜数据，以减轻数据共享中的信息不对称。最后，我们利用基于生成扩散模型的强化学习算法来识别有效的契约，以实现高效的数据共享。数值结果表明，所提出的方案在安全且高效地管理医疗保健数据方面是有效的。</p>
<h4 id="_252">一句话总结：</h4>
<p>本文提出了一种基于混合RAG和跨链架构的医疗MLLMs框架，旨在解决医疗保健数据管理中的安全性和新鲜度问题，实现高效且安全的数据共享。</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
    
  </body>
</html>