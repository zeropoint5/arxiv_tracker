
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../09/">
      
      
        <link rel="next" href="../07/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.31">
    
    
      
        <title>2024-08(115) - ArXiv Tracker</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="ArXiv Tracker" class="md-header__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ArXiv Tracker
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2024-08(115)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Emission%20Trading%20System/2024/09/" class="md-tabs__link">
          
  
  Emission Trading System

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../09/" class="md-tabs__link">
          
  
  Retrieval Augmented Generation

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="ArXiv Tracker" class="md-nav__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ArXiv Tracker
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Emission Trading System
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Emission Trading System
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2016/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2011/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2011-10(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Retrieval Augmented Generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Retrieval Augmented Generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-09(17)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2024-08(115)
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(134)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(161)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-05(126)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-04(86)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-03(70)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(87)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(50)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(33)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-11(38)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(43)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(20)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-08(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-07(11)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(33)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-04(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-02(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(15)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-09(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-07(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-05(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-04(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-12(9)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-09(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-08(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-06(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-06(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-04(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-12(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2015/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2012/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2012-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2009/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2009-03(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="retrieval-augmented-generation-202408">Retrieval Augmented Generation - 2024年08月</h1>
<h2 id="genai-powered-multi-agent-paradigm-for-smart-urban-mobility-opportunities-and-challenges-for-integrating-large-language-models-llms-and-retrieval-augmented-generation-rag-with-intelligent-transportation-systems"><a href="http://arxiv.org/abs/2409.00494v2">GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility: Opportunities and Challenges for Integrating Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems</a></h2>
<p>发布时间：2024-08-31</p>
<p>作者：Haowen Xu, Jinghui Yuan, Anye Zhou, Guanhao Xu, Wan Li, Xuegang Ban, Xinyue Ye</p>
<h4 id="_1">中文摘要：</h4>
<p>利用近年来在生成式人工智能方面的最新进展，多智能体系统正日益被开发以增强智能城市应用的功能和效率。本文探讨了大型语言模型（LLMs）和新兴的检索增强生成（RAG）技术在智能交通系统（ITS）中的变革潜力，为解决城市交通中的关键挑战提供了创新解决方案。我们首先提供了关于移动数据、ITS和连接车辆（CV）应用当前最先进技术的全面概述。在此基础上，我们讨论了RAG背后的原理，并探讨了将这些生成式人工智能（GenAI）技术整合到智能交通领域的机遇。我们提出一个概念框架，旨在开发能够智能和对话式地提供智能交通服务的多智能体系统，服务于城市通勤者、交通运营商和决策者。我们的方法旨在培养一种自主和智能的方法，(a) 促进基于科学的建议以减少多尺度上的交通拥堵、事故和碳排放，(b) 促进公众教育和参与式交通管理，以及(c) 自动化专业交通管理任务和关键ITS平台的发展，如数据分析与解释、知识表示和交通模拟。通过整合LLM和RAG，我们的方法旨在克服传统基于规则的智能体系统的局限性，这些系统依赖于固定的知识库和有限的推理能力。这种整合为更可扩展、直观和自动化的多智能体范式铺平了道路，推动了ITS和城市交通的进步。</p>
<h4 id="_2">一句话总结：</h4>
<p>本文提出了一种基于大型语言模型和检索增强生成技术的多智能体系统框架，旨在提升智能交通系统的功能和效率，以解决城市交通中的关键挑战。</p>
<hr />
<h2 id="ya-ta-towards-personalized-question-answering-teaching-assistants-using-instructor-student-dual-retrieval-augmented-knowledge-fusion"><a href="http://arxiv.org/abs/2409.00355v1">YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion</a></h2>
<p>发布时间：2024-08-31</p>
<p>作者：Dongil Yang, Suyeon Lee, Minjin Kim, Jungsoo Won, Namyoung Kim, Dongha Lee, Jinyoung Yeo</p>
<h4 id="_3">中文摘要：</h4>
<p>教师与学生之间的互动在提升学生学术表现方面发挥着至关重要的作用。然而，在大班教学中，教师往往难以提供及时和个性化的支持。为了应对这一挑战，我们提出了一种名为YA-TA的新型虚拟教学助手（VTA），旨在为学生提供基于讲座内容且易于理解的反应。为了实现YA-TA，我们引入了双检索增强知识融合（DRAKE）框架，该框架结合了教师和学生知识的双检索以及针对个性化响应生成的知识融合。在现实课堂环境中的实验表明，DRAKE框架在将响应与从教师和学生两方面检索到的知识对齐方面表现出色。此外，我们还提供了YA-TA的额外扩展，例如问答板和自我练习工具，以增强整体的学习体验。我们的视频是公开可用的。</p>
<h4 id="_4">一句话总结：</h4>
<p>本研究提出了一种基于DRAKE框架的虚拟教学助手YA-TA，旨在通过知识融合和个性化响应提升大班教学中的学生学术表现。</p>
<hr />
<h2 id="understanding-the-user-an-intent-based-ranking-dataset"><a href="http://arxiv.org/abs/2408.17103v1">Understanding the User: An Intent-Based Ranking Dataset</a></h2>
<p>发布时间：2024-08-30</p>
<p>作者：Abhijit Anand, Jurek Leonhardt, V Venktesh, Avishek Anand</p>
<p>翻译：</p>
<h4 id="_5">中文摘要：</h4>
<p>随着信息检索系统的不断发展，对这些系统的准确评估和基准测试变得至关重要。如MS MARCO这样的网络搜索数据集，主要提供没有伴随意图或描述的简短关键词查询，这给理解潜在的信息需求带来了挑战。本文提出了一种增强此类数据集的方法，以标注信息查询描述，重点关注两个突出的基准数据集：TREC-DL-21和TREC-DL-22。我们的方法涉及利用最先进的语言模型（LLMs）来分析和理解基准数据集中单个查询中的隐含意图。通过提取关键语义元素，我们为这些查询构建了详细且具有丰富上下文描述。为了验证生成的查询描述，我们采用众包作为一种可靠的方法，以获得关于描述准确性和信息性的多样化人类观点。这些信息可以用作评估诸如排名、查询重写等任务的评估集。</p>
<h4 id="_6">一句话总结：</h4>
<p>本文提出了一种利用最先进的语言模型和众包技术来增强网络搜索数据集，以生成更准确和丰富的查询描述的方法。</p>
<hr />
<h2 id="rissole-parameter-efficient-diffusion-models-via-block-wise-generation-and-retrieval-guidance"><a href="http://arxiv.org/abs/2408.17095v2">RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation and Retrieval-Guidance</a></h2>
<p>发布时间：2024-08-30</p>
<p>作者：Avideep Mukherjee, Soumya Banerjee, Piyush Rai, Vinay P. Namboodiri</p>
<h4 id="_7">中文摘要：</h4>
<p>基于扩散的模型展示了令人印象深刻的生成能力。然而，它们也有大量的参数，导致模型尺寸巨大，这使得它们不适合在资源受限的设备上部署。块状生成可以成为设计紧凑型（参数高效）深度生成模型的有希望的选择，因为模型可以一次生成一个块而不是一次性生成整个图像。然而，块状生成也是一个相当具有挑战性的问题，因为确保生成块之间的连贯性可能并不简单。为此，我们设计了一种检索增强生成（RAG）方法，并利用RAG模块检索到的图像的相应块来条件化块状去噪扩散模型的训练和生成阶段。我们的条件化方案确保了训练期间以及生成期间不同块之间的连贯性。虽然我们使用潜在扩散模型（LDM）作为基础模型展示了我们的方法，但它也可以用于去噪扩散模型的其它变体。我们通过报告实质性实验来验证连贯性问题的解决方案，以证明我们方法在紧凑模型尺寸和优秀生成质量方面的有效性。</p>
<h4 id="_8">一句话总结：</h4>
<p>本研究提出了一种基于检索增强的生成方法，通过条件化块状去噪扩散模型，实现了在紧凑模型尺寸下保持生成质量，有效解决了块状生成中的连贯性问题。</p>
<hr />
<h2 id="retrieval-augmented-natural-language-reasoning-for-explainable-visual-question-answering"><a href="http://arxiv.org/abs/2408.17006v1">Retrieval-Augmented Natural Language Reasoning for Explainable Visual Question Answering</a></h2>
<p>发布时间：2024-08-30</p>
<p>作者：Su Hyeon Lim, Minkuk Kim, Hyeon Bae Kim, Seong Tae Kim</p>
<h4 id="_9">中文摘要：</h4>
<p>视觉问答与自然语言解释（VQA-NLE）任务因其对基于推理的推理能力的高要求而具有挑战性。最近的VQA-NLE研究集中于增强模型网络以增强模型的推理能力，但这种方法资源消耗大且不稳定。在这项工作中，我们引入了一种新的VQA-NLE模型，ReRe（检索增强的自然语言推理），该模型利用从记忆中检索的信息来帮助生成准确的答案和有说服力的解释，而不依赖于复杂的网络和额外的数据集。ReRe是一个编码器-解码器架构模型，使用预训练的clip视觉编码器作为编码器，以及预训练的GPT-2语言模型作为解码器。在GPT-2中添加了交叉注意力层来处理检索特征。ReRe在VQA准确性和解释分数方面优于先前的方法，并在NLE中显示出更具说服力和可靠性的改进。</p>
<h4 id="_10">一句话总结：</h4>
<p>该研究提出了一种新的VQA-NLE模型ReRe，通过利用检索信息来增强推理能力，实现了更准确和有说服力的视觉问答与自然语言解释。</p>
<hr />
<h2 id="memlong-memory-augmented-retrieval-for-long-text-modeling"><a href="http://arxiv.org/abs/2408.16967v1">MemLong: Memory-Augmented Retrieval for Long Text Modeling</a></h2>
<p>发布时间：2024-08-30</p>
<p>作者：Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, Min Zhang</p>
<h4 id="_11">中文摘要：</h4>
<p>近年来，大型语言模型（LLMs）在各个领域取得了显著的进展。然而，由于注意力机制的二次时间空间复杂性和生成过程中关键值缓存内存消耗的增长，处理长文本内容仍然是LLMs面临的一个重大挑战。本研究提出了MemLong：一种用于长文本生成的记忆增强检索方法，该方法通过利用外部检索器进行历史信息检索来增强长文本语言模型的能力。MemLong结合了一个不可微分的“ret-mem”模块和一个部分可训练的仅解码语言模型，并引入了一种细粒度、可控制的检索注意力机制，该机制利用语义级别的相关片段。在多个长文本语言模型基准测试上的全面评估表明，MemLong在性能上始终优于其他最先进的LLMs。更重要的是，MemLong可以将单个3090 GPU上的上下文长度从4k扩展到80k。我们的代码可在https://github.com/Bui1dMySea/MemLong上获取。</p>
<h4 id="_12">一句话总结：</h4>
<p>MemLong通过外部检索和细粒度检索注意力机制，显著提升了长文本语言模型的性能和上下文处理能力。</p>
<hr />
<h2 id="retrieval-augmented-instruction-tuning-for-automated-process-engineering-calculations-a-tool-chaining-problem-solving-framework-with-attributable-reflection"><a href="http://arxiv.org/abs/2408.15866v1">Retrieval-Augmented Instruction Tuning for Automated Process Engineering Calculations : A Tool-Chaining Problem-Solving Framework with Attributable Reflection</a></h2>
<p>发布时间：2024-08-28</p>
<p>作者：Sagar Srinivas Sakhinana, Geethan Sannidhi, Venkataramana Runkana</p>
<h4 id="_13">中文摘要：</h4>
<p>当前的技术领域缺乏一个用于解决过程工程计算的基础人工智能模型。在这项工作中，我们介绍了一种利用检索增强指令调整（Retrieval-Augmented Instruction-Tuning，RAIT）的全新自主代理框架，以增强开放、可定制的简短代码语言模型（SLMs）用于这些计算。通过结合指令调整的代码SLMs和使用外部工具的检索增强代码生成（Retrieval-Augmented Code Generation，RACG），代理可以从自然语言规范中生成、调试和优化代码。我们的方法解决了目前缺乏专门过程工程任务基础AI模型的局限性，并提供了可解释性、知识编辑和成本效益的优势。此外，我们精心制作了化学和过程工程问题的解决方案的定制数据集，以克服数据稀缺的问题。实验结果表明，我们的框架在基准数据集上的性能与大规模专有模型相匹配，证明了其有效性和可用性。</p>
<h4 id="_14">一句话总结：</h4>
<p>本研究提出了一种基于RAIT的自主代理框架，用于增强过程工程计算中的代码生成，有效解决了现有AI模型在专业领域应用的局限性。</p>
<hr />
<h2 id="boosting-lossless-speculative-decoding-via-feature-sampling-and-partial-alignment-distillation"><a href="http://arxiv.org/abs/2408.15562v1">Boosting Lossless Speculative Decoding via Feature Sampling and Partial Alignment Distillation</a></h2>
<p>发布时间：2024-08-28</p>
<p>作者：Lujun Gui, Bin Xiao, Lei Su, Weipeng Chen</p>
<h4 id="_15">中文摘要：</h4>
<p>本文提出了一种名为FSPAD（特征采样与部分对齐蒸馏）的无损推测解码方法，该方法通过在现有框架中引入两个简单而有效的组件来提升无损推测解码的性能。首先，FSPAD利用标记嵌入在生成草稿模型之前，在多维空间中对目标大型语言模型（LLM）的特征进行采样，因为特征的固有不确定性阻止了草稿模型通过目标LLM获得特定的标记输出。其次，FSPAD引入了部分对齐蒸馏，以减弱草稿模型中特征与logits之间的联系，旨在减少训练过程中特征对齐与logits置信度之间的冲突。实验涵盖了从Vicuna和LLaMA3-Instruct系列中选取的最大和最小模型，以及多轮对话、翻译、摘要、问答、数学推理和检索增强生成等任务。结果表明，FSPAD在所有上述任务和目标LLM上均优于现有最佳方法。</p>
<h4 id="_16">一句话总结：</h4>
<p>FSPAD通过特征采样和部分对齐蒸馏技术，有效提升了无损推测解码的性能，在多个NLP任务上实现了超越现有最佳方法的成果。</p>
<hr />
<h2 id="lrp4rag-detecting-hallucinations-in-retrieval-augmented-generation-via-layer-wise-relevance-propagation"><a href="http://arxiv.org/abs/2408.15533v2">LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation</a></h2>
<p>发布时间：2024-08-28</p>
<p>作者：Haichuan Hu, Yuhan Sun, Quanjun Zhang</p>
<h4 id="_17">中文摘要：</h4>
<p>检索增强生成（RAG）已成为缓解大型语言模型（LLMs）中幻觉的主要技术。然而，知识提取不完整和理解不足仍然可能导致LLMs产生无关甚至矛盾的反应，这意味着在RAG中幻觉仍然存在。在本文中，我们提出了LRP4RAG，这是一种基于层相关传播（LRP）算法的检测RAG中幻觉的方法。具体来说，我们首先利用LRP计算RAG生成器的输入和输出之间的相关性。然后，我们对相关性矩阵进行进一步提取和重采样。处理过的相关性数据被输入到多个分类器中，以确定输出是否包含幻觉。据我们所知，这是首次将LRP用于检测RAG幻觉，大量的实验表明LRP4RAG优于现有的基线。</p>
<h4 id="_18">一句话总结：</h4>
<p>本文提出了一种基于层相关传播算法的LRP4RAG方法，用于检测RAG中的幻觉，并通过实验证明其优于现有基线。</p>
<hr />
<h2 id="enhancing-and-accelerating-large-language-models-via-instruction-aware-contextual-compression"><a href="http://arxiv.org/abs/2408.15491v1">Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression</a></h2>
<p>发布时间：2024-08-28</p>
<p>作者：Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, Fei Yu</p>
<h4 id="_19">中文摘要：</h4>
<p>大型语言模型（LLMs）因其在不同任务上的出色表现而受到广泛关注。然而，为了减轻幻觉问题，LLMs通常采用检索增强型管道，以提供丰富的外部知识和上下文。尽管如此，从检索器中检索到的上下文不准确且粒度粗糙，带来了挑战。向LLMs提供无关的上下文可能导致响应质量下降、推理延迟增加和成本上升。本文提出了一种名为指令感知上下文压缩的方法，该方法通过过滤掉不具信息量的内容，从而加速并增强LLMs的使用。实验结果表明，指令感知上下文压缩显著降低了内存消耗并最小化了生成延迟，同时保持了与使用完整上下文相当的性能水平。具体来说，我们实现了50%的上下文相关成本降低，导致推理内存使用量减少5%，推理速度提高2.2倍，同时Rouge-1评分仅略有下降0.047。这些发现表明，我们的方法在效率和性能之间取得了有效的平衡。</p>
<h4 id="_20">一句话总结：</h4>
<p>本文提出的指令感知上下文压缩方法有效提升了大型语言模型的使用效率，同时保持了性能水平。</p>
<hr />
<h2 id="text2sql-is-not-enough-unifying-ai-and-databases-with-tag"><a href="http://arxiv.org/abs/2408.14717v1">Text2SQL is Not Enough: Unifying AI and Databases with TAG</a></h2>
<p>发布时间：2024-08-27</p>
<p>作者：Asim Biswal, Liana Patel, Siddarth Jha, Amog Kamsetty, Shu Liu, Joseph E. Gonzalez, Carlos Guestrin, Matei Zaharia</p>
<h4 id="_21">中文摘要：</h4>
<p>本文提出了一种名为“表增强生成”（Table-Augmented Generation，简称TAG）的统一和通用范式，用于在数据库上回答自然语言问题。这种模型代表了语言模型（LM）与数据库之间广泛而之前未被探索的交互，为利用语言模型在数据上的世界知识和推理能力提供了新的研究机会。本文系统地开发了基准测试来研究TAG问题，并发现标准方法正确回答的查询不超过20%，证实了在该领域进行进一步研究的必要性。基准测试的代码已发布在https://github.com/TAG-Research/TAG-Bench。</p>
<h4 id="_22">一句话总结：</h4>
<p>本文提出了一种名为“表增强生成”的新范式，旨在提高数据库中自然语言问题的回答能力，并揭示了当前方法在处理此类问题上的局限性。</p>
<hr />
<h2 id="probing-causality-manipulation-of-large-language-models"><a href="http://arxiv.org/abs/2408.14380v1">Probing Causality Manipulation of Large Language Models</a></h2>
<p>发布时间：2024-08-26</p>
<p>作者：Chenyang Zhang, Haibo Tong, Bin Zhang, Dongyu Zhang</p>
<h4 id="_23">中文摘要：</h4>
<p>大型语言模型（LLMs）在自然语言处理领域展现出各种能力，包括因果关系问题。由于预训练模型通常基于统计关联，并不专注于句子中的因果关系，因此LLMs对因果关系的掌控并不直观。因此，探究LLMs内部对因果关系的操控是必要的。本文提出了一种新颖的方法，通过为模型提供不同的捷径并观察其行为，对因果关系操控进行分层探究。我们利用检索增强生成（RAG）和上下文学习（ICL）在设计的因果关系分类任务上对模型进行实验。我们在包括GPT-4在内的主流LLMs和一些较小、特定领域的模型上进行了实验。我们的结果表明，LLMs可以检测与因果关系相关的实体，并识别直接的因果关系。然而，LLMs缺乏对因果关系的专门认知，只是将其视为句子全局语义的一部分。</p>
<h4 id="_24">一句话总结：</h4>
<p>本文提出了一种通过检索增强生成和上下文学习来探究大型语言模型在因果关系处理能力的新方法。</p>
<hr />
<h2 id="claim-verification-in-the-age-of-large-language-models-a-survey"><a href="http://arxiv.org/abs/2408.14317v1">Claim Verification in the Age of Large Language Models: A Survey</a></h2>
<p>发布时间：2024-08-26</p>
<p>作者：Alphaeus Dmonte, Roland Oruche, Marcos Zampieri, Prasad Calyam, Isabelle Augenstein</p>
<h4 id="_25">中文摘要：</h4>
<p>随着互联网上可用的数据量越来越大，以及手动进行主张和事实核查的繁琐任务，人们开始对开发自动主张验证系统产生兴趣。近年来，已经提出了几种基于深度学习和Transformer的模型来完成这项任务。随着大型语言模型（LLMs）的引入以及它们在多个自然语言处理（NLP）任务中的卓越表现，我们看到基于LLMs的主张验证方法以及使用检索增强生成（RAG）等新颖方法的激增。在这篇综述中，我们详细介绍了使用LLMs的最近主张验证框架。我们详细描述了这些框架中使用的主张验证管道的不同组件，包括检索、提示和微调的常见方法。最后，我们描述了为这项任务创建的公开可用的英文数据集。</p>
<h4 id="_26">一句话总结：</h4>
<p>本文综述了基于大型语言模型的主张验证框架，详细介绍了其组件和检索、提示及微调等关键技术。</p>
<hr />
<h2 id="retrieval-augmented-generation-for-dynamic-graph-modeling"><a href="http://arxiv.org/abs/2408.14523v1">Retrieval Augmented Generation for Dynamic Graph Modeling</a></h2>
<p>发布时间：2024-08-26</p>
<p>作者：Yuxia Wu, Yuan Fang, Lizi Liao</p>
<h4 id="_27">中文摘要：</h4>
<p>动态图建模对于分析各种应用中的演变模式至关重要。现有的方法通常将图神经网络与时间模块相结合，或将动态图建模重新定义为生成序列任务。然而，这些方法通常依赖于目标节点从狭隘视角的孤立历史上下文，忽略了与其他节点相关的类似模式或相关案例的发生。在本工作中，我们引入了检索增强生成用于动态图建模（RAG4DyG）框架，该框架利用上下文和时间上类似示例的指导来拓宽每个节点的视角。这种方法提出了两个关键挑战：（1）如何识别和检索与动态图样本在上下文和时间上类似的高质量示例？（2）如何有效地整合这些示例以改进动态图建模？为了解决这些挑战，我们提出了RAG4DyG，它通过检索和学习上下文和时间相关的示例来丰富对历史上下文的理解。具体来说，我们采用了一个时间和上下文感知的对比学习模块来识别和检索每个查询序列的相关案例。此外，我们设计了一种图融合策略来整合检索到的案例，从而增强固有的历史上下文以改善预测。在多个领域的真实世界数据集上的大量实验证明了RAG4DyG在动态图建模中的有效性。</p>
<h4 id="_28">一句话总结：</h4>
<p>RAG4DyG通过检索和利用上下文和时间上类似的历史案例，有效提升了动态图建模的性能。</p>
<hr />
<h2 id="evaluating-chatgpt-on-nuclear-domain-specific-data"><a href="http://arxiv.org/abs/2409.00090v1">Evaluating ChatGPT on Nuclear Domain-Specific Data</a></h2>
<p>发布时间：2024-08-26</p>
<p>作者：Muhammad Anwar, Mischa de Costa, Issam Hammad, Daniel Lau</p>
<h4 id="_29">中文摘要：</h4>
<p>本文探讨了大型语言模型（LLM）ChatGPT在核数据这一高度专业领域中的问答（Q&amp;A）任务应用。主要关注评估ChatGPT在精心挑选的测试数据集上的表现，比较独立LLM生成的结果与通过检索增强生成（RAG）方法生成的结果。尽管LLM近年来取得了显著进展，但它们容易生成错误或“幻觉”信息，这在需要高准确性和可靠性的应用中是一个重大限制。本研究探讨了在LLM中利用RAG的潜力，这是一种结合外部知识库和复杂的检索技术以增强生成输出的准确性和相关性的方法。在此背景下，本文评估了ChatGPT回答特定领域问题的能力，采用了两种方法：A）直接从LLM获取回答，B）在RAG框架内从LLM获取回答。这些方法的有效性通过人类和LLM的双重评估机制进行评估，对回答的正确性和其他指标进行评分。研究发现，在LLM中引入RAG管道可以显著提高性能，特别是在生成更准确和上下文相关的核领域特定查询回答方面。此外，本文还强调了进一步改进此类专业领域答案质量的替代方法。</p>
<h4 id="_30">一句话总结：</h4>
<p>本文评估了ChatGPT在核数据问答任务中的应用，发现结合RAG技术的LLM能够显著提高回答的准确性和相关性。</p>
<hr />
<h2 id="lowclip-adapting-the-clip-model-architecture-for-low-resource-languages-in-multimodal-image-retrieval-task"><a href="http://arxiv.org/abs/2408.13909v1">LowCLIP: Adapting the CLIP Model Architecture for Low-Resource Languages in Multimodal Image Retrieval Task</a></h2>
<p>发布时间：2024-08-25</p>
<p>作者：Ali Asgarov, Samir Rustamov</p>
<h4 id="_31">中文摘要：</h4>
<p>本研究探讨了在低资源语言（尤其是阿塞拜疆语）中开发多模态视觉-语言模型以进行图像检索的发展。现有的视觉-语言模型主要支持高资源语言，而对其进行微调在计算上仍然具有挑战性。为了解决低资源语言视觉-语言检索的挑战，我们集成了CLIP模型架构，并采用了几种技术来平衡计算效率与性能。这些技术包括通过机器翻译生成合成数据、图像增强以及使用特定领域的数据进一步训练基于transformer的模型的注意力机制。我们将Multilingual BERT作为文本编码器与ResNet50、EfficientNet0、Vision Transformer（ViT）和Tiny Swin Transformer等图像编码器相结合。我们的研究发现，EfficientNet0和Tiny Swin Transformer等模型在它们所训练的数据集（如COCO、Flickr30k和Flickr8k）上表现最佳。增强技术将EfficientNet0在Flickr30k上的MAP从0.84提升到0.87，将ResNet50在MSCOCO上的MAP从0.70提升到0.80，为视觉-语言检索创造了新的最佳水平。我们分享了我们的配置和结果以支持进一步的研究。代码和预训练模型可在https://github.com/aliasgerovs/azclip上获取。</p>
<h4 id="_32">一句话总结：</h4>
<p>本研究通过集成CLIP模型架构和采用多种技术，成功提升了低资源语言（如阿塞拜疆语）的视觉-语言检索性能。</p>
<hr />
<h2 id="biomedical-large-languages-models-seem-not-to-be-superior-to-generalist-models-on-unseen-medical-data"><a href="http://arxiv.org/abs/2408.13833v1">Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data</a></h2>
<p>发布时间：2024-08-25</p>
<p>作者：Felix J. Dorfner, Amin Dada, Felix Busch, Marcus R. Makowski, Tianyu Han, Daniel Truhn, Jens Kleesiek, Madhumita Sushil, Jacqueline Lammert, Lisa C. Adams, Keno K. Bressem</p>
<h4 id="_33">中文摘要：</h4>
<p>大型语言模型（LLMs）在生物医学应用中展现出潜力，促使研究人员在特定领域的数据上对其进行微调。然而，这种方法的有效性尚不明确。本研究评估了在多种临床任务上，生物医学微调的LLMs与其通用型对应模型的表现。我们评估了它们在《新英格兰医学杂志》（NEJM）和《美国医学会杂志》（JAMA）的临床案例挑战以及多个临床任务（例如信息提取、文档摘要和临床编码）上的表现。使用专门选择的、可能超出生物医学模型微调数据集的基准，我们发现生物医学LLMs在大多数情况下表现不如其通用型对应模型，尤其是在不专注于医学知识的任务上。虽然较大的模型在案例任务上表现出相似的性能（例如，OpenBioLLM-70B：66.4% vs. Llama-3-70B-Instruct：65%在JAMA案例中），但较小的生物医学模型表现不佳更为明显（例如，OpenBioLLM-8B：30% vs. Llama-3-8B-Instruct：64.3%在NEJM案例中）。在CLUE（临床语言理解评估）基准任务中也观察到了类似趋势，通用型模型在文本生成、问答和编码任务上通常表现更好。我们的结果表明，将LLMs微调到生物医学数据可能不会带来预期的益处，甚至可能导致性能下降，挑战了关于LLMs领域特定适应的普遍假设，并突出了在医疗保健AI中需要更严格的评估框架。检索增强生成等替代方法可能在增强LLMs的生物医学能力的同时，不损害其通用知识。</p>
<h4 id="_34">一句话总结：</h4>
<p>本研究发现，将LLMs微调到生物医学数据可能不会提升其性能，甚至可能降低其表现，挑战了LLMs领域特定适应的普遍观点。</p>
<hr />
<h2 id="towards-reliable-medical-question-answering-techniques-and-challenges-in-mitigating-hallucinations-in-language-models"><a href="http://arxiv.org/abs/2408.13808v1">Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models</a></h2>
<p>发布时间：2024-08-25</p>
<p>作者：Duy Khoa Pham, Bao Quoc Vo</p>
<h4 id="_35">中文摘要：</h4>
<p>本文对现有技术进行了范围研究，以减轻大型语言模型（LLMs）在知识型任务中，尤其是在医疗领域中的幻觉现象。这些技术包括基于检索增强生成（RAG）的技术、迭代反馈循环、监督微调和提示工程。尽管这些技术在一般环境中具有前景，但由于医疗领域对最新、专业知识的独特需求以及对医疗指南的严格遵循，这些技术需要进一步适应和优化。解决这些挑战对于开发可信赖的人工智能系统至关重要，这些系统能够提高临床决策和患者安全，以及生物医学科学研究的准确性。</p>
<h4 id="_36">一句话总结：</h4>
<p>本文探讨了减轻大型语言模型在医疗领域知识型任务中幻觉现象的现有技术，并强调了针对医疗领域需求进行优化的重要性。</p>
<hr />
<h2 id="colberts-mask-based-query-augmentation-effects-of-quadrupling-the-query-input-length"><a href="http://arxiv.org/abs/2408.13672v1">ColBERT's [MASK]-based Query Augmentation: Effects of Quadrupling the Query Input Length</a></h2>
<p>发布时间：2024-08-24</p>
<p>作者：Ben Giacalone, Richard Zanibbi</p>
<h4 id="_37">中文摘要：</h4>
<p>ColBERT的独特之处在于其在查询中使用（ColBERT）token对文档进行评分（查询增强）。先前的研究表明，（ColBERT）token对非查询词进行加权，强调某些token而忽略其他token，而不是像最初提出的那样引入全新的term。我们首先证明了之前在ColBERTv1中报告的（ColBERT）term加权行为在ColBERTv2中也成立。然后，我们考察了将（ColBERT）token的数量从零增加到训练中使用的查询输入长度的四倍对第一阶段检索和候选评分的影响，观察到随着（ColBERT）token数量的增加，性能最初有所下降，当添加足够的（ColBERT）token使查询长度达到平均32个token时，性能大幅提升，之后性能趋于平稳。此外，我们将基线性能与查询长度扩展到128个token时的性能进行了比较，发现差异很小（例如，在各种指标上差异在1%以内）且通常不具有统计学意义，表明如果ColBERT遇到比预期更多的（ColBERT）token，其性能不会崩溃。</p>
<h4 id="_38">一句话总结：</h4>
<p>本研究探讨了ColBERT中（ColBERT）token的使用对文档评分性能的影响，发现适当增加（ColBERT）token数量可以显著提升性能，但过多的token并不会导致性能下降。</p>
<hr />
<h2 id="towards-human-level-understanding-of-complex-process-engineering-schematics-a-pedagogical-introspective-multi-agent-framework-for-open-domain-question-answering"><a href="http://arxiv.org/abs/2409.00082v1">Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering</a></h2>
<p>发布时间：2024-08-24</p>
<p>作者：Sagar Srinivas Sakhinana, Geethan Sannidhi, Venkataramana Runkana</p>
<h4 id="_39">中文摘要：</h4>
<p>在化工和流程工业中，工艺流程图（PFDs）和管道与仪表图（P&amp;IDs）对于设计、建设和维护至关重要。近年来，生成式人工智能（Generative AI）的进步，如大型多模态模型（LMMs）GPT4（Omni），在理解和解释工艺流程图以进行视觉问答（VQA）方面展现出潜力。然而，专有模型存在数据隐私风险，其计算复杂性限制了在消费级硬件上进行特定领域定制化知识编辑。为了克服这些挑战，我们提出了一种安全、本地部署的企业级解决方案，采用分层、多智能体检索增强生成（RAG）框架，用于开放域问答（ODQA）任务，提供增强的数据隐私、可解释性和成本效益。我们的新颖多智能体框架采用内省和专门的子智能体，使用开源、小规模的多模态模型，结合ReAct（Reason+Act）提示技术对PFD和P&amp;ID进行分析，整合多个信息源以提供准确和上下文相关的答案。我们的方法通过迭代自我校正支持，旨在在ODQA任务中实现卓越的性能。我们进行了严格的实验研究，经验结果验证了所提出方法的有效性。</p>
<h4 id="_40">一句话总结：</h4>
<p>本研究提出了一种基于多智能体框架的本地化企业级解决方案，用于提高化工和流程工业中工艺流程图分析的数据隐私、可解释性和成本效益。</p>
<hr />
<h2 id="pandoras-box-or-aladdins-lamp-a-comprehensive-analysis-revealing-the-role-of-rag-noise-in-large-language-models"><a href="http://arxiv.org/abs/2408.13533v1">Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models</a></h2>
<p>发布时间：2024-08-24</p>
<p>作者：Jinyang Wu, Feihu Che, Chuyuan Zhang, Jianhua Tao, Shuai Zhang, Pengpeng Shao</p>
<h4 id="_41">中文摘要：</h4>
<p>检索增强生成（RAG）已成为解决大型语言模型（LLMs）中幻觉问题的关键方法。尽管最近的研究将RAG模型扩展到复杂的噪声场景，但这些探索通常局限于有限的噪声类型，并假设噪声本质上对LLMs有害，这可能导致与真实世界的检索环境不符，并限制其实际应用。在本文中，我们从语言学的角度定义了七种不同的噪声类型，并建立了噪声RAG基准（NoiserBench），这是一个包含多个数据集和推理任务的全面评估框架。通过对八种具有不同架构和规模的代表性LLMs进行实证评估，我们发现这些噪声可以进一步分为两组实用的噪声：对LLMs有益的噪声（又称有益噪声）和对LLMs有害的噪声（又称有害噪声）。虽然有害噪声通常会损害性能，但有益噪声可能会增强模型能力的多个方面和整体性能。我们的分析为开发更稳健、适应性强的RAG解决方案以及减轻不同检索场景中的幻觉提供了见解。</p>
<h4 id="_42">一句话总结：</h4>
<p>本文通过定义七种噪声类型并建立噪声RAG基准，揭示了噪声对LLMs性能的双重影响，为开发更鲁棒的RAG解决方案提供了新的视角。</p>
<hr />
<h2 id="vitality-2-reviewing-academic-literature-using-large-language-models"><a href="http://arxiv.org/abs/2408.13450v1">vitaLITy 2: Reviewing Academic Literature Using Large Language Models</a></h2>
<p>发布时间：2024-08-24</p>
<p>作者：Hongye An, Arpit Narechania, Emily Wall, Kai Xu</p>
<h4 id="_43">中文摘要：</h4>
<p>学术文献综述传统上依赖于关键词搜索和积累相关参考文献的技术，使用如谷歌学术或IEEE Xplore等数据库。然而，这些搜索技术的精确性和准确性受到特定关键词是否存在的影响，使得文献综述类似于在稻草堆里找针。我们提出了vitaLITy 2，这是一种利用大型语言模型（LLM）方法在文本嵌入空间中识别语义相关文献的解决方案。我们包含了一个从1970年到2023年的66,692篇论文的语料库，这些论文可以通过由三个语言模型创建的文本嵌入进行搜索。vitaLITy 2贡献了一种新的检索增强生成（RAG）架构，并且可以通过具有增强提示的大型语言模型进行交互，包括对一系列论文的总结。vitaLITy 2还提供了一个聊天界面，允许用户在不学习任何新编程语言的情况下执行复杂查询。这也使用户能够利用LLM从其庞大的训练语料库中捕获的知识。最后，我们通过两个使用场景展示了vitaLITy 2的应用性。vitaLITy 2作为开源软件可在https://vitality-vis.github.io获取。</p>
<h4 id="_44">一句话总结：</h4>
<p>vitaLITy 2通过大型语言模型和文本嵌入技术，提供了一种高效、易用的学术文献综述工具。</p>
<hr />
<h2 id="coderefine-a-pipeline-for-enhancing-llm-generated-code-implementations-of-research-papers"><a href="http://arxiv.org/abs/2408.13366v1">CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers</a></h2>
<p>发布时间：2024-08-23</p>
<p>作者：Ekaterina Trofimova, Emil Sataev, Abhijit Singh Jowhari</p>
<h4 id="_45">中文摘要：</h4>
<p>本文提出了一种名为CodeRefine的新框架，该框架利用大型语言模型（LLMs）将研究论文的方法自动转换为功能性代码。该多步骤方法首先从论文中提取和总结关键文本片段，分析其代码相关性，并使用预定义的本体论创建知识图谱。然后，从这种结构化表示中生成代码，并通过提出的回顾性检索增强生成方法进行增强。CodeRefine解决了将理论研究与实际应用相结合的挑战，为LLM零样本提示提供了一种更准确的替代方案。对各种科学论文的评估表明，CodeRefine能够改进论文中的代码实现，可能加速尖端算法在现实世界应用中的采用。</p>
<h4 id="_46">一句话总结：</h4>
<p>CodeRefine通过利用大型语言模型和知识图谱技术，实现了将研究论文中的方法论自动转换为功能性代码，从而加速了前沿算法在实际应用中的采纳。</p>
<hr />
<h2 id="knowledge-graph-modeling-driven-large-language-model-operating-system-llm-os-for-task-automation-in-process-engineering-problem-solving"><a href="http://arxiv.org/abs/2408.14494v1">Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving</a></h2>
<p>发布时间：2024-08-23</p>
<p>作者：Sakhinana Sagar Srinivas, Vijay Sri Vaikunth, Venkataramana Runkana</p>
<h4 id="_47">中文摘要：</h4>
<p>本文提出了一种名为过程工程操作助手（PEOA）的人工智能驱动框架，旨在解决化工和流程工业中的复杂问题。该框架采用模块化架构，由一个元代理进行协调，作为中央协调器，管理动作生成器和指令调整的小规模语言模型（专家模型）。动作生成器将复杂问题分解为子任务，并为每个子任务识别合适的专家模型来执行，从而为多步骤问题解决提供精确的解决方案。关键技术包括使用属性图进行高级知识建模，以改进信息检索，促进更准确和上下文相关的解决方案。此外，该框架利用GPT-4（Omni）的教师-学生迁移学习方法来微调动作生成器和专家模型以实现领域适应性，同时结合复杂的错误处理机制。开发了定制数据集以评估该框架在各种工程任务上与领先专有语言模型的性能。结果表明，该框架在自动化计算、加速原型设计和为工业过程提供人工智能增强决策支持方面具有显著效果，标志着过程工程能力的重大进步。</p>
<h4 id="_48">一句话总结：</h4>
<p>该研究开发了一种名为PEOA的人工智能框架，用于解决化工和流程工业中的复杂问题，显著提升了过程工程的能力。</p>
<hr />
<h2 id="in-context-learning-with-reinforcement-learning-for-incomplete-utterance-rewriting"><a href="http://arxiv.org/abs/2408.13028v1">In-Context Learning with Reinforcement Learning for Incomplete Utterance Rewriting</a></h2>
<p>发布时间：2024-08-23</p>
<p>作者：Haowei Du, Dongyan Zhao</p>
<h4 id="_49">中文摘要：</h4>
<p>在上下文学习（In-context learning，ICL）领域，大型语言模型（Large Language Models，LLMs）仅基于少量示例增强的指令进行预测，这一领域引起了越来越多的关注。现有的ICL示例选择方法利用稀疏或密集检索器，并取得了有效的性能。然而，这些方法没有利用LLM的直接反馈来训练检索器和选定的示例，不一定能提高LLM的类比能力。为了解决这个问题，我们提出了基于策略强化学习（Policy-based reinforcement learning，RLS）的示例选择框架，该框架包括一个语言模型（Language Model，LM）选择器和LLM生成器。LM选择器将候选示例编码成密集表示，并选择前k个示例作为LLM的演示。LLM的输出被用来计算奖励和政策梯度，以优化LM选择器。我们在不同的数据集上进行了实验，并显著优于现有的示例选择方法。此外，我们的方法在少样本设置中显示出优于监督微调（Supervised Fine-tuning，SFT）模型的优势。进一步的实验表明，示例的丰富性和与测试案例的相似性对于LLM的ICL性能至关重要。</p>
<h4 id="_50">一句话总结：</h4>
<p>本研究提出了一种基于策略强化学习的示例选择框架，有效提升了大型语言模型在上下文学习中的性能。</p>
<hr />
<h2 id="a-comparative-analysis-of-faithfulness-metrics-and-humans-in-citation-evaluation"><a href="http://arxiv.org/abs/2408.12398v1">A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation</a></h2>
<p>发布时间：2024-08-22</p>
<p>作者：Weijia Zhang, Mohammad Aliannejadi, Jiahuan Pei, Yifei Yuan, Jia-Hong Huang, Evangelos Kanoulas</p>
<h4 id="_51">中文摘要：</h4>
<p>大型语言模型（LLMs）在生成内容时常常会出现不支持或无法验证的内容，这种现象被称为“幻觉”。为了解决这个问题，检索增强型LLMs被采用，它们在内容中包含引用，将内容基于可验证的来源。尽管有了这样的发展，但手动评估引用对相关陈述的支持程度仍然是一个主要挑战。先前的研究通过利用忠实度指标来自动估计引用支持，来应对这一挑战。然而，它们将这种引用支持估计限制在二元分类场景中，忽略了实际场景中细粒度的引用支持。为了调查忠实度指标在细粒度场景中的有效性，我们提出了一种比较评估框架，该框架评估了指标在区分三个类别支持水平（完全支持、部分支持和无支持）之间的引用时的有效性。我们的框架采用相关性分析、分类评估和检索评估来全面衡量指标分数与人类判断之间的对齐程度。我们的结果表明，没有单一的指标在所有评估中都能持续表现出色，突显了准确评估细粒度支持水平的复杂性。特别是，我们发现表现最好的指标在区分部分支持与完全支持或无支持方面存在困难。基于这些发现，我们提供了关于开发更有效指标的实用建议。</p>
<h4 id="_52">一句话总结：</h4>
<p>本研究提出了一种评估大型语言模型引用支持有效性的框架，发现现有指标在区分细粒度支持水平时存在局限性，并提出了改进建议。</p>
<hr />
<h2 id="garmentaligner-text-to-garment-generation-via-retrieval-augmented-multi-level-corrections"><a href="http://arxiv.org/abs/2408.12352v2">GarmentAligner: Text-to-Garment Generation via Retrieval-augmented Multi-level Corrections</a></h2>
<p>发布时间：2024-08-22</p>
<p>作者：Shiyue Zhang, Zheng Chong, Xujie Zhang, Hanhui Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</p>
<h4 id="_53">中文摘要：</h4>
<p>本文提出了一种名为GarmentAligner的文本到服装扩散模型，该模型通过检索增强的多级校正进行训练，以解决在服装生成中，即使是先进的文本到图像模型也难以实现细粒度语义对齐的问题，特别是在服装部件的数量、位置和相互关系方面。为了在部件级别实现语义对齐，我们引入了一个自动部件提取流程，从相应的图像和标题中获取服装部件的空间和定量信息。随后，为了利用服装图像中的部件关系，我们通过基于部件级别相似性排名的检索增强为每个服装构建检索子集，并执行对比学习以增强模型对正负样本中部件的感知。为了进一步增强跨语义、空间和定量粒度的部件对齐，我们提出了利用多级校正损失的方案，该方案利用详细的部件信息。实验结果表明，与现有竞争者相比，GarmentAligner实现了更高的保真度和细粒度语义对齐。</p>
<h4 id="_54">一句话总结：</h4>
<p>GarmentAligner通过多级校正和检索增强，实现了服装生成中的细粒度语义对齐，显著提升了文本到服装模型的性能。</p>
<hr />
<h2 id="graph-retrieval-augmented-trustworthiness-reasoning"><a href="http://arxiv.org/abs/2408.12333v2">Graph Retrieval Augmented Trustworthiness Reasoning</a></h2>
<p>发布时间：2024-08-22</p>
<p>作者：Ying Zhu, Shengchang Li, Ziqian Kong, Peilan Xu</p>
<h4 id="_55">中文摘要：</h4>
<p>在信息不完整的多玩家游戏中，可信度推理至关重要，它使智能体能够识别潜在的盟友和对手，从而增强推理和决策过程。传统的依赖于预训练模型的方法需要大量的领域特定数据和大量的奖励反馈，而缺乏实时适应性限制了它们在动态环境中的有效性。在本文中，我们引入了图检索增强推理（GRATR）框架，利用检索增强生成（RAG）技术来增强智能体的可信度推理。GRATR构建了一个动态可信度图，实时更新它以包含证据信息，并检索相关的可信度数据以增强大型语言模型（LLMs）的推理能力。我们通过在多人游戏“狼人杀”上的实验验证了我们的方法，将GRATR与基线LLM以及使用原生RAG和重排序RAG增强的LLM进行了比较。我们的结果表明，GRATR在胜率上超过了基线方法超过30%，具有优越的推理性能。此外，GRATR有效地减轻了LLM的幻觉，如身份和目标健忘，并且关键的是，它通过使用可信度图使推理过程更加透明和可追溯。</p>
<h4 id="_56">一句话总结：</h4>
<p>本文提出的GRATR框架通过构建动态可信度图和利用RAG技术，显著提升了多玩家游戏中的可信度推理能力，提高了胜率和推理透明度。</p>
<hr />
<h2 id="llms-are-not-zero-shot-reasoners-for-biomedical-information-extraction"><a href="http://arxiv.org/abs/2408.12249v1">LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction</a></h2>
<p>发布时间：2024-08-22</p>
<p>作者：Aishik Nagar, Viktor Schlegel, Thanh-Tung Nguyen, Hao Li, Yuping Wu, Kuluhan Binici, Stefan Winkler</p>
<h4 id="_57">中文摘要：</h4>
<p>大型语言模型（LLMs）在医疗保健领域的应用日益广泛，在问答和文档摘要等任务上已达到领域专家的水平。尽管在这些任务上取得了成功，但在生物医学领域传统追求的任务，如结构化信息提取，LLMs的表现如何尚不清楚。为了填补这一差距，本文系统地评估了LLMs在医学分类和命名实体识别（NER）任务上的性能。我们旨在分析不同因素对性能的贡献，特别是LLMs的任务知识、推理能力、（参数化）领域知识以及外部知识的添加的影响。为此，我们评估了各种开源LLMs（包括BioMistral和Llama-2模型）在一系列生物医学数据集上的表现，使用了标准提示、思维链（CoT）和基于自我一致性的推理，以及PubMed和维基百科语料库的检索增强生成（RAG）。出人意料的是，我们的结果表明，在两项任务中，标准提示始终优于更复杂的技术，揭示了当前在生物医学领域应用CoT、自我一致性和RAG的局限性。我们的发现表明，为知识密集型或推理密集型任务（如CoT或RAG）开发的先进提示方法并不容易移植到需要精确结构化输出的生物医学任务中。这突显了在LLMs中更有效地整合外部知识和推理机制的需求，以增强其在现实世界生物医学应用中的性能。</p>
<h4 id="_58">一句话总结：</h4>
<p>本文评估了大型语言模型在生物医学领域的性能，发现标准提示方法优于更复杂的推理技术，并强调了在LLMs中整合外部知识和推理机制的重要性。</p>
<hr />
<h2 id="rag-optimized-tibetan-tourism-llms-enhancing-accuracy-and-personalization"><a href="http://arxiv.org/abs/2408.12003v1">RAG-Optimized Tibetan Tourism LLMs: Enhancing Accuracy and Personalization</a></h2>
<p>发布时间：2024-08-21</p>
<p>作者：Jinhu Qi, Shuai Yan, Yibo Zhang, Wentao Zhang, Rong Jin, Yuwei Hu, Ke Wang</p>
<h4 id="_59">中文摘要：</h4>
<p>随着现代社会经济的发展，旅游业已成为满足人们精神需求的重要途径，为旅游业带来了发展机遇。然而，现有的大型语言模型（LLMs）在个性化推荐能力和内容生成方面面临挑战，有时甚至会产生幻觉。本研究基于检索增强生成（RAG）技术，提出了一种针对西藏旅游LLMs的优化方案。通过构建旅游观点数据库并使用向量化技术处理数据，我们显著提高了检索准确性。RAG技术的应用有效地解决了内容生成中的幻觉问题。优化后的模型在内容生成的流畅性、准确性和相关性方面均有显著提升。这项研究展示了RAG技术在文化旅游信息标准化和数据分析中的潜力，为智能文化旅游服务系统的发展提供了理论和技术支持。</p>
<h4 id="_60">一句话总结：</h4>
<p>本研究基于RAG技术优化了西藏旅游LLMs，有效提升了内容生成的质量，为智能文化旅游服务系统的发展提供了理论和技术支持。</p>
<hr />
<h2 id="ancient-wisdom-modern-tools-exploring-retrieval-augmented-llms-for-ancient-indian-philosophy"><a href="http://arxiv.org/abs/2408.11903v2">Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented LLMs for Ancient Indian Philosophy</a></h2>
<p>发布时间：2024-08-21</p>
<p>作者：Priyanka Mandikal</p>
<h4 id="_61">中文摘要：</h4>
<p>大型语言模型（LLMs）已经彻底改变了信息检索和知识传播的格局。然而，它们在特定领域的应用常常受到事实错误和幻觉的限制，尤其是在长尾知识分布中。我们探讨了检索增强生成（RAG）模型在特定知识领域长文本问答（LFQA）中的潜力。我们呈现了VedantaNY-10M，这是一个从关于古代印度哲学Advaita Vedanta的广泛公开讨论中精心制作的语料库。我们开发并基准测试了一个RAG模型，与一个标准的非RAG LLM进行比较，重点关注转录、检索和生成性能。计算语言学家和领域专家的人工评估表明，RAG模型在生成事实性和全面性回答且幻觉较少方面显著优于标准模型。此外，一个基于关键词的混合检索器，强调独特的低频术语，进一步提高了结果。我们的研究为有效地将现代大型语言模型与古代知识系统相结合提供了见解。</p>
<h4 id="_62">一句话总结：</h4>
<p>本研究提出了一种基于RAG的模型，用于在特定知识领域内进行长文本问答，显著提高了问答的准确性，并为将现代大型语言模型与古代知识系统相结合提供了新思路。</p>
<hr />
<h2 id="permitqa-a-benchmark-for-retrieval-augmented-generation-in-wind-siting-and-permitting-domain"><a href="http://arxiv.org/abs/2408.11800v1">PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting and Permitting domain</a></h2>
<p>发布时间：2024-08-21</p>
<p>作者：Rounak Meyur, Hung Phan, Sridevi Wagle, Jan Strube, Mahantesh Halappanavar, Sameera Horawalavithana, Anurag Acharya, Sai Munikoti</p>
<h4 id="_63">中文摘要：</h4>
<p>在自然语言处理（NLP）和文本生成快速发展的领域中，检索增强生成（RAG）的出现为通过利用从用户指定的数据库中检索到的信息来提高生成文本的质量和可靠性提供了一条有前景的途径。基准测试对于评估和比较不同RAG配置（包括检索器和生成器）的性能至关重要，这有助于了解它们的有效性、可扩展性和针对特定领域和应用的适用性。在本文中，我们提出了一种综合框架来生成一个与特定领域相关的RAG基准。我们的框架基于自动问答生成，结合了人类（领域专家）与人工智能大型语言模型（LLM）的合作。作为一个案例研究，我们通过介绍PermitQA，这是第一个关于风力场选址和许可领域的基准，它包含了与风力能源项目环境影响相关的多个科学文献/报告，来展示我们的框架。我们的框架系统地使用多种指标和不同复杂程度的多种问题类型来评估RAG的性能。我们还展示了不同模型在我们基准上的性能。</p>
<h4 id="_64">一句话总结：</h4>
<p>本文提出了一种基于人类与AI合作的自动问答生成框架，用于评估风力场选址和许可领域的RAG基准，并通过多种指标和问题类型展示了不同模型的性能。</p>
<hr />
<h2 id="leveraging-chemistry-foundation-models-to-facilitate-structure-focused-retrieval-augmented-generation-in-multi-agent-workflows-for-catalyst-and-materials-design"><a href="http://arxiv.org/abs/2408.11793v1">Leveraging Chemistry Foundation Models to Facilitate Structure Focused Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and Materials Design</a></h2>
<p>发布时间：2024-08-21</p>
<p>作者：Nathaniel H. Park, Tiffany J. Callahan, James L. Hedrick, Tim Erdmann, Sara Capponi</p>
<h4 id="_65">中文摘要：</h4>
<p>通过深度学习模型进行分子性质预测和生成式设计一直是研究的热点，因为其有望加速新型高性能材料的开发。最近，随着大型语言模型（LLMs）和能够利用预训练模型在更复杂的研究任务中进行预测的LLM驱动代理系统的出现，这些工作流程得到了显著增强。尽管如此，在代理系统中检索对材料设计任务至关重要的关键信息方面，仍有很大的改进空间。此外，预测深度学习模型的替代用途，例如利用其潜在表示来促进代理系统中的跨模态检索增强生成，以实现特定任务的材料设计，仍处于未探索状态。在此，我们展示了大型预训练化学基础模型可以作为实现小分子、复杂聚合物材料和反应的语义化学信息检索的基础。此外，我们还展示了化学基础模型与图像模型（如OpenCLIP）结合使用，在多个表征数据域中促进前所未有的查询和信息检索。最后，我们展示了这些系统在多代理系统中的集成，以促进复杂研究任务的结构和拓扑基于的自然语言查询和信息检索。</p>
<h4 id="_66">一句话总结：</h4>
<p>本文提出了一种基于大型预训练化学基础模型的方法，通过自然语言查询和信息检索，加速复杂材料设计任务的研究。</p>
<hr />
<h2 id="leveraging-fine-tuned-retrieval-augmented-generation-with-long-context-support-for-3gpp-standards"><a href="http://arxiv.org/abs/2408.11775v1">Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards</a></h2>
<p>发布时间：2024-08-21</p>
<p>作者：Omar Erak, Nouf Alabbasi, Omar Alhussein, Ismail Lotfi, Amr Hussein, Sami Muhaidat, Merouane Debbah</p>
<h4 id="_67">中文摘要：</h4>
<p>近期研究表明，大型语言模型（LLMs）在电信领域的标准化技术方面存在困难。我们提出了一种基于Phi-2小型语言模型（SLM）的微调检索增强生成（RAG）系统，作为通信网络的咨询工具。我们开发的系统利用前瞻性语义分块技术，根据嵌入相似度自适应地确定解析断点，从而有效处理多种文档格式。为了应对技术标准中多个相似上下文的挑战，我们采用了一种重排序算法，优先检索最相关的片段。鉴于Phi-2的小型上下文窗口限制，我们实施了一种最新的技术，即SelfExtend，在推理过程中扩展上下文窗口，这不仅提升了性能，还能满足从客户到专业技术人员更广泛的查询和设计要求。在微调方面，我们利用低秩适应（LoRA）技术提高训练过程中的计算效率，并允许在小型数据集上进行有效的微调。我们的全面实验表明，在电信领域，我们的方法在现有问答方法的基础上取得了显著的改进，性能超过了GPT-4等大型语言模型（其大小约为GPT-4的880倍）。这项工作提出了利用SLMs为通信网络提供一种平衡效率与性能的新方法，这可以作为构建网络代理语言模型的基础。</p>
<h4 id="_68">一句话总结：</h4>
<p>本研究提出了一种基于Phi-2 SLM的RAG系统，用于解决大型语言模型在电信技术标准处理中的挑战，实现了高效且性能优越的通信网络问答服务。</p>
<hr />
<h2 id="xinyu-an-efficient-llm-based-system-for-commentary-generation"><a href="http://arxiv.org/abs/2408.11609v2">Xinyu: An Efficient LLM-based System for Commentary Generation</a></h2>
<p>发布时间：2024-08-21</p>
<p>作者：Yiquan Wu, Bo Tang, Chenyang Xi, Yu Yu, Pengyu Wang, Yifei Liu, Kun Kuang, Haiying Deng, Zhiyu Li, Feiyu Xiong, Jie Hu, Peng Cheng, Zhonghao Wang, Yi Wang, Yi Luo, Mingchuan Yang</p>
<h4 id="_69">中文摘要：</h4>
<p>评论文章通过呈现多样化的论点和证据，为读者提供对事件的深入理解。然而，即使是经验丰富的评论员，创作评论也是一个耗时的工作。大型语言模型（LLMs）简化了自然语言生成的过程，但它们在评论创作中的直接应用仍面临挑战，这主要是因为独特的任务要求。这些要求可以分为两个层次：1）基本要求，包括创建结构良好且逻辑一致的叙述；2）高级要求，涉及生成高质量的论点和提供令人信服的证据。在本文中，我们介绍了Xinyu，这是一个基于LLM的高效系统，旨在协助评论员生成中文评论。为了满足基本要求，我们将生成过程分解为一系列步骤，并为每个步骤提出针对性的策略和监督微调（SFT）。为了解决高级要求，我们提出了一种论点排序模型，并建立了一个包含最新事件和经典书籍的全面证据数据库，通过检索增强生成（RAG）技术加强证据的佐证。为了更公平地评估生成的评论，针对两个层次的要求，我们引入了一个综合评估指标，该指标考虑了评论生成中的五个不同视角。我们的实验证实了我们提出系统的有效性。我们还观察到，在现实场景中，评论员的效率显著提高，平均创作评论的时间从4小时缩短到20分钟。重要的是，这种效率的提高并没有降低评论的质量。</p>
<h4 id="_70">一句话总结：</h4>
<p>本文提出了一种基于LLM的评论生成系统Xinyu，通过优化生成过程和引入证据数据库，显著提高了评论员生成中文评论的效率，同时保持了评论质量。</p>
<hr />
<h2 id="t2vindexer-a-generative-video-indexer-for-efficient-text-video-retrieval"><a href="http://arxiv.org/abs/2408.11432v1">T2VIndexer: A Generative Video Indexer for Efficient Text-Video Retrieval</a></h2>
<p>发布时间：2024-08-21</p>
<p>作者：Yili Li, Jing Yu, Keke Gai, Bang Liu, Gang Xiong, Qi Wu</p>
<h4 id="_71">中文摘要：</h4>
<p>当前文本-视频检索方法主要依赖于查询与视频之间的跨模态匹配来计算它们的相似度得分，然后对这些得分进行排序以获得检索结果。这种方法考虑了每个候选视频与查询之间的匹配，但会带来显著的时间成本，并且随着候选视频数量的增加而显著增加。生成模型在自然语言处理和计算机视觉中很常见，并且在文档检索中得到了成功的应用，但它们在多模态检索中的应用尚未得到探索。为了提高检索效率，在本文中，我们引入了一个基于模型的视频索引器T2VIndexer，它是一个序列到序列的生成模型，可以直接生成视频标识符并使用恒定的时间复杂度检索候选视频。T2VIndexer旨在在保持高准确率的同时减少检索时间。为了实现这一目标，我们提出了视频标识符编码和查询-标识符增强方法，以将视频表示为短序列同时保留其语义信息。我们的方法在四个标准数据集上持续提高了当前最先进模型的检索效率。它使得基线模型在原始检索时间的30%-50%内实现了更好的检索性能，在MSR-VTT (+1.0%)、MSVD (+1.8%)、ActivityNet (+1.5%)和DiDeMo (+0.2%)上分别提高了检索性能。</p>
<h4 id="_72">一句话总结：</h4>
<p>本文提出的T2VIndexer模型通过将视频表示为短序列并保留语义信息，显著提高了文本-视频检索的效率。</p>
<hr />
<h2 id="raglab-a-modular-and-research-oriented-unified-framework-for-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2408.11381v1">RAGLAB: A Modular and Research-Oriented Unified Framework for Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-08-21</p>
<p>作者：Xuanwang Zhang, Yunze Song, Yidong Wang, Shuyun Tang, Xinfeng Li, Zhengran Zeng, Zhen Wu, Wei Ye, Wenyuan Xu, Yue Zhang, Xinyu Dai, Shikun Zhang, Qingsong Wen</p>
<h4 id="_73">中文摘要：</h4>
<p>大型语言模型（LLMs）在对话、推理和知识保留方面展现出与人类相当的能力。然而，即使是最高级的LLMs也面临着诸如幻觉和知识实时更新等挑战。当前的研究通过为LLMs配备外部知识，即所谓的检索增强生成（RAG）技术来解决这一瓶颈。然而，RAG的发展受到两个关键问题的制约。首先，新型RAG算法之间缺乏全面和公平的比较。其次，如LlamaIndex和LangChain等开源工具采用高级抽象，这导致缺乏透明度，并限制了开发新型算法和评估指标的能力。为了填补这一差距，我们引入了RAGLAB，这是一个模块化和面向研究的开源库。RAGLAB重现了6种现有算法，并为研究RAG算法提供了一个全面的生态系统。利用RAGLAB，我们对6种RAG算法在10个基准上进行了公平的比较。通过RAGLAB，研究人员可以有效地比较各种算法的性能并开发新型算法。</p>
<h4 id="_74">一句话总结：</h4>
<p>RAGLAB是一个开源库，旨在促进RAG算法的公平比较和新型算法的开发。</p>
<hr />
<h2 id="reading-with-intent"><a href="http://arxiv.org/abs/2408.11189v1">Reading with Intent</a></h2>
<p>发布时间：2024-08-20</p>
<p>作者：Benjamin Reichman, Kartik Talamadupula, Toshish Jawale, Larry Heck</p>
<h4 id="_75">中文摘要：</h4>
<p>检索增强生成（RAG）系统通过整合外部信息源（如维基百科、内部文档、科学论文或公开互联网）来增强知识语言模型。依赖于公开互联网作为知识源的RAG系统必须应对人类生成内容所带来的复杂性。人类的交流远远超出了文字本身。意图、语气和内涵都可能改变所传达的意义。最近RAG系统的实际部署显示，在理解人类交流的细微差别方面存在一些困难。这些系统的一个显著挑战在于处理讽刺。尽管构成这些RAG系统骨干的大型语言模型（LLMs）能够检测讽刺，但它们目前并不总是利用这些检测来对文本进行后续处理。为了解决这些问题，在本文中，我们从Natural Question的维基百科检索语料库中合成了讽刺段落。然后，我们测试了这些段落对RAG管道检索器和阅读器部分性能的影响。我们引入了一个提示系统，旨在增强模型在存在讽刺的情况下解释和生成响应的能力，从而提高整体系统性能。最后，我们进行了消融研究，以验证我们方法的有效性，证明了在RAG系统中处理讽刺内容方面的改进。</p>
<h4 id="_76">一句话总结：</h4>
<p>本文提出了一种增强RAG系统处理讽刺内容能力的提示系统，通过合成讽刺段落并测试其对系统性能的影响，验证了该方法的有效性。</p>
<hr />
<h2 id="reconciling-methodological-paradigms-employing-large-language-models-as-novice-qualitative-research-assistants-in-talent-management-research"><a href="http://arxiv.org/abs/2408.11043v1">Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research</a></h2>
<p>发布时间：2024-08-20</p>
<p>作者：Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Anshul Mittal, Rutu Mulkar</p>
<h4 id="_77">中文摘要：</h4>
<p>本研究提出了一种新颖的方法，通过利用基于检索增强生成（RAG）的大语言模型（LLMs）来分析访谈记录，以解决手动分析定性数据所需时间和精力的问题。该方法将研究调查策略化为一种由LLM作为新手研究助手增强的研究。这项研究探讨了LLMs的心理模型，使其能够作为人才管理领域研究人员的新手定性研究助手。基于RAG的LLM方法被扩展以实现半结构化访谈数据的话题建模，展示了这些模型在信息检索和搜索传统用途之外的适用性。研究发现，LLM增强的RAG方法可以成功提取感兴趣的话题，与手动从同一数据集中生成的主题相比，覆盖范围显著。这证实了将LLMs作为新手定性研究助手的应用可行性。此外，研究建议，利用此类模型的学者应依赖传统定性研究中使用的质量标准，以确保其方法的严谨性和可信度。最后，论文为寻求将LLMs与既定定性研究范式相协调的业界从业者提供了关键建议，为在人才分析定性数据集中有效整合这些强大但新手的人工智能工具提供了路线图。</p>
<h4 id="_78">一句话总结：</h4>
<p>本研究提出了一种利用大语言模型辅助分析定性数据的新方法，为人才管理领域的研究提供了高效、可靠的定性研究助手。</p>
<hr />
<h2 id="hierarchical-retrieval-augmented-generation-model-with-rethink-for-multi-hop-question-answering"><a href="http://arxiv.org/abs/2408.11875v1">Hierarchical Retrieval-Augmented Generation Model with Rethink for Multi-hop Question Answering</a></h2>
<p>发布时间：2024-08-20</p>
<p>作者：Xiaoming Zhang, Ming Wang, Xiaocui Yang, Daling Wang, Shi Feng, Yifei Zhang</p>
<h4 id="_79">中文摘要：</h4>
<p>多跳问答（Multi-hop Question Answering）需要通过整合多个信息片段进行复杂的推理来解答复杂问题。然而，现有的问答系统面临着诸如信息过时、上下文窗口长度限制以及准确性与数量的权衡等挑战。为了解决这些问题，我们提出了一种新的框架，即带有Rethink（重新思考）的分层检索增强生成模型（Hierarchical Retrieval-Augmented Generation Model，HiRAG），该框架包含分解器（Decomposer）、定义器（Definer）、检索器（Retriever）、过滤器（Filter）和摘要器（Summarizer）五个关键模块。我们引入了一种新的分层检索策略，该策略结合了文档级别的稀疏检索和块级别的密集检索，有效地整合了它们的优势。此外，我们还提出了一种单候选检索方法，以减轻多候选检索的限制。我们还构建了两个新的语料库，即索引维基语料库（Indexed Wikicorpus）和配置文件维基语料库（Profile Wikicorpus），以解决知识过时和不足的问题。在我们的四个数据集上的实验结果表明，HiRAG在大多数指标上优于最先进的模型，并且我们的索引维基语料库是有效的。HiRAG的代码可在https://github.com/2282588541a/HiRAG找到。</p>
<h4 id="_80">一句话总结：</h4>
<p>HiRAG通过分层检索和单候选检索策略，有效提升了多跳问答系统的性能。</p>
<hr />
<h2 id="enhanced-document-retrieval-with-topic-embeddings"><a href="http://arxiv.org/abs/2408.10435v1">Enhanced document retrieval with topic embeddings</a></h2>
<p>发布时间：2024-08-19</p>
<p>作者：Kavsar Huseynova, Jafar Isbarov</p>
<h4 id="_81">中文摘要：</h4>
<p>随着检索增强生成（RAG）的出现，文档检索系统重新引起了人们的关注。RAG架构相比仅使用大型语言模型（LLM）的应用，具有更低的幻觉率。然而，检索机制的准确性被认为是这些应用效率的瓶颈。在语料库中包含多个来自不同但相关主题的文档的情况下，观察到检索性能不佳的特定情况。我们设计了一种新的向量化方法，该方法考虑了文档的主题信息。本文介绍了这种新的文本向量化方法，并在RAG的背景下对其进行了评估。此外，我们还讨论了评估RAG系统的挑战，这与当前的情况相关。</p>
<h4 id="_82">一句话总结：</h4>
<p>本文提出了一种新的文本向量化方法，旨在提高RAG系统中检索机制的准确性，并讨论了评估RAG系统的挑战。</p>
<hr />
<h2 id="legalbench-rag-a-benchmark-for-retrieval-augmented-generation-in-the-legal-domain"><a href="http://arxiv.org/abs/2408.10343v1">LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain</a></h2>
<p>发布时间：2024-08-19</p>
<p>作者：Nicholas Pipitone, Ghita Houir Alami</p>
<h4 id="_83">中文摘要：</h4>
<p>检索增强生成（RAG）系统展现出巨大的潜力，并在人工智能驱动的法律应用中变得越来越重要。现有的基准，如LegalBench，评估了大型语言模型（LLMs）在法律领域的生成能力，但在评估RAG系统的检索组件方面存在一个关键的差距。为了解决这个问题，我们引入了LegalBench-RAG，这是第一个专门设计来评估法律空间内RAG管道检索步骤的基准。LegalBench-RAG强调精确检索，通过专注于从法律文件中提取最小、高度相关的文本片段。这些高度相关的片段优先于检索文档ID或大量不精确的片段，后者都可能超过上下文窗口的限制。长上下文窗口处理成本更高，导致更高的延迟，并使LLMs忘记或虚构信息。此外，精确的结果还允许LLMs为最终用户生成引用。LegalBench-RAG基准是通过将LegalBench查询中使用的上下文回溯到法律语料库中的原始位置构建的，结果是一个包含6,858个查询-答案对的语料库，超过79M字符，全部由法律专家人工标注。我们还引入了LegalBench-RAG-mini，这是一个轻量级的版本，用于快速迭代和实验。通过提供专门的法律检索基准，LegalBench-RAG成为公司和研究人员提高法律领域RAG系统准确性和性能的关键工具。LegalBench-RAG数据集可在https://github.com/zeroentropy-cc/legalbenchrag公开获取。</p>
<h4 id="_84">一句话总结：</h4>
<p>LegalBench-RAG是首个专门针对法律领域RAG系统检索步骤的基准，旨在提高法律应用中RAG系统的准确性和性能。</p>
<hr />
<h2 id="icing-on-the-cake-automatic-code-summarization-at-ericsson"><a href="http://arxiv.org/abs/2408.09735v1">Icing on the Cake: Automatic Code Summarization at Ericsson</a></h2>
<p>发布时间：2024-08-19</p>
<p>作者：Giriprasad Sridhara, Sujoy Roychowdhury, Sumit Soman, Ranjani H G, Ricardo Britto</p>
<h4 id="_85">中文摘要：</h4>
<p>本文介绍了我们在全球电信公司爱立信内部对Java方法自动摘要的研究成果。我们评估了一种名为自动语义提示增强（ASAP）的方法的性能，该方法利用大型语言模型（LLM）为Java方法生成摘要评论。ASAP通过整合静态程序分析和信息检索技术来识别类似示例方法及其开发者编写的Javadoc，并作为我们研究的基准。相比之下，我们探索并比较了四种更简单的方法的性能，这些方法不需要静态程序分析、信息检索，也不需要像ASAP方法那样有示例。我们的方法仅依赖于Java方法体作为输入，这使得它们轻量级且更适合在商业软件开发环境中快速部署。我们在爱立信的一个软件项目上进行了实验，并使用两个广泛使用的开源Java项目Guava和Elasticsearch复制了该研究，以确保结果的可靠性。性能在八个指标上进行了测量，这些指标捕捉了相似性的各个方面。值得注意的是，我们的一种更简单的方法在爱立信项目和开源项目上表现与ASAP方法相当甚至更好。此外，我们还进行了一项消融研究，以检查方法名称对我们提出的四种方法和ASAP方法在Javadoc摘要生成中的影响。通过遮蔽方法名称并观察生成的摘要，我们发现与基准相比，我们的方法在方法名称缺失的情况下受到的影响统计上显著较小。这表明我们的方法对方法名称的变化更加鲁棒，并且可能比ASAP方法从方法体中提取的摘要更全面。</p>
<h4 id="_86">一句话总结：</h4>
<p>本文提出了一种基于大型语言模型的Java方法自动摘要方法，并通过实验证明其性能优于现有方法，且对方法名称变化具有更强的鲁棒性。</p>
<hr />
<h2 id="carbon-footprint-accounting-driven-by-large-language-models-and-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2408.09713v2">Carbon Footprint Accounting Driven by Large Language Models and Retrieval-augmented Generation</a></h2>
<p>发布时间：2024-08-19</p>
<p>作者：Haijin Wang, Mianrong Zhang, Zheng Chen, Nan Shang, Shangheng Yao, Fushuan Wen, Junhua Zhao</p>
<h4 id="_87">中文摘要：</h4>
<p>碳足迹核算对于量化温室气体排放和实现碳中和至关重要。过程的动态性、核算规则、碳相关政策和能源供应结构需要实时更新碳足迹核算。传统的生命周期评估方法高度依赖人工专业知识，使得近乎实时的更新具有挑战性。本文介绍了一种将大型语言模型（LLMs）与检索增强生成（RAG）技术相结合的新方法，以提升碳足迹信息检索和分析的实时性、专业性和经济性。通过利用LLMs的逻辑和语言理解能力以及RAG的高效检索能力，提出的方法LLMs-RAG-CFA能够检索更多相关的专业信息以辅助LLMs，增强模型的生成能力。该方法具有广泛的专业覆盖范围，高效的实时碳足迹信息获取和核算，以及无需频繁更新LLMs参数的成本效益自动化。在五个行业（包括初级铝、锂电池、光伏、新能源汽车和变压器）的实验结果表明，LLMs-RAG-CFA方法优于传统方法和其他LLMs，实现了更高的信息检索率和显著降低的信息偏差以及碳足迹核算偏差。经济可行的设计利用RAG技术平衡实时更新与成本效益，为实时碳排放管理提供了一种高效、可靠且节省成本的解决方案，从而增强了环境可持续性实践。</p>
<h4 id="_88">一句话总结：</h4>
<p>本文提出了一种结合大型语言模型和检索增强生成技术的碳足迹核算方法，显著提升了信息检索效率和准确性，为实时碳排放管理提供了高效、可靠的解决方案。</p>
<hr />
<h2 id="navero-unlocking-fine-grained-semantics-for-video-language-compositionality"><a href="http://arxiv.org/abs/2408.09511v1">NAVERO: Unlocking Fine-Grained Semantics for Video-Language Compositionality</a></h2>
<p>发布时间：2024-08-18</p>
<p>作者：Chaofan Tao, Gukyeong Kwon, Varad Gunjal, Hao Yang, Zhaowei Cai, Yonatan Dukler, Ashwin Swaminathan, R. Manmatha, Colin Jon Taylor, Stefano Soatto</p>
<h4 id="_89">中文摘要：</h4>
<p>本研究探讨了视频-语言（VidL）模型在理解物体、属性、动作及其关系之间的组合能力。由于视频数据中组合关系随时间迅速变化，组合理解对于视频数据来说尤其具有挑战性。我们首先构建了一个名为AARO的基准，用于评估基于空间概念的动作相关的组合理解。该基准通过为给定视频生成带有错误动作描述的负面文本来构建，模型需要将正面文本与其对应的视频配对。此外，我们提出了一种名为NAVERO的训练方法，该方法利用带有负面文本的视频-文本数据增强来提高组合理解能力。我们还开发了一种负面增强的视觉-语言匹配损失，该损失被明确用于从生成的负面文本中获益。我们在组合理解和视频-文本检索性能方面将NAVERO与其他最先进的方法进行了比较。NAVERO在视频-语言和图像-语言组合理解方面均显著优于其他方法，同时在传统的文本-视频检索任务上保持了强大的性能。</p>
<h4 id="_90">一句话总结：</h4>
<p>本研究提出了一种名为NAVERO的训练方法，通过负面文本增强和视觉-语言匹配损失，显著提升了视频-语言模型在组合理解方面的性能。</p>
<hr />
<h2 id="retrieval-augmented-generation-meets-data-driven-tabula-rasa-approach-for-temporal-knowledge-graph-forecasting"><a href="http://arxiv.org/abs/2408.13273v1">Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting</a></h2>
<p>发布时间：2024-08-18</p>
<p>作者：Geethan Sannidhi, Sagar Srinivas Sakhinana, Venkataramana Runkana</p>
<h4 id="_91">中文摘要：</h4>
<p>预训练的大型语言模型（PLLMs）如OpenAI ChatGPT和Google Gemini在时间知识图谱（tKG）预测中面临诸如事实回忆不准确、幻觉、偏见和未来数据泄露等挑战。为了解决这些问题，我们引入了sLA-tKGF（用于tKG预测的小规模语言助手），它通过一种tabula rasa的方法从头开始利用检索增强生成（RAG）辅助，定制训练小规模语言模型，以实现有效的tKG预测。我们的框架通过从tKGs、网络搜索结果和PLLMs生成的文本描述中构建包含相关历史数据的知识提示，来理解目标时间之前的历史实体关系。它利用这些外部知识提示进行更深入的理解和推理，以对特定上下文的语义和时间信息进行零样本提示，从而对小规模语言模型进行更准确的未来事件预测。它通过理解随时间变化的趋势来减少幻觉并缓解分布偏移挑战。因此，它能够在最小化计算需求的同时，实现更准确和基于上下文的未来事件预测。严格的实证研究表明，我们的框架在基准数据集上具有鲁棒性、可扩展性和最先进的（SOTA）性能，并实现了可解释和可信的tKG预测。</p>
<h4 id="_92">一句话总结：</h4>
<p>sLA-tKGF通过利用外部知识提示和零样本提示小规模语言模型，实现了对时间知识图谱中未来事件的更准确预测。</p>
<hr />
<h2 id="agentic-retrieval-augmented-generation-for-time-series-analysis"><a href="http://arxiv.org/abs/2408.14484v1">Agentic Retrieval-Augmented Generation for Time Series Analysis</a></h2>
<p>发布时间：2024-08-18</p>
<p>作者：Chidaksh Ravuru, Sagar Srinivas Sakhinana, Venkataramana Runkana</p>
<h4 id="_93">中文摘要：</h4>
<p>时间序列建模对于许多应用至关重要，然而，它在从历史背景中学习以预测特定任务结果时面临着诸如复杂的时空依赖性和分布偏移等挑战。为了解决这些挑战，我们提出了一种使用代理检索增强生成（RAG）框架进行时间序列分析的新方法。该框架利用一个分层、多代理架构，其中主代理协调专门的子代理并将最终用户请求委托给相关的子代理。子代理通过使用指令调整和直接偏好优化进行微调，利用为特定时间序列任务定制的较小、预训练的语言模型（SLMs），并从包含关于历史模式和趋势的浓缩知识的共享提示库中检索相关提示，以改进对新数据的预测。我们提出的模块化、多代理RAG方法通过比特定任务定制方法更有效地解决复杂挑战，在主要时间序列任务上实现了最先进的性能。</p>
<h4 id="_94">一句话总结：</h4>
<p>本文提出了一种基于代理检索增强生成（RAG）框架的时间序列分析新方法，通过多代理架构和预训练语言模型，有效解决了时间序列建模中的复杂挑战，实现了最先进的性能。</p>
<hr />
<h2 id="developing-a-llama-based-chatbot-for-cicd-question-answering-a-case-study-at-ericsson"><a href="http://arxiv.org/abs/2408.09277v1">Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson</a></h2>
<p>发布时间：2024-08-17</p>
<p>作者：Daksh Chaudhary, Sri Lakshmi Vadlamani, Dimple Thomas, Shiva Nejati, Mehrdad Sabetzadeh</p>
<h4 id="_95">中文摘要：</h4>
<p>本文介绍了我们在爱立信（一家跨国电信公司）开发基于Llama的聊天机器人用于持续集成和持续交付（CI/CD）问题解答的经验。我们的聊天机器人旨在处理爱立信CI/CD文档的特定性，采用检索增强生成（RAG）模型来提高准确性和相关性。我们对聊天机器人在工业CI/CD相关问题上的实证评估表明，结合BM25和嵌入检索器的集成检索器能够提供最佳性能。在与爱立信72个CI/CD问题及答案的基准对比中，我们最精确的聊天机器人配置为61.11%的问题提供了完全正确的答案，为26.39%的问题提供了部分正确的答案，为12.50%的问题提供了错误的答案。通过对部分正确和错误答案的错误分析，我们讨论了不准确性的潜在原因，并提供了进一步改进的见解。我们还反思了所学到的经验教训，并建议了进一步提高聊天机器人准确性的未来方向。</p>
<h4 id="_96">一句话总结：</h4>
<p>本文提出了一种基于Llama的聊天机器人，用于提高爱立信CI/CD文档问答的准确性和相关性。</p>
<hr />
<h2 id="tc-ragturing-complete-rags-case-study-on-medical-llm-systems"><a href="http://arxiv.org/abs/2408.09199v1">TC-RAG:Turing-Complete RAG's Case study on Medical LLM Systems</a></h2>
<p>发布时间：2024-08-17</p>
<p>作者：Xinke Jiang, Yue Fang, Rihong Qiu, Haoyu Zhang, Yongxin Xu, Hao Chen, Wentao Zhang, Ruizhe Zhang, Yuchen Fang, Xu Chu, Junfeng Zhao, Yasha Wang</p>
<h4 id="_97">中文摘要：</h4>
<p>在追求提升特定领域的大型语言模型（LLMs）的过程中，检索增强生成（RAG）作为一种有前景的解决方案，被提出用以缓解诸如幻觉、过时知识和在高度专业查询中有限的专家知识等问题。然而，现有的RAG方法由于忽略了系统状态变量，这些变量对于确保自适应控制、检索停止和系统收敛至关重要，因此存在不足。在本文中，我们通过严格的证明引入了TC-RAG，这是一种新颖的框架，通过整合一个图灵完备系统来管理状态变量，从而实现更高效和准确的知识检索。通过利用具有自适应检索、推理和规划能力的内存堆栈系统，TC-RAG不仅确保了检索过程的受控停止，还通过推（Push）和出（Pop）操作减轻了错误知识的积累。在医疗领域的案例研究中，我们对真实世界医疗健康数据集的广泛实验表明，TC-RAG在准确性上比现有方法高出7.20%。我们的数据集和代码可在https://github.com/Artessay/SAMA.git上获取。</p>
<h4 id="_98">一句话总结：</h4>
<p>TC-RAG通过引入图灵完备系统管理状态变量，实现了更高效和准确的知识检索，显著提升了特定领域大型语言模型在准确性上的表现。</p>
<hr />
<h2 id="a-primer-on-generative-ai-for-telecom-from-theory-to-practice"><a href="http://arxiv.org/abs/2408.09031v1">A Primer on Generative AI for Telecom: From Theory to Practice</a></h2>
<p>发布时间：2024-08-16</p>
<p>作者：Xingqin Lin, Lopamudra Kundu, Chris Dick, Maria Amparo Canaveras Galdon, Janaki Vamaraju, Swastika Dutta, Vinay Raman</p>
<h4 id="_99">中文摘要：</h4>
<p>电信行业正经历着生成式人工智能（GenAI）的崛起，这种转变正在推动创新、提高效率和提供卓越的客户服务。本文从理论到实践概述了电信领域的GenAI。我们回顾了GenAI模型，并讨论了它们在电信领域的实际应用。此外，我们描述了将GenAI有效应用于电信的关键技术推动者和最佳实践。我们强调了检索增强生成（RAG）在将大型语言模型（LLMs）连接到电信领域特定数据源以增强LLMs响应准确性的重要性。我们展示了一个基于RAG的聊天机器人的实际案例，该聊天机器人可以回答开放无线接入网络（O-RAN）特定的问题。该聊天机器人在O-RAN联盟的演示引发了行业内的极大兴趣。我们已在GitHub上使O-RAN RAG聊天机器人公开可访问。</p>
<h4 id="_100">一句话总结：</h4>
<p>本文探讨了生成式人工智能在电信领域的应用，特别是通过检索增强生成技术提升大型语言模型在电信领域的准确性和效率。</p>
<hr />
<h2 id="vera-validation-and-evaluation-of-retrieval-augmented-systems"><a href="http://arxiv.org/abs/2409.03759v1">VERA: Validation and Evaluation of Retrieval-Augmented Systems</a></h2>
<p>发布时间：2024-08-16</p>
<p>作者：Tianyu Ding, Adi Banerjee, Laurent Mombaerts, Yunhong Li, Tarik Borogovac, Juan Pablo De la Cruz Weinstein</p>
<h4 id="_101">中文摘要：</h4>
<p>随着检索增强生成（RAG）系统在各种应用中的日益普及，确保这些系统的准确性、安全性和与用户意图的一致性变得至关重要。本文介绍了VERA（检索增强系统的验证与评估）框架，该框架旨在提高使用检索信息的大型语言模型（LLM）输出结果的透明度和可靠性。VERA通过两种重要方式改进了我们对RAG系统的评估方式：（1）它引入了一种基于交叉编码器的机制，将一系列多维指标整合为一个综合排名分数，解决了优先考虑单个指标的问题；（2）它在对文档库中基于LLM的指标应用Bootstrap统计，以建立置信区间，确保库的主题覆盖范围，并提高检索系统的整体可靠性。通过几个用例，我们展示了VERA如何加强决策过程和AI应用的信任。我们的发现不仅有助于LLM基于的RAG评估指标的理论理解，还促进了负责任AI系统的实际实施，标志着可靠和透明生成AI技术发展的重大进步。</p>
<h4 id="_102">一句话总结：</h4>
<p>VERA框架通过综合评估和置信区间建立，提高了基于LLM的检索增强生成系统的透明度和可靠性。</p>
<hr />
<h2 id="meta-knowledge-for-retrieval-augmented-large-language-models"><a href="http://arxiv.org/abs/2408.09017v1">Meta Knowledge for Retrieval Augmented Large Language Models</a></h2>
<p>发布时间：2024-08-16</p>
<p>作者：Laurent Mombaerts, Terry Ding, Adi Banerjee, Florian Felice, Jonathan Taws, Tarik Borogovac</p>
<h4 id="_103">中文摘要：</h4>
<p>检索增强生成（RAG）是一种技术，用于在不改变底层模型参数的情况下，通过上下文相关、时间敏感或特定领域的相关信息来增强大型语言模型（LLMs）。然而，构建能够有效地从大量且多样化的文档集中合成信息的RAG系统仍然是一个重大挑战。我们为LLMs引入了一种新颖的数据中心化RAG工作流程，将传统的检索后阅读系统转变为更先进的准备-重写-检索-阅读框架，以实现更高领域专家级别的知识库理解。我们的方法依赖于为每个文档生成元数据和合成问答（QA），以及引入基于元数据的文档簇的新概念——元知识摘要（MK Summary）。所提出的创新使得个性化用户查询增强和知识库的深度信息检索成为可能。我们的研究做出了两个重要贡献：使用LLMs作为评估者和采用新的比较性能指标，我们证明了（1）使用合成问题匹配的增强查询显著优于依赖于文档分块的传统的RAG管道（p &lt; 0.01），以及（2）元知识增强查询进一步显著提高了检索的精确度、召回率，以及最终答案的广度、深度、相关性和特异性。我们的方法成本效益高，使用Claude 3 Haiku，每2000篇研究论文的成本不到20美元，并且可以通过对语言或嵌入模型进行微调来适应，从而进一步提高端到端RAG管道的性能。</p>
<h4 id="_104">一句话总结：</h4>
<p>本研究提出了一种创新的RAG工作流程，通过元数据和合成问答增强大型语言模型，显著提高了信息检索的准确性和全面性。</p>
<hr />
<h2 id="retrieval-augmented-few-shot-medical-image-segmentation-with-foundation-models"><a href="http://arxiv.org/abs/2408.08813v1">Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models</a></h2>
<p>发布时间：2024-08-16</p>
<p>作者：Lin Zhao, Xiao Chen, Eric Z. Chen, Yikang Liu, Terrence Chen, Shanhui Sun</p>
<h4 id="_105">中文摘要：</h4>
<p>医学图像分割对于临床决策至关重要，但标注数据的稀缺性带来了重大挑战。少样本分割（FSS）方法显示出希望，但通常需要针对目标领域重新训练，并且难以在不同模态之间泛化。同样，将基础模型如Segment Anything Model（SAM）应用于医学成像也存在局限性，包括需要微调和领域特定适应。为了解决这些问题，我们提出了一种新颖的方法，该方法将DINOv2和Segment Anything Model 2（SAM 2）应用于检索增强的少样本医学图像分割。我们的方法使用DINOv2的特征作为查询，从有限的标注数据中检索相似样本，这些样本随后被编码为记忆并存储在记忆库中。借助SAM 2的记忆注意力机制，模型利用这些记忆作为条件来生成目标图像的准确分割。我们在三个医学图像分割任务上评估了我们的框架，证明了在各种模态上具有优越的性能和泛化能力，而无需任何重新训练或微调。总的来说，这种方法为少样本医学图像分割提供了一个实用且有效的解决方案，并在临床应用中作为有价值的标注工具具有重大潜力。</p>
<h4 id="_106">一句话总结：</h4>
<p>提出了一种基于检索增强的少样本医学图像分割方法，通过记忆库和注意力机制实现跨模态泛化，无需重新训练或微调。</p>
<hr />
<h2 id="extracting-polygonal-footprints-in-off-nadir-images-with-segment-anything-model"><a href="http://arxiv.org/abs/2408.08645v1">Extracting polygonal footprints in off-nadir images with Segment Anything Model</a></h2>
<p>发布时间：2024-08-16</p>
<p>作者：Kai Li, Jingbo Chen, Yupeng Deng, Yu Meng, Diyou Liu, Junxian Ma, Chenhao Wang</p>
<h4 id="_107">中文摘要：</h4>
<p>本文针对非正射航空影像中的建筑占地面积提取（BFE）问题，提出了一种名为OBMv2的新方法。该方法支持端到端和可提示的多边形占地面积预测。与OBM不同，OBMv2采用了一种新提出的自我偏移注意力（SOFA）机制来弥合平房和摩天大楼之间的性能差距，实现了无需后处理的真正端到端占地面积多边形预测。为了充分利用屋顶掩码、建筑掩码和偏移量中包含的信息，我们提出了一种多级信息系统（MISS）用于占地面积预测，使得OBMv2即使在预测信息不足的情况下也能预测占地面积。此外，受到自然语言处理中的检索增强生成（RAG）的启发，我们提出了“BFE中的RAG”问题，以从同一模型中提取更多信息。为了验证所提方法的有效性，我们在公开数据集BONAI和OmniCity-view3上进行了实验，并在惠州市测试集上进行了泛化测试。代码将在\url{https://github.com/likaiucas/OBM}上提供。</p>
<h4 id="_108">一句话总结：</h4>
<p>本文提出了一种基于SOFA和MISS的OBMv2方法，实现了非正射航空影像中建筑占地面积的端到端预测，并通过RAG技术提高了预测的准确性。</p>
<hr />
<h2 id="communitykg-rag-leveraging-community-structures-in-knowledge-graphs-for-advanced-retrieval-augmented-generation-in-fact-checking"><a href="http://arxiv.org/abs/2408.08535v1">CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking</a></h2>
<p>发布时间：2024-08-16</p>
<p>作者：Rong-Ching Chang, Jiawei Zhang</p>
<h4 id="_109">中文摘要：</h4>
<p>尽管大型语言模型（LLMs）和检索增强生成（RAG）系统取得了进展，但它们的有效性通常受到与实体关系和社区结构缺乏整合的限制，这限制了它们提供丰富和准确的信息检索以进行事实核查的能力。我们引入了CommunityKG-RAG（社区知识图谱-RAG），这是一个新颖的无需额外训练即可适应新领域和查询的零样本框架，它将知识图谱（KGs）中的社区结构整合到RAG系统中，以增强事实核查过程。CommunityKG-RAG利用KGs中社区结构的跨跳特性，显著提高了信息检索的准确性和相关性。我们的实验结果表明，CommunityKG-RAG优于传统方法，通过提供一种稳健、可扩展和高效的解决方案，代表了事实核查的重大进步。</p>
<h4 id="_110">一句话总结：</h4>
<p>CommunityKG-RAG通过整合知识图谱中的社区结构，显著提升了检索增强生成系统在事实核查中的准确性和相关性。</p>
<hr />
<h2 id="murar-a-simple-and-effective-multimodal-retrieval-and-answer-refinement-framework-for-multimodal-question-answering"><a href="http://arxiv.org/abs/2408.08521v1">MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering</a></h2>
<p>发布时间：2024-08-16</p>
<p>作者：Zhengyuan Zhu, Daniel Lee, Hong Zhang, Sai Sree Harsha, Loic Feujio, Akash Maharaj, Yunyao Li</p>
<h4 id="_111">中文摘要：</h4>
<p>近年来，检索增强生成（RAG）在问答（QA）任务中的性能得到了显著提升。然而，大多数先前的研究主要关注基于文本的答案。尽管一些研究涉及多模态数据，但它们在生成全面的多模态答案方面仍存在不足，尤其是在解释概念或提供如何实现特定目标的逐步教程方面。这种能力对于企业聊天机器人、客户服务和教育系统等应用尤其有价值，因为这些答案来源于多模态数据。在本文中，我们介绍了一个简单而有效的框架，名为MuRAR（多模态检索和答案细化）。MuRAR通过检索相关多模态数据并细化响应来增强基于文本的答案，从而创建连贯的多模态答案。此框架可以轻松扩展，通过最小修改支持企业聊天机器人的多模态答案。人类评估结果表明，MuRAR生成的多模态答案比纯文本答案更有用且更易读。</p>
<h4 id="_112">一句话总结：</h4>
<p>MuRAR通过检索和细化多模态数据，有效提升了基于文本的问答系统的答案质量和可读性。</p>
<hr />
<h2 id="w-rag-weakly-supervised-dense-retrieval-in-rag-for-open-domain-question-answering"><a href="http://arxiv.org/abs/2408.08444v1">W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering</a></h2>
<p>发布时间：2024-08-15</p>
<p>作者：Jinming Nian, Zhiyuan Peng, Qifan Wang, Yi Fang</p>
<h4 id="_113">中文摘要：</h4>
<p>在知识密集型任务，如开放域问答（OpenQA）中，大型语言模型（LLMs）往往难以仅依靠其内部（参数化）知识生成事实性答案。为了解决这一局限性，检索增强生成（RAG）系统通过从外部来源检索相关信息来增强LLMs，从而将检索器定位为关键组件。尽管密集检索展示了最先进的性能，但其训练由于缺乏真实证据而面临挑战，这主要归因于人工标注的高成本。在本文中，我们通过利用LLMs的排序能力来创建用于训练密集检索器的弱标签数据，提出了W-RAG。具体来说，我们通过评估LLMs基于问题和每个段落生成正确答案的概率，对通过BM25检索到的top-$K$段落进行重新排序。然后，使用排名最高的段落作为密集检索的积极训练示例。我们跨越四个公开可用的OpenQA数据集的全面实验表明，与基线模型相比，我们的方法提高了检索和OpenQA性能。</p>
<h4 id="_114">一句话总结：</h4>
<p>本文提出的W-RAG方法通过利用LLMs的排序能力，有效提升了密集检索和开放域问答的性能。</p>
<hr />
<h2 id="assessing-and-enhancing-large-language-models-in-rare-disease-question-answering"><a href="http://arxiv.org/abs/2408.08422v1">Assessing and Enhancing Large Language Models in Rare Disease Question-answering</a></h2>
<p>发布时间：2024-08-15</p>
<p>作者：Guanchu Wang, Junhao Ran, Ruixiang Tang, Chia-Yuan Chang, Chia-Yuan Chang, Yu-Neng Chuang, Zirui Liu, Vladimir Braverman, Zhandong Liu, Xia Hu</p>
<h4 id="_115">中文摘要：</h4>
<p>尽管大型语言模型（LLMs）在一般医学领域展现出令人印象深刻的性能，但在罕见病诊断方面的表现仍存在疑问。为了回答这个问题，我们旨在评估LLMs在罕见病诊断中的性能，并探索提高其在该领域有效性的方法。在本工作中，我们引入了一个罕见病问答（ReDis-QA）数据集，以评估LLMs在诊断罕见病方面的性能。具体来说，我们在ReDis-QA数据集中收集了1360对高质量的问答，涵盖了205种罕见疾病。此外，我们对每个问题进行了元数据标注，便于提取针对任何给定疾病及其特性的特定子集。基于ReDis-QA数据集，我们对几个开源LLMs进行了基准测试，发现诊断罕见病对这些模型来说仍然是一个重大挑战。为了促进罕见病诊断中的检索增强生成，我们收集了首个罕见病语料库（ReCOP），来源于美国罕见疾病组织（NORD）数据库。具体来说，我们将每种罕见病的报告分割成多个片段，每个片段代表疾病的某个不同属性，包括概述、症状、原因、影响、相关疾病、诊断和标准疗法。这种结构确保了每个片段中的信息与问题的一致性。实验结果表明，ReCOP可以平均提高LLMs在ReDis-QA数据集上的准确率8%。此外，它显著引导LLMs生成可追溯至现有文献的可靠答案和解释。</p>
<h4 id="_116">一句话总结：</h4>
<p>本研究通过构建罕见病问答数据集和语料库，评估了LLMs在罕见病诊断中的性能，并提出了提高其诊断准确性的方法。</p>
<hr />
<h2 id="graph-retrieval-augmented-generation-a-survey"><a href="http://arxiv.org/abs/2408.08921v1">Graph Retrieval-Augmented Generation: A Survey</a></h2>
<p>发布时间：2024-08-15</p>
<p>作者：Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang</p>
<h4 id="_117">中文摘要：</h4>
<p>最近，检索增强生成（Retrieval-Augmented Generation，RAG）在解决大型语言模型（Large Language Models，LLMs）的挑战方面取得了显著的成功，而无需重新训练。通过引用外部知识库，RAG可以优化LLMs的输出，有效缓解了诸如“幻觉”（hallucination）、缺乏特定领域知识以及信息过时等问题。然而，数据库中不同实体之间复杂的关系结构给RAG系统带来了挑战。为了应对这一挑战，GraphRAG利用实体之间的结构信息，以实现更精确和全面的检索，捕捉关系知识，并促进更准确、上下文感知的响应。鉴于GraphRAG的创新性和潜力，对当前技术的系统回顾是必不可少的。本文提供了GraphRAG方法的首次全面概述。我们正式化了GraphRAG的工作流程，包括基于图索引（Graph-Based Indexing）、图引导检索（Graph-Guided Retrieval）和图增强生成（Graph-Enhanced Generation）。然后，我们概述了每个阶段的核心理技术和训练方法。此外，我们还考察了GraphRAG的下游任务、应用领域、评估方法和工业用例。最后，我们探讨了未来的研究方向，以激发进一步的探索并推动该领域的发展。</p>
<h4 id="_118">一句话总结：</h4>
<p>本文全面概述了GraphRAG方法，通过利用实体间的结构信息，实现了更精确和全面的检索，从而提升大型语言模型的生成质量。</p>
<hr />
<h2 id="extracting-sentence-embeddings-from-pretrained-transformer-models"><a href="http://arxiv.org/abs/2408.08073v1">Extracting Sentence Embeddings from Pretrained Transformer Models</a></h2>
<p>发布时间：2024-08-15</p>
<p>作者：Lukas Stankevičius, Mantas Lukoševičius</p>
<h4 id="_119">中文摘要：</h4>
<p>背景/引言：预训练的转换器模型在许多自然语言处理任务中表现出色，因此人们期望它们能够承载输入句子或文本的意义表示。这些句子级嵌入在检索增强生成中也非常重要。但是，常用的简单平均或提示模板是否足以提取这些表示呢？
方法：针对包含1100万个参数的BERT模型的多层和多个标记的隐藏表示，我们尝试了各种方法来提取最优的句子表示。我们测试了各种标记聚合和表示后处理技术。我们还测试了多种使用通用Wikitext数据集来补充BERT句子表示的方法。所有方法都在8个语义文本相似度（STS）、6个短文本聚类和12个分类任务上进行了测试。我们还在其他静态模型上评估了我们的表示塑造技术，包括随机标记表示。
结果：所提出的表示提取方法提高了所有考虑的模型在STS和聚类任务上的性能。对于静态基于标记的模型，尤其是对于STS任务的随机嵌入，性能提升非常高，几乎达到了BERT派生表示的性能。
结论：我们的工作表明，对于多个任务，简单的基线结合表示塑造技术可以达到甚至超过更复杂的基于BERT的模型，或者能够为它们的性能做出贡献。</p>
<h4 id="_120">一句话总结：</h4>
<p>通过使用表示塑造技术，简单的基线模型在多个自然语言处理任务上达到了甚至超过了复杂BERT模型的性能。</p>
<hr />
<h2 id="ragchecker-a-fine-grained-framework-for-diagnosing-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2408.08067v2">RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-08-15</p>
<p>作者：Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He, Zhiguo Wang, Pengfei Liu, Yue Zhang, Zheng Zhang</p>
<h4 id="_121">中文摘要：</h4>
<p>尽管检索增强生成（RAG）在利用外部知识方面展现出有前景的能力，但由于RAG的模块化特性、长格式响应的评估以及测量结果的可靠性问题，对RAG系统的全面评估仍然具有挑战性。在本文中，我们提出了一种细粒度评估框架RAGChecker，该框架结合了一套针对检索和生成模块的诊断指标。元评估验证了RAGChecker与其他评估指标相比，与人类判断有显著更好的相关性。使用RAGChecker，我们评估了8个RAG系统，并对其性能进行了深入分析，揭示了RAG架构设计选择中的洞察力和权衡。</p>
<h4 id="_122">一句话总结：</h4>
<p>本文提出的RAGChecker评估框架能够有效评估RAG系统性能，为研究者与实践者提供指导，以开发更有效的RAG系统。</p>
<hr />
<h2 id="plan-with-code-comparing-approaches-for-robust-nl-to-dsl-generation"><a href="http://arxiv.org/abs/2408.08335v1">Plan with Code: Comparing approaches for robust NL to DSL generation</a></h2>
<p>发布时间：2024-08-15</p>
<p>作者：Nastaran Bassamzadeh, Chhaya Methani</p>
<h4 id="_123">中文摘要：</h4>
<p>在许多编排任务中，将规划以代码形式进行被认为是更可靠的方法。这是因为代码比通过自然语言生成的步骤更容易处理，并且通过将确定性逻辑抽象为函数，可以轻松地支持更复杂的序列。此外，它还允许通过在代码上运行的解析检查来发现不正确的函数名问题。然而，代码生成方法在进展上仍然局限于通用语言如C、C++和Python。大型语言模型（LLMs）在领域特定语言（DSLs）或特定领域的自定义函数名上继续面临挑战，导致更高的幻觉率和语法错误。对于通常是计划一部分的自定义函数名，这种情况更为常见。此外，保持LLMs与最新的函数名同步也是一个问题。这在像在大量API上执行任务规划这样的场景中构成了挑战，因为计划以具有自定义API名称的DSL的形式表示。在这篇论文中，我们将RPA（机器人流程自动化）领域的流程自动化作为任务规划的一个特殊情况来研究。我们提出了使用LLMs进行DSL生成的检索增强生成（RAG）优化，以及与微调模型比较的消融研究。我们的结果表明，微调模型在代码相似度指标上得分最高。然而，通过我们的优化，RAG方法能够匹配测试集中领域内API名称的质量。此外，它为领域外或未见的API名称提供了显著的优势，在相似度指标上优于微调模型7分。</p>
<h4 id="_124">一句话总结：</h4>
<p>本文提出了一种基于RAG的优化方法，用于提高LLMs在生成领域特定语言代码时的性能，特别是在处理自定义API名称时。</p>
<hr />
<h2 id="the-death-of-schema-linking-text-to-sql-in-the-age-of-well-reasoned-language-models"><a href="http://arxiv.org/abs/2408.07702v2">The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models</a></h2>
<p>发布时间：2024-08-14</p>
<p>作者：Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, Amine Mhedhbi</p>
<h4 id="_125">中文摘要：</h4>
<p>模式链接是文本到SQL管道中的关键步骤。其目标是检索用户查询所需的目标数据库中的相关表和列，同时忽略不相关的部分。然而，不完善的模式链接往往会导致排除生成准确查询所需的必要列。在本工作中，我们重新审视了在使用最新一代大型语言模型（LLMs）时的模式链接。我们通过实证研究发现，即使存在大量不相关的模式元素，新的模型也擅长在生成过程中利用相关的模式元素。因此，在我们的文本到SQL管道中，当模式适合模型上下文窗口时，我们完全放弃了模式链接，以最小化由于过滤所需模式元素而产生的问题。此外，我们不是过滤上下文信息，而是强调诸如增强、选择和校正等技术，并将它们应用于提高我们的文本到SQL管道的准确性。我们的方法在BIRD基准测试中排名第一，达到了71.83%的准确率。</p>
<h4 id="_126">一句话总结：</h4>
<p>本研究通过利用新一代大型语言模型，改进了文本到SQL管道的模式链接，显著提高了查询生成的准确性。</p>
<hr />
<h2 id="weknow-rag-an-adaptive-approach-for-retrieval-augmented-generation-integrating-web-search-and-knowledge-graphs"><a href="http://arxiv.org/abs/2408.07611v2">WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs</a></h2>
<p>发布时间：2024-08-14</p>
<p>作者：Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu</p>
<h4 id="_127">中文摘要：</h4>
<p>大型语言模型（LLMs）在自适应智能代理的开发中做出了巨大贡献，并被视为实现通用人工智能（AGI）的重要途径。然而，LLMs容易产生事实错误的信息，并经常产生“幽灵”内容，这削弱了它们的可靠性，给其在现实场景中的应用带来了严重挑战。通过结合外部数据库和信息检索机制来增强LLMs是一条有效的途径。为了解决上述挑战，我们提出了一种名为WeKnow-RAG的新方法，该方法将网络搜索和知识图谱整合到一个“检索增强生成（RAG）”系统中。首先，通过结合知识图谱的结构化表示和密集向量检索的灵活性，提高了LLMs响应的准确性和可靠性。WeKnow-RAG随后利用特定领域的知识图谱来满足各种查询和领域，通过采用多阶段网页检索技术，结合稀疏和密集检索方法，从而在事实信息和复杂推理任务上提高性能。我们的方法有效地平衡了信息检索的效率和准确性，从而提高了整体检索过程。最后，我们还集成了一个自我评估机制，让LLMs评估其生成的答案的可信度。我们的方法在广泛的离线实验和在线提交中证明了其卓越的有效性。</p>
<h4 id="_128">一句话总结：</h4>
<p>WeKnow-RAG通过整合网络搜索和知识图谱，有效提升了大型语言模型的准确性和可靠性，从而在现实场景中实现更可靠的智能代理。</p>
<hr />
<h2 id="new-curriculum-new-chance-retrieval-augmented-generation-for-lesson-planning-in-ugandan-secondary-schools-prototype-quality-evaluation"><a href="http://arxiv.org/abs/2408.07542v1">New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson Planning in Ugandan Secondary Schools. Prototype Quality Evaluation</a></h2>
<p>发布时间：2024-08-14</p>
<p>作者：Simon Kloker, Herbertson Bukoli, Twaha Kateete</p>
<h4 id="_129">中文摘要：</h4>
<p>引言：在21世纪乌干达，尤其是农村地区，中学教育质量差仍然被视为一个主要挑战。研究确定了几个问题，包括教师教案质量低或缺失。随着政府推动实施新课程，现有的教案变得过时，问题加剧。我们采用检索增强生成方法，开发了一个原型，该原型基于政府认可的教科书生成定制教案。这有助于教师更高效、更高质量地创建教案，确保教案与新的课程和基于能力的教学方法完全一致。</p>
<p>方法：该原型使用Cohere LLM、句子嵌入和LangChain框架创建，之后在公共网站上提供。为三个新的课程教科书（ICT、数学、历史）训练了向量存储，所有这些教科书都是中学一年级水平。根据教科书中的建议时间段，按照伪随机生成协议生成了24个教案。三个独立评分者根据Ndihokubwayo等人（2022）设计的针对东非和基于能力的课程的具体教案分析协议（LPAP）对教案的技术质量进行了分析。</p>
<p>结果：使用LPAP对24个教案进行评估，平均质量在75%到80%之间，对应于“非常好的教案”。没有教案得分低于65%，尽管可以争论一个教案缺少主题。总之，生成的教案质量至少与人类创建的教案相当，甚至更好，正如在卢旺达的一项研究中所示，其中没有一个教案达到50%的基准。</p>
<h4 id="_130">一句话总结：</h4>
<p>本研究开发了一个基于检索增强生成的教案原型，显著提高了乌干达中学教师教案的质量和效率。</p>
<hr />
<h2 id="exploring-retrieval-augmented-generation-in-arabic"><a href="http://arxiv.org/abs/2408.07425v1">Exploring Retrieval Augmented Generation in Arabic</a></h2>
<p>发布时间：2024-08-14</p>
<p>作者：Samhaa R. El-Beltagy, Mohamed A. Abdallah</p>
<h4 id="_131">中文摘要：</h4>
<p>最近，检索增强生成（RAG）在自然语言处理领域崭露头角，它结合了基于检索和基于生成的模型的优势，以增强文本生成任务。然而，RAG在具有独特特性和资源限制的阿拉伯语中的应用仍处于探索阶段。本文对RAG在阿拉伯语文本中的应用进行了全面的研究，包括实施和评估。研究重点在于探索检索阶段的多种语义嵌入模型和生成阶段的多个大型语言模型（LLMs），以研究在阿拉伯语语境中哪些方法有效，哪些无效。此外，研究还触及了检索阶段中文档方言和查询方言之间差异的问题。结果表明，现有的语义嵌入模型和LLMs可以有效地用于构建阿拉伯语RAG管道。</p>
<h4 id="_132">一句话总结：</h4>
<p>本文研究了检索增强生成（RAG）在阿拉伯语文本生成中的应用，并验证了现有语义嵌入模型和大型语言模型在构建阿拉伯语RAG管道中的有效性。</p>
<hr />
<h2 id="openresearcher-unleashing-ai-for-accelerated-scientific-research"><a href="http://arxiv.org/abs/2408.06941v1">OpenResearcher: Unleashing AI for Accelerated Scientific Research</a></h2>
<p>发布时间：2024-08-13</p>
<p>作者：Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, Yang Xu, Qingkai Min, Zizhao Zhang, Yiwen Wang, Wenjie Li, Pengfei Liu</p>
<h4 id="_133">中文摘要：</h4>
<p>随着科学文献的快速增长，研究人员面临着巨大的挑战，他们需要跟上自己领域内的最新进展并深入研究新领域。我们介绍了OpenResearcher，这是一个利用人工智能（AI）技术来加速研究过程、通过回答研究人员多样化的问题的创新平台。OpenResearcher基于检索增强生成（RAG）构建，旨在将大型语言模型（LLMs）与最新的、特定领域的知识相结合。此外，我们还为OpenResearcher开发了各种工具，以理解研究人员的查询，从科学文献中进行搜索，过滤检索到的信息，提供准确和全面的答案，并自我优化这些答案。OpenResearcher可以灵活地使用这些工具来平衡效率和效果。因此，OpenResearcher使研究人员能够节省时间并提高发现新见解和推动科学突破的潜力。演示、视频和代码可在以下网址获取：https://github.com/GAIR-NLP/OpenResearcher。</p>
<h4 id="_134">一句话总结：</h4>
<p>OpenResearcher是一个利用人工智能技术，通过整合大型语言模型和特定领域知识，帮助研究人员加速研究过程、提高效率的创新平台。</p>
<hr />
<h2 id="a-rag-based-question-answering-solution-for-cyber-attack-investigation-and-attribution"><a href="http://arxiv.org/abs/2408.06272v1">A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution</a></h2>
<p>发布时间：2024-08-12</p>
<p>作者：Sampath Rajapaksha, Ruby Rani, Erisa Karafili</p>
<h4 id="_135">中文摘要：</h4>
<p>在网络安全这一不断发展的领域，分析师必须紧跟最新的攻击趋势和有助于调查和归因网络攻击的相关信息。在本研究中，我们引入了第一个问答（QA）模型及其应用，为网络安全专家提供关于网络攻击调查和归因的信息。我们的QA模型基于检索增强生成（RAG）技术，结合大型语言模型（LLM），并根据用户查询提供答案，这些答案基于我们包含有关网络攻击调查和归因的精选信息的知识库（KB）或用户提供的其他资源。我们使用包括基于KB、基于元数据、KB中的特定文档以及基于外部来源的问题在内的各种类型的问题测试和评估了我们的QA模型。我们将基于KB的问题的答案与OpenAI的GPT-3.5和最新的GPT-4o LLM的答案进行了比较。我们提出的QA模型通过提供答案来源并克服GPT模型幻觉限制，在网络安全攻击调查和归因中至关重要，从而优于OpenAI的GPT模型。此外，我们的分析表明，当RAG QA模型接收到少量示例而不是零样本指令时，与不提供额外示例的情况相比，它生成的答案更好。</p>
<h4 id="_136">一句话总结：</h4>
<p>本研究提出了一种基于RAG技术的问答模型，用于网络安全专家在网络攻击调查和归因中的信息查询，有效提升了网络安全分析的效率和准确性。</p>
<hr />
<h2 id="bayesian-inference-to-improve-quality-of-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2408.08901v1">Bayesian inference to improve quality of Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-08-12</p>
<p>作者：Dattaraj Rao</p>
<h4 id="_137">中文摘要：</h4>
<p>检索增强生成（Retrieval Augmented Generation，RAG）是现代大型语言模型（Large Language Model，LLM）应用中最流行的模式。RAG涉及从大型语料库中（通常捕获在向量数据库中）找到与用户查询相关的段落。一旦在向量数据库上完成第一级搜索，最相关的n个文本块将直接包含在上下文中，并作为提示发送给LLM。这种方法的缺点在于文本块的质量取决于搜索的有效性。在搜索之后没有强大的后处理来确定这些块是否包含足够的信息以包含在提示中。此外，有时可能会有关于同一主题的冲突信息的块，而模型没有先验经验来确定优先考虑哪个块来做出决定。通常，这会导致模型提供一个声明存在冲突陈述，但不能产生答案。在这项研究中，我们提出了一种贝叶斯方法来验证搜索结果中文本块的质量。贝叶斯定理试图将假设的条件概率与证据和先验概率联系起来。我们提出，找到文本块给出高质量答案的可能性，并使用文本块质量的先验概率可以帮助我们提高RAG系统响应的整体质量。我们可以使用LLM本身来获取上下文段落的相关性的可能性。对于文本块的先验概率，我们使用解析文档中的页码。假设是，早期页面中的段落有更好的概率是发现，并且更有助于概括答案。</p>
<h4 id="_138">一句话总结：</h4>
<p>本研究提出了一种基于贝叶斯方法的RAG系统，通过评估搜索结果中文本块的质量来提高LLM生成响应的整体质量。</p>
<hr />
<h2 id="optimizing-rag-techniques-for-automotive-industry-pdf-chatbots-a-case-study-with-locally-deployed-ollama-models"><a href="http://arxiv.org/abs/2408.05933v1">Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models</a></h2>
<p>发布时间：2024-08-12</p>
<p>作者：Fei Liu, Zejun Kang, Xing Han</p>
<h4 id="_139">中文摘要：</h4>
<p>随着对汽车工业生产环境中离线PDF聊天机器人的需求日益增长，优化在本地、低性能环境下部署大型语言模型（LLMs）变得越来越重要。本研究专注于通过本地部署的Ollama模型增强检索增强生成（RAG）技术，以处理复杂的汽车工业文档。基于Langchain框架，我们提出了一个针对Ollama本地RAG实现的多元优化方法。我们的方法解决了汽车文档处理中的关键挑战，包括多列布局和技术规范。我们引入了针对汽车工业文档独特特征的PDF处理、检索机制和上下文压缩的改进。此外，我们根据LangGraph最佳实践设计了支持嵌入管道的自定义类和支持基于自我-RAG的代理。为了评估我们的方法，我们构建了一个包含典型汽车工业文档的专有数据集，包括技术报告和公司法规。我们将我们的优化RAG模型和自我-RAG代理与一个简单的RAG基线在三个数据集上进行了比较：我们的汽车工业数据集、QReCC和CoQA。结果表明，在上下文精确度、上下文召回率、答案相关性和忠实度方面均有显著提升，特别是在汽车工业数据集上表现尤为突出。我们的优化方案为在汽车行业部署本地RAG系统提供了一个有效的解决方案，满足了工业生产环境中PDF聊天机器人的特定需求。这项研究对推动汽车行业的信息处理和智能生产具有重要意义。</p>
<h4 id="_140">一句话总结：</h4>
<p>本研究通过优化本地RAG系统，提高了汽车工业文档处理的效率和准确性，为汽车行业的信息处理和智能生产提供了有效解决方案。</p>
<hr />
<h2 id="a-new-pipeline-for-generating-instruction-dataset-via-rag-and-self-fine-tuning"><a href="http://arxiv.org/abs/2408.05911v1">A New Pipeline For Generating Instruction Dataset via RAG and Self Fine-Tuning</a></h2>
<p>发布时间：2024-08-12</p>
<p>作者：Chih-Wei Song, Yu-Kai Lee, Yin-Te Tsai</p>
<h4 id="_141">中文摘要：</h4>
<p>近年来，随着大型语言模型的快速发展，对能够满足企业和组织独特需求的特定领域代理的需求日益增长。与追求广泛覆盖的通用模型不同，这些专业代理依赖于针对其预期应用定制的专注数据集。本研究提出了一种利用大型语言模型（LLM）和检索增强生成相关框架构建高质量指令数据集的管道，用于针对特定领域进行微调，并使用定制文档集合。通过摄取特定领域的文档，该管道生成相关且情境适当的指令，从而有效地创建了一个用于在目标领域微调LLM的全面数据集。这种方法克服了传统数据集创建方法的局限性，这些方法通常依赖于手动编纂或网络抓取技术，可能会引入噪声和不相关数据。值得注意的是，我们的管道提供了一个动态解决方案，可以快速适应特定领域文档集合的更新或修改，无需完全重新训练。此外，它通过允许从有限数量的初始文档生成指令数据集来解决数据稀缺的问题，使其适用于数据集稀缺的不受欢迎或专业领域。作为案例研究，我们将这种方法应用于精神病学领域，这是一个需要专业知识和对患者信息敏感处理领域。结果微调的LLM展示了所提出方法的可行性，并强调了其在需要定制、准确和情境相关语言模型的各个行业和领域中的广泛应用潜力。</p>
<h4 id="_142">一句话总结：</h4>
<p>本研究提出了一种利用LLM和检索增强生成框架构建特定领域指令数据集的方法，以实现LLM的微调，并有效解决数据稀缺和领域特定需求问题。</p>
<hr />
<h2 id="validation-requirements-for-ai-based-intervention-evaluation-in-aging-and-longevity-research-and-practice"><a href="http://arxiv.org/abs/2408.15264v1">Validation Requirements for AI-based Intervention-Evaluation in Aging and Longevity Research and Practice</a></h2>
<p>发布时间：2024-08-11</p>
<p>作者：Georg Fuellen, Anton Kulaga, Sebastian Lobentanzer, Maximilian Unfried, Roberto Avelar, Daniel Palmer, Brian K. Kennedy</p>
<h4 id="_143">中文摘要：</h4>
<p>老龄化与长寿研究领域面临着海量数据的挑战，迫切需要使用人工智能（AI），包括大型语言模型（LLMs），来评估抗衰老干预措施。这些评估应当是正确的、有用的、全面的、可解释的，并应考虑因果关系、跨学科性、标准遵循、纵向数据和已知的衰老生物学。特别是，全面分析应超越基于经典生物医学数据库的数据比较，建议使用AI来解释生物标志物和结果的变化。我们的需求推动了使用包含知识图谱的LLMs以及采用例如检索增强生成等专用工作流程。尽管对AI工具的简单信任可能造成伤害，但将我们的需求添加到LLM查询中可以提高响应质量，呼吁进行基准测试工作，并证明LLMs在长寿干预建议方面的知情使用是合理的。</p>
<h4 id="_144">一句话总结：</h4>
<p>利用人工智能技术，特别是大型语言模型，对抗衰老干预措施进行科学评估，以提高评估的准确性和实用性。</p>
<hr />
<h2 id="a-decoding-acceleration-framework-for-industrial-deployable-llm-based-recommender-systems"><a href="http://arxiv.org/abs/2408.05676v1">A Decoding Acceleration Framework for Industrial Deployable LLM-based Recommender Systems</a></h2>
<p>发布时间：2024-08-11</p>
<p>作者：Yunjia Xi, Hangyu Wang, Bo Chen, Jianghao Lin, Menghui Zhu, Weiwen Liu, Ruiming Tang, Weinan Zhang, Yong Yu</p>
<h4 id="_145">中文摘要：</h4>
<p>最近，基于大型语言模型（LLM）的推荐系统受到了越来越多的关注，但其在工业界的部署仍在探索中。大多数部署利用LLM作为特征增强器，在离线阶段生成增强知识。然而，在涉及众多用户和物品的推荐场景中，即使使用LLM进行离线生成也会消耗大量时间和资源。这种生成效率低下源于LLM的自回归特性，而加速的一个有前景的方向是推测解码，这是一种“先草拟后验证”的范式，通过增加每一步解码中生成的标记数量。在本文中，我们首先确定推荐知识生成适合基于检索的推测解码。然后，我们识别出两个特征：（1）在推荐系统（RSs）中，广泛的物品和用户会导致检索效率低下；（2）RSs对LLM生成的文本表现出高多样性容忍度。基于上述见解，我们提出了一种基于LLM的推荐解码加速框架（称为DARE），其中包含定制的检索池以提高检索效率，以及宽松的验证以增加草拟标记的接受率。广泛的实验表明，DARE实现了3-5倍的加速，并且与各种框架和骨干LLM兼容。DARE还已在大型商业环境中的在线广告场景中部署，实现了3.45倍的加速，同时保持了下游性能。</p>
<h4 id="_146">一句话总结：</h4>
<p>本文提出了一种基于LLM的推荐解码加速框架DARE，通过定制检索池和宽松验证，显著提高了推荐系统的生成效率。</p>
<hr />
<h2 id="you-augment-me-exploring-chatgpt-based-data-augmentation-for-semantic-code-search"><a href="http://arxiv.org/abs/2408.05542v2">You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic Code Search</a></h2>
<p>发布时间：2024-08-10</p>
<p>作者：Yanlin Wang, Lianghong Guo, Ensheng Shi, Wenqing Chen, Jiachi Chen, Wanjun Zhong, Menghan Wang, Hui Li, Hongyu Zhang, Ziyu Lyu, Zibin Zheng</p>
<h4 id="_147">中文摘要：</h4>
<p>代码搜索在软件开发中扮演着至关重要的角色，它允许开发者通过自然语言查询检索和重用代码。尽管随着高质量数据的增加，代码搜索模型的性能得到提升，但获取此类数据可能具有挑战性和成本高昂。最近，大型语言模型（LLMs）如ChatGPT在自然语言和编程语言的理解与生成方面取得了显著的进步，通过简单的提示提供用户友好的交互。受这些进步的启发，我们提出了一种新颖的方法ChatDANCE，该方法利用由大型语言模型生成的高质量和多样化的增强数据，并利用过滤机制来消除低质量的增强数据。具体来说，我们首先提出了一套专为源代码和查询设计的ChatGPT提示规则。然后，我们利用ChatGPT根据相应的提示重写代码和查询，并提出了一种过滤机制，该机制训练了一个从骨干模型UniXcoder中提取的交叉编码器，以过滤掉匹配分数低的代码和查询对。最后，我们使用获得的高质量增强数据重新训练骨干模型。实验结果表明，ChatDANCE实现了最先进的性能，将最佳基线提高了13.2%（R@1）和7%（MRR）。令人惊讶的是，我们发现这种增强-过滤-重新训练策略使得骨干模型（UniXcoder）能够自我增长。此外，广泛的实验表明了每个组件的有效性，ChatDANCE在不同超参数设置下具有稳定的性能。此外，我们进行了定性和定量分析，以研究ChatDANCE为何表现良好，并发现它学习到了更均匀的表示分布，并有效地对齐了代码和查询空间。</p>
<h4 id="_148">一句话总结：</h4>
<p>ChatDANCE通过利用大型语言模型生成的增强数据，结合过滤机制和重新训练策略，显著提升了代码搜索的性能。</p>
<hr />
<h2 id="context-driven-index-trimming-a-data-quality-perspective-to-enhancing-precision-of-ralms"><a href="http://arxiv.org/abs/2408.05524v1">Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs</a></h2>
<p>发布时间：2024-08-10</p>
<p>作者：Kexin Ma, Ruochun Jin, Xi Wang, Huan Chen, Jing Ren, Yuhua Tang</p>
<h4 id="_149">中文摘要：</h4>
<p>检索增强大型语言模型（RALMs）在提高生成响应的准确性方面取得了显著进展。然而，现有研究往往忽略了检索结果中的数据质量问题，这些问题通常是由不准确的存在基于向量距离的检索方法引起的。我们提出通过上下文驱动索引修剪（CDIT）框架从数据质量的角度提高RALMs答案的精确度，其中上下文匹配依赖（CMDs）被用作逻辑数据质量规则来捕捉和调节检索上下文之间的一致性。基于大型语言模型（LLMs）的语义理解能力，CDIT可以有效地识别和丢弃与查询上下文不一致的检索结果，并进一步修改数据库中的索引，从而提高答案质量。实验在具有挑战性的问答任务上进行了验证。此外，CDIT的灵活性通过其与各种语言模型和索引方法的兼容性得到验证，这为同时提高RALMs的数据质量和检索精度提供了一种有希望的方法。</p>
<h4 id="_150">一句话总结：</h4>
<p>通过上下文驱动索引修剪框架，我们提高了检索增强大型语言模型的数据质量和检索精度。</p>
<hr />
<h2 id="laida-linguistics-aware-in-context-learning-with-data-augmentation-for-metaphor-components-identification"><a href="http://arxiv.org/abs/2408.05404v1">LaiDA: Linguistics-aware In-context Learning with Data Augmentation for Metaphor Components Identification</a></h2>
<p>发布时间：2024-08-10</p>
<p>作者：Hongde Liu, Chenyuan He, Feiyang Meng, Changyong Niu, Yuxiang Jia</p>
<h4 id="_151">中文摘要：</h4>
<p>隐喻成分识别（MCI）有助于增强机器对隐喻的理解，从而推进下游的自然语言处理任务。然而，隐喻的复杂性、多样性和对上下文及背景知识的依赖性为MCI带来了重大挑战。大型语言模型（LLMs）由于其强大的语义分析和广泛的知识体系，为准确理解复杂的自然语言文本提供了新的途径。在本研究中，提出了一种基于LLM的新框架，命名为语言感知的上下文学习与数据增强（LaiDA）。具体来说，利用ChatGPT和监督微调来定制高质量数据集。LaiDA包含一个用于预训练的明喻数据集。图注意力网络编码器生成丰富的语言特征表示以检索相似示例。随后，使用包含语言相似示例的提示对LLM进行微调。LaiDA在NLPCC2024共享任务9的子任务2中排名第二，证明了其有效性。代码和数据可在https://github.com/WXLJZ/LaiDA获取。</p>
<h4 id="_152">一句话总结：</h4>
<p>本研究提出了一种基于大型语言模型的隐喻成分识别新框架LaiDA，通过语言感知的上下文学习和数据增强，有效提升了机器对隐喻的理解能力。</p>
<hr />
<h2 id="temporal-analysis-and-repair-of-flaky-dockerfiles"><a href="http://arxiv.org/abs/2408.05379v1">Temporal Analysis and Repair of Flaky Dockerfiles</a></h2>
<p>发布时间：2024-08-09</p>
<p>作者：Taha Shabani, Noor Nashid, Parsa Alian, Ali Mesbah</p>
<h4 id="_153">中文摘要：</h4>
<p>Dockerfile的不稳定性（在没有Dockerfile或项目源代码更改的情况下出现的不可预测的构建行为）在持续集成和持续交付（CI/CD）管道中带来了重大挑战。这一问题可能导致部署不可靠和调试工作量的增加，然而在当前的研究中，这一问题仍被低估。我们对Dockerfile的不稳定性进行了系统分析，提出了一个包含依赖相关错误和服务器连接问题等常见不稳定类别在内的全面分类法。此外，我们引入了FlakiDock工具，该工具利用大型语言模型和检索增强生成技术，结合动态分析和迭代反馈循环来自动修复不稳定的Dockerfile。我们的评估显示，FlakiDock实现了73.55%的修复准确率，比现有的工具如PARFUM（提高了12,581%）和基于GPT-4的提示（提高了94.63%）表现更佳。这些结果强调了FlakiDock在解决Dockerfile不稳定性和提高构建可靠性方面的有效性。</p>
<h4 id="_154">一句话总结：</h4>
<p>FlakiDock通过利用大型语言模型和检索增强生成技术，实现了对Dockerfile不稳定性的有效修复，显著提升了构建的可靠性。</p>
<hr />
<h2 id="fist-financial-style-transfer-with-hallucination-and-creativity-control-framework"><a href="http://arxiv.org/abs/2408.05365v1">FiST-Financial Style Transfer with Hallucination and Creativity Control Framework</a></h2>
<p>发布时间：2024-08-09</p>
<p>作者：Sohini Roychowdhury, Marko Krema, Brian Moore, Xingjian Lai, Dike Effedua, Bharat Jethwani</p>
<h4 id="_155">中文摘要：</h4>
<p>使用通用的大型语言模型生成财务报告面临两大挑战，包括缺乏复合句和幻觉。高级提示工程和检索增强生成（RAG）技术无法治愈写作风格的差异。在这项工作中，我们提出了一种新颖的两阶段微调过程，其中公共领域的财务报告被处理成提示补全，并使用简单的LLM提示进行增强，然后通过最小指令和表格数据输入实现部分财务报告的生成。我们提出的微调框架将正确问答的数量翻倍，并将幻觉减少了超过50%。此外，两阶段微调模型具有更低的困惑度，提高了ROUGE、TER和BLEU分数，具有更高的创造性和知识密度，同时不确定性交叉熵更低。</p>
<h4 id="_156">一句话总结：</h4>
<p>本研究提出了一种两阶段微调框架，显著提高了财务报告生成模型的准确性和创造力。</p>
<hr />
<h2 id="a-hybrid-rag-system-with-comprehensive-enhancement-on-complex-reasoning"><a href="http://arxiv.org/abs/2408.05141v3">A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning</a></h2>
<p>发布时间：2024-08-09</p>
<p>作者：Ye Yuan, Chengwu Liu, Jingyang Yuan, Gongbo Sun, Siqi Li, Ming Zhang</p>
<h4 id="_157">中文摘要：</h4>
<p>检索增强生成（RAG）是一种框架，它通过整合外部知识库，使大型语言模型（LLMs）能够提高其准确性并减少幻觉。在本文中，我们介绍了一种通过一系列优化措施增强的混合RAG系统，这些优化措施显著提高了检索质量、增强了推理能力，并提升了数值计算能力。我们优化了网页中的文本块和表格，添加了属性预测器以减少幻觉，进行了LLM知识提取器和知识图谱提取器的开发，并最终构建了一个包含所有参考文献的推理策略。我们通过Meta CRAG KDD Cup 2024竞赛对CRAG数据集进行了系统评估。无论是本地评估还是在线评估，都表明我们的系统显著增强了复杂的推理能力。在本地评估中，与基线模型相比，我们显著提高了准确率并降低了错误率，实现了分数的显著提升。同时，我们在在线评估中取得了优异的成绩，展示了所提出系统的性能和泛化能力。我们系统的源代码已发布在\url{https://gitlab.aicrowd.com/shizueyy/crag-new}。</p>
<h4 id="_158">一句话总结：</h4>
<p>本文提出了一种通过优化检索质量、增强推理能力和提升数值计算能力的混合RAG系统，显著提高了大型语言模型的复杂推理能力。</p>
<hr />
<h2 id="retrieval-augmented-code-completion-for-local-projects-using-large-language-models"><a href="http://arxiv.org/abs/2408.05026v1">Retrieval-augmented code completion for local projects using large language models</a></h2>
<p>发布时间：2024-08-09</p>
<p>作者：Marko Hostnik, Marko Robnik-Šikonja</p>
<h4 id="_159">中文摘要：</h4>
<p>随着大型语言模型（LLMs）在软件开发者中的广泛应用，其隐私和计算需求成为商业解决方案和LLMs使用中的问题。在本研究中，我们关注使用约160百万参数的LLMs，这些模型适合本地执行以及从本地项目中进行检索增强。我们基于transformer架构训练了两个模型，即生成模型GPT-2和检索适应的RETRO模型，在开源Python文件上进行训练，并对其进行实证评估和比较，证实了基于向量嵌入的检索的优势。此外，我们通过上下文检索增强生成（In-context retrieval-augmented generation）来提高模型性能，该方法基于标记的Jaccard相似度检索代码片段。我们在更大的模型上评估了上下文检索增强生成，并得出结论，尽管该方法简单，但比使用RETRO架构更适合。我们强调了适当标记化在实现LLMs在代码补全中全部潜力中的关键作用。</p>
<h4 id="_160">一句话总结：</h4>
<p>本研究通过结合向量嵌入检索和上下文检索增强生成，提高了大型语言模型在代码补全中的性能，并强调了适当标记化的重要性。</p>
<hr />
<h2 id="rag-and-roll-an-end-to-end-evaluation-of-indirect-prompt-manipulations-in-llm-based-application-frameworks"><a href="http://arxiv.org/abs/2408.05025v2">Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks</a></h2>
<p>发布时间：2024-08-09</p>
<p>作者：Gianluca De Stefano, Lea Schönherr, Giancarlo Pellegrino</p>
<h4 id="_161">中文摘要：</h4>
<p>检索增强生成（RAG）是一种常用于为模型提供分布外知识的技巧。这个过程涉及收集、索引、检索并向大型语言模型（LLM）提供信息以生成响应。尽管由于其灵活性和低成本而日益受到欢迎，但RAG的安全影响尚未得到广泛研究。此类系统的数据通常来自公共来源，为攻击者提供了间接提示注入的途径，以操纵模型的响应。在本文中，我们研究了RAG系统对端到端间接提示操纵的安全性。首先，我们回顾了现有的RAG框架流程，推导出一个典型架构并确定了关键参数。然后，我们检查了先前的研究，寻找攻击者可以用来执行间接提示操纵的技术。最后，我们实现了Rag 'n Roll框架，用于确定针对端到端RAG应用的攻击的有效性。我们的结果表明，现有的攻击大多优化了在检索阶段提升恶意文档排名。然而，更高的排名并不立即转化为可靠的攻击。大多数攻击，在各种配置下，成功率大约在40%，如果将模糊答案视为成功的攻击（包括预期的良性答案），成功率可升至60%。此外，当使用未经优化的文档时，攻击者部署两个（或更多）针对目标查询的文档可以达到与使用优化文档相似的结果。最后，探索RAG的配置空间对阻止攻击的影响有限，其中最成功的组合严重破坏了功能。</p>
<h4 id="_162">一句话总结：</h4>
<p>本文研究了检索增强生成（RAG）系统的安全性，发现现有攻击主要针对检索阶段提升恶意文档排名，但成功率有限，且配置空间探索对防御效果影响不大。</p>
<hr />
<h2 id="hybridrag-integrating-knowledge-graphs-and-vector-retrieval-augmented-generation-for-efficient-information-extraction"><a href="http://arxiv.org/abs/2408.04948v1">HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction</a></h2>
<p>发布时间：2024-08-09</p>
<p>作者：Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, Dhagash Mehta</p>
<h4 id="_163">中文摘要：</h4>
<p>从金融应用中产生的非结构化文本数据（如收益电话会议记录）中提取和解释复杂信息，对大型语言模型（LLMs）来说是一项重大挑战，即使使用当前最佳实践，如检索增强生成（RAG）（称为VectorRAG技术，它利用向量数据库进行信息检索）也难以应对，因为存在诸如领域特定术语和文档复杂格式等挑战。我们提出了一种基于知识图谱（KGs）的RAG技术（称为GraphRAG）和VectorRAG技术的结合，称为HybridRAG，的新型方法，以增强从金融文档中提取信息的问答（Q&amp;A）系统，该系统已被证明能够生成准确且与上下文相关的答案。通过在一系列以问答格式出现的金融收益电话会议记录文档上的实验，这些文档提供了自然的一对一真实问答对，我们展示了HybridRAG在检索和生成阶段均优于传统的VectorRAG和GraphRAG，无论是在检索准确性还是答案生成方面。</p>
<h4 id="_164">一句话总结：</h4>
<p>HybridRAG通过结合知识图谱和向量数据库检索，显著提升了从金融文档中提取信息的问答系统的准确性和上下文相关性。</p>
<hr />
<h2 id="confusedpilot-confused-deputy-risks-in-rag-based-llms"><a href="http://arxiv.org/abs/2408.04870v3">ConfusedPilot: Confused Deputy Risks in RAG-based LLMs</a></h2>
<p>发布时间：2024-08-09</p>
<p>作者：Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari</p>
<h4 id="_165">中文摘要：</h4>
<p>检索增强生成（RAG）是一种过程，其中大型语言模型（LLM）从数据库中检索有用信息，然后生成响应。它在企业日常业务操作中变得越来越受欢迎，例如，Microsoft 365的Copilot已经积累了数百万家企业。然而，采用基于RAG的系统所带来的安全影响尚不明确。在本文中，我们介绍了ConfusedPilot，这是一类RAG系统的安全漏洞，它会使Copilot困惑，并导致其响应中的完整性和机密性违规。首先，我们研究了一种漏洞，该漏洞将恶意文本嵌入到RAG中修改的提示中，从而破坏LLM生成的响应。其次，我们展示了一种漏洞，该漏洞通过检索过程中的缓存机制泄露秘密数据。第三，我们研究了如何利用这两种漏洞在企业内部传播虚假信息，并最终影响其运营，如销售和制造。我们还通过研究基于RAG系统的架构来探讨这些攻击的根本原因。这项研究突出了当今基于RAG系统的安全漏洞，并提出了确保未来基于RAG系统的设计指南。</p>
<h4 id="_166">一句话总结：</h4>
<p>本文揭示了基于RAG系统的安全漏洞，并提出了确保未来RAG系统安全的指导原则。</p>
<hr />
<h2 id="next-generation-wi-fi-networks-with-generative-ai-design-and-insights"><a href="http://arxiv.org/abs/2408.04835v1">Next-Generation Wi-Fi Networks with Generative AI: Design and Insights</a></h2>
<p>发布时间：2024-08-09</p>
<p>作者：Jingyu Wang, Xuming Fang, Dusit Niyato, Tie Liu</p>
<h4 id="_167">中文摘要：</h4>
<p>生成式人工智能（GAI），以其在图像和文本处理方面的强大能力而闻名，同时也为未来无线网络的设计和性能提升带来了巨大的潜力。在本文中，我们探讨了GAI在下一代Wi-Fi网络中的变革潜力，利用其先进的能力来解决关键挑战并提升整体网络性能。我们首先回顾了主要Wi-Fi世代的发展，并说明了未来Wi-Fi网络可能遇到的挑战。接着，我们介绍了典型的GAI模型，并详细阐述了它们在Wi-Fi网络优化、性能提升以及其他应用中的潜在能力。此外，我们还提供了一个案例研究，其中我们提出了一种由检索增强型大型语言模型（RA-LLM）支持的Wi-Fi设计框架，该框架有助于问题制定，随后使用基于生成扩散模型（GDM）的深度强化学习（DRL）框架来解决，以优化各种网络参数。数值结果表明，我们提出的算法在高密度部署场景中是有效的。最后，我们为GAI辅助的Wi-Fi网络提供了一些潜在的未来研究方向。</p>
<h4 id="_168">一句话总结：</h4>
<p>本文探讨了生成式人工智能在下一代Wi-Fi网络设计和性能提升中的潜在应用，并通过案例研究验证了其在高密度部署场景中的有效性。</p>
<hr />
<h2 id="hybrid-student-teacher-large-language-model-refinement-for-cancer-toxicity-symptom-extraction"><a href="http://arxiv.org/abs/2408.04775v1">Hybrid Student-Teacher Large Language Model Refinement for Cancer Toxicity Symptom Extraction</a></h2>
<p>发布时间：2024-08-08</p>
<p>作者：Reza Khanmohammadi, Ahmed I. Ghanem, Kyle Verdecchia, Ryan Hall, Mohamed Elshaikh, Benjamin Movsas, Hassan Bagher-Ebadian, Bing Luo, Indrin J. Chetty, Tuka Alhanai, Kundan Thind, Mohammad M. Ghassemi</p>
<h4 id="_169">中文摘要：</h4>
<p>大型语言模型（LLMs）在临床症状提取方面具有巨大潜力，但其应用于医疗环境受到隐私担忧、计算限制和运营成本的制约。本研究通过一种新颖的迭代优化方法，探讨了紧凑型LLMs在癌症毒性症状提取中的优化。我们采用学生-教师架构，使用Zephyr-7b-beta和Phi3-mini-128作为学生模型，GPT-4o作为教师模型，动态地在提示优化、检索增强生成（RAG）和微调策略之间进行选择。在涵盖12种放疗后毒性症状的294份临床记录上的实验表明了该方法的有效性。RAG方法证明最为高效，在优化过程中，Zephyr-7b-beta的平均准确率从0.32提升到0.73，Phi3-mini-128从0.40提升到0.87。在测试集中，两个模型在所有症状上的准确率均提高了约0.20。值得注意的是，这种改进的成本仅为GPT-4o的1/45（Zephyr）和1/79（Phi-3）。这些结果突显了迭代优化技术在增强紧凑型LLMs临床应用能力方面的潜力，在医疗环境中实现了性能、成本效益和隐私保护的平衡。</p>
<h4 id="_170">一句话总结：</h4>
<p>本研究通过迭代优化方法显著提升了紧凑型LLMs在癌症毒性症状提取中的性能，同时降低了成本，为医疗环境中的隐私保护提供了新的解决方案。</p>
<hr />
<h2 id="towards-explainable-network-intrusion-detection-using-large-language-models"><a href="http://arxiv.org/abs/2408.04342v1">Towards Explainable Network Intrusion Detection using Large Language Models</a></h2>
<p>发布时间：2024-08-08</p>
<p>作者：Paul R. B. Houssel, Priyanka Singh, Siamak Layeghy, Marius Portmann</p>
<h4 id="_171">中文摘要：</h4>
<p>大型语言模型（LLMs）已经彻底改变了自然语言处理任务，尤其是作为聊天代理。然而，它们在威胁检测问题上的适用性尚不明确。本文探讨了在计算需求高的情况下，将LLMs作为网络入侵检测系统（NIDS）的可行性，主要是为了提高可解释性。此外，大量资源已经投入到LLMs的开发中，它们可能对NIDS具有实用性。目前最先进的NIDS依赖于人工基准数据集，导致其在实际网络环境中的应用性能出现偏差。因此，我们将GPT-4和LLama3模型与传统的架构和基于transformer的模型进行比较，以评估它们在不依赖人工偏差数据集的情况下检测恶意NetFlows的能力，但仅依赖于它们庞大的预训练知识。我们的结果表明，尽管LLMs在精确攻击检测方面存在困难，但它们在可解释NIDS的道路上具有巨大的潜力。我们的初步探索表明，LLMs不适合检测恶意NetFlows。然而，最有希望的是，这些模型在NIDS中作为补充代理具有显著潜力，尤其是在与检索增强生成（RAG）和功能调用能力集成时，提供解释并协助威胁响应。</p>
<h4 id="_172">一句话总结：</h4>
<p>本文研究了将大型语言模型应用于网络入侵检测系统的可行性，发现其在提供可解释性和辅助威胁响应方面具有潜力，尽管在精确攻击检测方面存在挑战。</p>
<hr />
<h2 id="efficientrag-efficient-retriever-for-multi-hop-question-answering"><a href="http://arxiv.org/abs/2408.04259v1">EfficientRAG: Efficient Retriever for Multi-Hop Question Answering</a></h2>
<p>发布时间：2024-08-08</p>
<p>作者：Ziyuan Zhuang, Zhiyang Zhang, Sitao Cheng, Fangkai Yang, Jia Liu, Shujian Huang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</p>
<h4 id="_173">中文摘要：</h4>
<p>本文提出了一种名为EfficientRAG的高效检索器，用于解决多跳查询等复杂问题的多跳问答。与现有方法依赖多次调用大型语言模型（LLMs）不同，EfficientRAG在每次迭代中无需调用LLMs即可生成新的查询，并过滤掉无关信息。实验结果表明，EfficientRAG在三个开放域多跳问答数据集上优于现有的RAG方法。</p>
<h4 id="_174">一句话总结：</h4>
<p>EfficientRAG通过减少对大型语言模型的依赖，实现了高效的多跳问答检索。</p>
<hr />
<h2 id="medical-graph-rag-towards-safe-medical-large-language-model-via-graph-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2408.04187v1">Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-08-08</p>
<p>作者：Junde Wu, Jiayuan Zhu, Yunli Qi</p>
<h4 id="_175">中文摘要：</h4>
<p>我们提出了一种新型的基于图的检索增强生成（RAG）框架，专门针对医疗领域设计，称为MedGraphRAG，旨在增强大型语言模型（LLM）的能力并生成基于证据的结果，从而在处理私人医疗数据时提高安全性和可靠性。我们的综合流程从混合静态语义方法对文档分块开始，显著提高了对传统方法的上下文捕获能力。提取的实体用于创建一个三层层次图结构，将实体与源自医学论文和词典的基础医学知识相连接。然后，这些实体相互连接形成元图，根据语义相似性合并以开发一个全面的全球图。这种结构支持精确的信息检索和响应生成。检索过程采用U-retrieve方法来平衡LLM的全局意识和索引效率。我们的方法通过比较文档分块、图构建和信息检索的各种方法进行了全面的消融研究。结果表明，我们的层次图构建方法在多个医疗问答基准测试中始终优于最先进的模型，并且确认生成的响应包括源文档，显著增强了医疗LLM在实际应用中的可靠性。代码将在：https://github.com/MedicineToken/Medical-Graph-RAG/tree/main</p>
<h4 id="_176">一句话总结：</h4>
<p>MedGraphRAG通过构建层次图结构，结合U-retrieve方法，显著提升了医疗领域LLM在处理私人医疗数据时的安全性和可靠性。</p>
<hr />
<h2 id="improving-retrieval-augmented-code-comment-generation-by-retrieving-for-generation"><a href="http://arxiv.org/abs/2408.03623v1">Improving Retrieval-Augmented Code Comment Generation by Retrieving for Generation</a></h2>
<p>发布时间：2024-08-07</p>
<p>作者：Hanzhen Lu, Zhongxin Liu</p>
<h4 id="_177">中文摘要：</h4>
<p>代码注释生成旨在自动从源代码中生成高质量的注释，这一领域已经研究了多年。最近的研究提出了将信息检索技术与神经生成模型相结合来解决这一问题，即检索增强注释生成（RACG）方法，并取得了最先进的结果。然而，先前工作中的检索器是独立于其生成器构建的。这导致检索到的示例不一定是最有用的，限制了现有方法的性能。为了解决这一局限性，我们提出了一种新颖的训练策略，使检索器能够从生成器的反馈中学习并检索用于生成的示例。具体来说，在训练过程中，我们使用检索器检索前k个示例并计算它们的检索分数，并使用生成器根据每个示例计算生成损失。通过将检索器检索到的得分高的示例与生成器观察到的低损失示例对齐，检索器可以学习检索出最能提高生成注释质量的示例。基于这一策略，我们提出了一种名为JOINTCOM的新颖RACG方法，并在两个真实世界数据集（JCSD和PCSD）上对其进行了评估。实验结果表明，我们的方法在两个数据集上的五个指标上超过了最先进的基线，提高了7.3%至30.0%。我们还进行了人工评估，以比较JOINTCOM与表现最佳的基线。结果表明，JOINTCOM优于基线，生成的注释更加自然、信息丰富且有用。</p>
<h4 id="_178">一句话总结：</h4>
<p>提出了一种名为JOINTCOM的RACG方法，通过检索器与生成器之间的反馈学习，显著提高了代码注释生成的质量和自然度。</p>
<hr />
<h2 id="a-comparison-of-llm-finetuning-methods-evaluation-metrics-with-travel-chatbot-use-case"><a href="http://arxiv.org/abs/2408.03562v1">A Comparison of LLM Finetuning Methods &amp; Evaluation Metrics with Travel Chatbot Use Case</a></h2>
<p>发布时间：2024-08-07</p>
<p>作者：Sonia Meyer, Shreya Singh, Bertha Tam, Christopher Ton, Angel Ren</p>
<h4 id="_179">中文摘要：</h4>
<p>本研究比较了大型语言模型（LLM）微调方法，包括量化低秩适配器（QLoRA）、检索增强微调（RAFT）和基于人类反馈的强化学习（RLHF），并进一步比较了LLM评估方法，包括“黄金答案”的端到端（E2E）基准方法、传统的自然语言处理（NLP）指标、RAG评估（Ragas）、OpenAI GPT-4评估指标和人工评估，使用旅行聊天机器人用例。旅行数据集通过请求来自旅行相关subreddits的帖子从Reddit API获取，以获取旅行相关对话提示和个性化旅行体验，并为每种微调方法进行了增强。我们使用了两个用于微调研究的预训练LLM：LLaMa 2 7B和Mistral 7B。QLoRA和RAFT应用于这两个预训练模型。这些模型的推理结果与上述指标进行了广泛评估。根据人工评估和一些GPT-4指标，最佳模型是Mistral RAFT，因此它经历了基于人类反馈的强化学习（RLHF）训练流程，并最终被评为最佳模型。我们的主要发现是：1）定量和Ragas指标与人工评估不一致，2）OpenAI GPT-4评估与人工评估最为一致，3）在评估过程中保持人类参与至关重要，因为4）传统的NLP指标不足，5）Mistral通常优于LLaMa，6）RAFT优于QLoRA，但仍需后处理，7）RLHF显著提高了模型性能。下一步包括提高数据质量、增加数据量、探索RAG方法，并专注于特定城市的资料收集，这将通过缩小焦点来提高数据质量，同时创造一个有用的产品。</p>
<h4 id="_180">一句话总结：</h4>
<p>本研究通过旅行聊天机器人用例，比较了不同LLM微调和评估方法，发现RLHF训练的Mistral RAFT模型在人类评估和GPT-4指标中表现最佳。</p>
<hr />
<h2 id="exploring-rag-based-vulnerability-augmentation-with-llms"><a href="http://arxiv.org/abs/2408.04125v1">Exploring RAG-based Vulnerability Augmentation with LLMs</a></h2>
<p>发布时间：2024-08-07</p>
<p>作者：Seyed Shayan Daneshvar, Yu Nong, Xu Yang, Shaowei Wang, Haipeng Cai</p>
<h4 id="_181">中文摘要：</h4>
<p>检测漏洞是维护软件系统完整性、可用性和安全性的关键任务。近年来，利用基于深度学习（DL）的模型进行漏洞检测已成为常态。然而，这种基于深度学习的漏洞检测器（DLVD）面临着有效训练所需的大量数据集的短缺。数据增强可以潜在地缓解数据短缺的问题，但增强易受攻击的代码具有挑战性，需要设计一个既能保持漏洞又能生成易受攻击代码的生成解决方案。因此，生成易受攻击代码样本的工作受到了限制，先前的工作仅关注生成包含单个语句或特定类型漏洞的样本。最近，大型语言模型（LLMs）被用于解决各种代码生成和理解任务，并显示出鼓舞人心的结果，尤其是在与检索增强生成（RAG）结合使用时。在本研究中，我们探索了三种使用LLMs增强单语句和多语句漏洞的策略，即变异、注入和扩展。我们使用两种LLMs在三个漏洞数据集和三个DLVD模型上对我们提出的方法进行了广泛的评估。我们的结果表明，基于注入的聚类增强RAG方法在f1分数上分别比基线设置（NoAug）、Vulgen、VGX（两种SOTA方法）和随机过采样（ROS）提高了30.80%、27.48%、27.93%和15.41%，平均生成5K个易受攻击的样本，并且使用15K个生成的易受攻击样本时分别提高了53.84%、54.10%、69.90%和40.93%。我们的方法通过以每1000个样本1.88美元的低价生成1000个样本，证明了其在大规模数据增强方面的可行性。</p>
<h4 id="_182">一句话总结：</h4>
<p>本研究提出了一种基于大型语言模型的数据增强方法，有效提高了基于深度学习的漏洞检测器的性能。</p>
<hr />
<h2 id="kapo-knowledge-aware-preference-optimization-for-controllable-knowledge-selection-in-retrieval-augmented-language-models"><a href="http://arxiv.org/abs/2408.03297v1">KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models</a></h2>
<p>发布时间：2024-08-06</p>
<p>作者：Ruizhe Zhang, Yongxin Xu, Yuzhen Xiao, Runchuan Zhu, Xinke Jiang, Xu Chu, Junfeng Zhao, Yasha Wang</p>
<h4 id="_183">中文摘要：</h4>
<p>通过整合外部知识，检索增强生成（RAG）已成为缓解大型语言模型（LLMs）在处理知识密集型任务时遇到的幻觉问题的有效策略。然而，在将外部非参数支持证据与内部参数化知识整合的过程中，不可避免地会出现知识冲突，导致模型响应的混淆。为了增强LLMs在各种情境下的知识选择，一些研究集中于通过指令调整来细化其行为模式。尽管如此，由于缺乏明确的负面信号和比较目标，以这种方式微调的模型在复杂和现实的检索场景中仍可能表现出不受欢迎的行为。为此，我们提出了一种名为KaPO（知识感知偏好优化）的方法，旨在实现真实检索场景中的可控知识选择。具体来说，我们探索和模拟了不同情境组合下的错误类型，并通过偏好优化方法学习如何避免这些负面信号。同时，通过调整响应长度与代表不同行为模式的偏好数据比例之间的平衡，我们以平衡的方式增强了LLMs的遵循能力和噪声鲁棒性。实验结果表明，KaPO在处理知识冲突方面优于先前方法超过37%，同时在各种分布外数据集上表现出稳健的泛化能力。</p>
<h4 id="_184">一句话总结：</h4>
<p>KaPO通过知识感知偏好优化，有效提升了大型语言模型在知识密集型任务中的知识选择能力和检索场景下的鲁棒性。</p>
<hr />
<h2 id="openomni-a-collaborative-open-source-tool-for-building-future-ready-multimodal-conversational-agents"><a href="http://arxiv.org/abs/2408.03047v1">OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents</a></h2>
<p>发布时间：2024-08-06</p>
<p>作者：Qiang Sun, Yuanyi Luo, Sirui Li, Wenxiao Zhang, Wei Liu</p>
<h4 id="_185">中文摘要：</h4>
<p>多模态对话代理因其提供自然且类似人类的交互而备受青睐。然而，目前缺乏全面端到端解决方案来支持协作开发和基准测试。尽管像GPT-4o和Gemini这样的专有系统展示了音频、视频和文本的出色集成，响应时间为200-250毫秒，但在平衡延迟、准确性、成本和数据隐私方面仍存在挑战。为了更好地理解和量化这些问题，我们开发了OpenOmni，这是一个开源的端到端管道基准测试工具，集成了诸如语音转文本、情感检测、检索增强生成、大型语言模型等先进技术，并具备集成自定义模型的能力。OpenOmni支持本地和云部署，确保数据隐私并支持延迟和准确性基准测试。这个灵活的框架允许研究人员自定义管道，专注于真实瓶颈，并促进快速概念验证开发。OpenOmni可以显著提升如室内辅助视觉障碍人士等应用，推进人机交互。我们的演示视频可在https://www.youtube.com/watch?v=zaSiT3clWqY查看，演示可通过https://openomni.ai4wa.com获取，代码可在https://github.com/AI4WA/OpenOmniFramework获取。</p>
<h4 id="_186">一句话总结：</h4>
<p>OpenOmni是一个开源的多模态对话代理基准测试工具，旨在解决现有多模态交互系统在延迟、准确性、成本和数据隐私方面的挑战。</p>
<hr />
<h2 id="a-real-time-adaptive-multi-stream-gpu-system-for-online-approximate-nearest-neighborhood-search"><a href="http://arxiv.org/abs/2408.02937v1">A Real-Time Adaptive Multi-Stream GPU System for Online Approximate Nearest Neighborhood Search</a></h2>
<p>发布时间：2024-08-06</p>
<p>作者：Yiping Sun, Yang Shi, Jiaolong Du</p>
<h4 id="_187">中文摘要：</h4>
<p>近年来，近似最近邻搜索（Approximate Nearest Neighbor Search，ANNS）在现代搜索和推荐系统中扮演着关键角色，尤其是在检索增强生成等新兴大型语言模型（LLM）应用中。人们正在积极探索利用GPU的并行计算能力来满足ANNS的大量需求。然而，现有的系统主要关注离线场景，忽视了在线应用中需要实时插入新向量的独特需求。这种局限性使得这些系统在现实场景中效率低下。此外，由于依赖于串行执行流，之前的架构在支持实时插入方面遇到了困难。在本文中，我们介绍了一种新型的实时自适应多流GPU ANNS系统（Real-Time Adaptive Multi-Stream GPU ANNS System，RTAMS-GANNS）。我们的架构通过三个关键进步实现了其目标：1）我们首先分析了现有GPU ANNS系统中实时插入机制的依赖性，并发现它们依赖于重复的复制和内存分配，这在GPU上显著阻碍了实时效率。作为解决方案，我们引入了一种基于内存块的动态向量插入算法，包括原地重排。2）为了实现并行实时向量插入，我们引入了一种多流并行执行模式，这与现有系统在单个流内串行操作不同。我们的系统利用动态资源池，允许多个流同时执行，而无需额外的执行阻塞。3）通过广泛的实验和比较，我们的方法有效地处理了不同数据集上的不同QPS水平，将延迟降低了40%-80%。该系统还已在现实世界的工业搜索和推荐系统中部署，每天服务数亿用户，并取得了良好的效果。</p>
<h4 id="_188">一句话总结：</h4>
<p>本文提出了一种新型的实时自适应多流GPU ANNS系统，通过优化实时插入机制和并行执行模式，显著提高了近似最近邻搜索的效率。</p>
<hr />
<h2 id="medtrinity-25m-a-large-scale-multimodal-dataset-with-multigranular-annotations-for-medicine"><a href="http://arxiv.org/abs/2408.02900v1">MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine</a></h2>
<p>发布时间：2024-08-06</p>
<p>作者：Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, Yuyin Zhou</p>
<h4 id="_189">中文摘要：</h4>
<p>本文介绍了MedTrinity-25M，这是一个涵盖医学领域的全面、大规模的多模态数据集，包含超过2500万张图像，跨越10种模态，并对超过65种疾病进行了多粒度标注。这些丰富的标注既包括全局文本信息，如疾病/病变类型、模态、特定区域的描述以及区域间关系，也包括对感兴趣区域（ROI）的详细局部标注，包括边界框、分割掩码。与现有方法受限于图像-文本对可用性不同，我们开发了首个自动化的流水线，通过生成多粒度视觉和文本标注（以图像-ROI-描述三元组的形式）来扩展多模态数据，无需任何配对的文本描述。具体来说，收集了来自超过90个不同来源的数据，并使用特定领域的专家模型进行预处理和归一化，以识别与异常区域相关的ROI。然后，我们构建了一个全面的数据库，并提示多模态大型语言模型以识别的ROI作为指导进行检索增强生成，从而产生多粒度文本描述。与现有数据集相比，MedTrinity-25M提供了最丰富的标注，支持包括字幕生成和报告生成在内的全面多模态任务，以及以视觉为中心的任务，如分类和分割。在MedTrinity-25M上进行预训练，我们的模型在VQA-RAD和PathVQA上实现了最先进的性能，超越了多模态大型语言模型和其他代表性SoTA方法。此数据集还可以用于支持大规模预训练多模态医学AI模型，有助于医学领域未来基础模型的发展。</p>
<h4 id="_190">一句话总结：</h4>
<p>MedTrinity-25M是一个包含丰富标注的多模态医学数据集，支持多种医学AI任务，并在多个基准测试中实现了最先进的性能。</p>
<hr />
<h2 id="knowpo-knowledge-aware-preference-optimization-for-controllable-knowledge-selection-in-retrieval-augmented-language-models"><a href="http://arxiv.org/abs/2408.03297v2">KnowPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models</a></h2>
<p>发布时间：2024-08-06</p>
<p>作者：Ruizhe Zhang, Yongxin Xu, Yuzhen Xiao, Runchuan Zhu, Xinke Jiang, Xu Chu, Junfeng Zhao, Yasha Wang</p>
<h4 id="_191">中文摘要：</h4>
<p>通过整合外部知识，检索增强生成（RAG）已成为缓解大型语言模型（LLMs）在处理知识密集型任务时遇到的幻觉问题的有效策略。然而，在将外部非参数支持证据与内部参数化知识整合的过程中，不可避免地会出现知识冲突，导致模型响应的混淆。为了增强LLMs在各种情境下的知识选择，一些研究集中于通过指令调整来细化其行为模式。尽管如此，由于缺乏明确的负面信号和比较目标，以这种方式微调的模型可能仍然表现出诸如情境无知和情境过度包含等不良行为。为此，我们提出了一种名为KnowPO的知识感知偏好优化策略，旨在根据实际检索场景中的上下文相关性实现自适应知识选择。具体而言，我们提出了一种构建知识冲突数据集的通用范式，该范式全面覆盖各种错误类型，并通过偏好优化方法学习如何避免这些负面信号。同时，我们提出了一种重写策略和数据比率优化策略来解决偏好不平衡问题。实验结果表明，KnowPO在处理知识冲突方面优于先前方法超过37%，同时在各种分布外数据集上表现出强大的泛化能力。</p>
<h4 id="_192">一句话总结：</h4>
<p>KnowPO通过优化知识选择策略，有效缓解了大型语言模型在处理知识密集型任务时的幻觉问题，并提高了模型在不同情境下的知识选择能力。</p>
<hr />
<h2 id="llm-based-mofs-synthesis-condition-extraction-using-few-shot-demonstrations"><a href="http://arxiv.org/abs/2408.04665v1">LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations</a></h2>
<p>发布时间：2024-08-06</p>
<p>作者：Lei Shi, Zhimeng Liu, Yi Yang, Weize Wu, Yuyang Zhang, Hongbo Zhang, Jing Lin, Siyu Wu, Zihan Chen, Ruiming Li, Nan Wang, Zipeng Liu, Huobin Tan, Hongyi Gao, Yue Zhang, Ge Wang</p>
<h4 id="_193">中文摘要：</h4>
<p>从文献文本中提取金属有机框架（MOFs）的合成条件一直是一个具有挑战性但至关重要的任务，这对于设计具有理想功能的新MOFs的逻辑设计至关重要。近期大型语言模型（LLMs）的出现为这一长期存在的问题提供了颠覆性的新解决方案，最新的研究报道了从MOFs文献中提取正确条件超过90%的F1分数。本文中，我们提出，大多数现有的使用LLMs的合成提取实践仍然停留在原始的无监督学习阶段，这可能导致由于缺乏专业知识而降低提取和应用性能。本研究开创并优化了LLMs提取材料合成条件的少样本情境学习范式。首先，我们提出了一种人机联合数据整理过程，以确保为少样本学习提供高质量的标注示例。其次，我们应用基于检索增强生成（RAG）技术的BM25算法，以自适应地为每个MOFs的提取选择少样本示例。在从84,898个定义明确的MOFs中随机抽取的数据集上，所提出的少样本方法在完全自动评估（比之前的人评估更客观）下，比使用相同GPT-4模型的原始无监督LLM实现了更高的平均F1性能（0.93 vs. 0.81，+14.8%）。通过实际材料实验进一步验证了所提出的方法：与基线无监督LLM相比，所提出的少样本方法平均提高了MOFs结构推断性能（R^2）29.4%。</p>
<h4 id="_194">一句话总结：</h4>
<p>本文提出了一种基于少样本情境学习的LLMs提取材料合成条件的新方法，显著提高了金属有机框架文献中合成条件的提取性能。</p>
<hr />
<h2 id="rag-foundry-a-framework-for-enhancing-llms-for-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2408.02545v1">RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-08-05</p>
<p>作者：Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat, Peter Izsak</p>
<h4 id="_195">中文摘要：</h4>
<p>实施检索增强生成（RAG）系统本质上很复杂，需要深入理解数据、用例以及复杂的设计决策。此外，评估这些系统也面临着重大挑战，需要通过多角度的方法来评估检索准确性和生成质量。我们引入了RAG Foundry，这是一个开源框架，用于增强大型语言模型以用于RAG用例。RAG Foundry将数据创建、训练、推理和评估整合到一个单一的工作流程中，便于创建用于训练和评估大型语言模型在RAG环境下的数据增强数据集。这种整合使得快速原型设计和实验各种RAG技术成为可能，使用户能够轻松地生成数据集并使用内部或专业知识源训练RAG模型。我们通过使用不同的RAG配置增强和微调Llama-3和Phi-3模型，展示了在三个知识密集型数据集上的一致性改进。代码已在https://github.com/IntelLabs/RAGFoundry上以开源形式发布。</p>
<h4 id="_196">一句话总结：</h4>
<p>RAG Foundry是一个开源框架，旨在简化大型语言模型在检索增强生成（RAG）用例中的数据增强、训练和评估过程。</p>
<hr />
<h2 id="wiping-out-the-limitations-of-large-language-models-a-taxonomy-for-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2408.02854v1">Wiping out the limitations of Large Language Models -- A Taxonomy for Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-08-05</p>
<p>作者：Mahei Manhai Li, Irina Nikishina, Özge Sevgili, Martin Semman</p>
<h4 id="_197">中文摘要：</h4>
<p>当前关于检索增强生成（RAG）的研究分散在各个学科领域，由于该技术发展迅速，其分析单元主要集中在技术创新上，而不是在商业环境中的应用。因此，本研究旨在创建一个分类法，以概念化对定义RAG应用的构成特征的全面概述，从而促进IS（信息系统）社区对该技术的采用。据我们所知，迄今为止尚未开发出任何RAG应用分类法。我们描述了开发分类法的方法，包括选择论文的标准、解释我们采用大型语言模型（LLM）支持的方法来提取和识别初始特征的理由，以及我们对概念化分类法的系统过程的简要概述。我们的系统分类法开发过程包括四个迭代阶段，旨在完善和增强我们对RAG核心维度的理解和展示。我们总共开发了五个元维度和十六个维度，以全面捕捉检索增强生成（RAG）应用的概念。在讨论我们的发现时，我们还详细说明了具体的研究领域，并提出了关键的研究问题，以指导未来的信息系统研究人员在探索RAG系统的新兴主题时。</p>
<h4 id="_198">一句话总结：</h4>
<p>本研究开发了一个RAG应用分类法，旨在全面理解RAG的核心维度，并指导信息系统研究人员探索RAG系统的新兴主题。</p>
<hr />
<h2 id="development-of-regai-rubric-enabled-generative-artificial-intelligence"><a href="http://arxiv.org/abs/2408.02811v1">Development of REGAI: Rubric Enabled Generative Artificial Intelligence</a></h2>
<p>发布时间：2024-08-05</p>
<p>作者：Zach Johnson, Jeremy Straub</p>
<h4 id="_199">中文摘要：</h4>
<p>本文提出并评估了一种基于检索增强生成（RAG）和大型语言模型（LLM）的人工智能（AI）新技术：评标辅助生成人工智能（REGAI）。REGAI利用评标，这些评标可以由人工创建或由系统自动生成，以增强LLM在评估目的上的性能。REGAI在经典LLM和基于RAG的LLM技术性能上均有提升。本文描述了REGAI，展示了其性能数据，并讨论了该技术的几个可能的应用领域。</p>
<h4 id="_200">一句话总结：</h4>
<p>本文提出了一种基于评标辅助的生成人工智能技术，旨在提升大型语言模型在评估任务中的性能。</p>
<hr />
<h2 id="wiping-out-the-limitations-of-large-language-models-a-taxonomy-for-retrieval-augmented-generation_1"><a href="http://arxiv.org/abs/2408.02854v2">Wiping out the limitations of Large Language Models -- A Taxonomy for Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-08-05</p>
<p>作者：Mahei Manhai Li, Irina Nikishina, Özge Sevgili, Martin Semman</p>
<h4 id="_201">中文摘要：</h4>
<p>当前关于检索增强生成（RAGs）的研究分散在各个学科领域，由于该技术发展迅速，其分析单元主要集中在技术创新上，而不是在商业环境中的应用。因此，本研究旨在创建一个分类法，以概念化对定义RAG应用的构成特征的全面概述，从而促进该技术在信息系统（IS）领域的采用。据我们所知，迄今为止尚未开发出任何RAG应用分类法。我们描述了开发分类法的方法，包括选择论文的标准、解释采用大型语言模型（LLM）支持的方法来提取和识别初始特征的理由，以及我们对概念化分类法的系统过程的简要概述。我们的系统分类法开发过程包括四个迭代阶段，旨在完善和增强我们对RAG核心维度的理解和展示。我们共开发了五个元维度和十六个维度，以全面捕捉检索增强生成（RAG）应用的概念。在讨论我们的发现时，我们还详细说明了具体的研究领域，并提出了关键的研究问题，以指导未来的信息系统研究人员在探索RAG系统的新兴主题时。</p>
<h4 id="_202">一句话总结：</h4>
<p>本研究旨在构建一个RAG应用分类法，以全面概述其构成特征，并促进该技术在信息系统领域的应用。</p>
<hr />
<h2 id="wiping-out-the-limitations-of-large-language-models-a-taxonomy-for-retrieval-augmented-generation_2"><a href="http://arxiv.org/abs/2408.02854v3">Wiping out the limitations of Large Language Models -- A Taxonomy for Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-08-05</p>
<p>作者：Mahei Manhai Li, Irina Nikishina, Özge Sevgili, Martin Semmann</p>
<h4 id="_203">中文摘要：</h4>
<p>当前关于检索增强生成（RAG）的研究分散在各个学科领域，由于该技术发展迅速，其分析单位主要集中于技术创新，而不是在商业环境中的应用。因此，本研究旨在创建一个分类法，以概念化对定义RAG应用的构成特征的全面概述，从而促进该技术在信息系统（IS）领域的采用。据我们所知，迄今为止尚未开发出RAG应用的分类法。我们描述了开发分类法的方法，包括选择论文的标准、解释我们采用大型语言模型（LLM）支持的策略来提取和识别初始特征的理由，以及我们对概念化分类法的系统过程的简要概述。我们的系统分类法开发过程包括四个迭代阶段，旨在完善和增强我们对RAG核心维度的理解和展示。我们总共开发了五个元维度和十六个维度，以全面捕捉检索增强生成（RAG）应用的概念。在讨论我们的发现时，我们还详细说明了具体的研究领域，并提出了关键的研究问题，以指导未来的信息系统研究人员在探索RAG系统的新兴主题时。</p>
<h4 id="_204">一句话总结：</h4>
<p>本研究旨在构建一个RAG应用分类法，以全面概述其构成特征，并促进该技术在信息系统领域的应用。</p>
<hr />
<h2 id="llm-agents-improve-semantic-code-search"><a href="http://arxiv.org/abs/2408.11058v1">LLM Agents Improve Semantic Code Search</a></h2>
<p>发布时间：2024-08-05</p>
<p>作者：Sarthak Jain, Aditya Dora, Ka Seng Sam, Prabhat Singh</p>
<h4 id="_205">中文摘要：</h4>
<p>代码搜索是许多程序员在解决问题开发解决方案时必须执行的关键任务。当前的方法在处理包含某些歧义或需要相对于代码库的额外上下文的提示时，往往无法准确执行。我们引入了使用检索增强生成（Retrieval Augmented Generation，RAG）驱动的代理注入信息到用户提示中的方法，从而允许更好地输入到嵌入模型中。通过利用RAG，代理使用GitHub存储库中的相关细节增强用户查询，使其更加信息丰富和上下文一致。此外，我们引入了一种多流集成方法，当与代理工作流程结合使用时，可以获得改进的检索准确性，我们将该方法部署在名为repo-rift.com的应用程序中。在CodeSearchNet数据集上的实验结果表明，RepoRift在Success@10上实现了78.2%的成功率，在Success@1上实现了34.6%的成功率。这项研究在语义代码搜索方面取得了重大进步，突出了代理型大型语言模型（LLMs）和RAG增强代码检索系统的潜力。</p>
<h4 id="_206">一句话总结：</h4>
<p>该研究通过引入RAG和代理工作流程，显著提升了代码搜索的准确性，为语义代码搜索领域带来了突破性进展。</p>
<hr />
<h2 id="malade-orchestration-of-llm-powered-agents-with-retrieval-augmented-generation-for-pharmacovigilance"><a href="http://arxiv.org/abs/2408.01869v1">MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance</a></h2>
<p>发布时间：2024-08-03</p>
<p>作者：Jihye Choi, Nils Palumbo, Prasad Chalasani, Matthew M. Engelhard, Somesh Jha, Anivarya Kumar, David Page</p>
<h4 id="_207">中文摘要：</h4>
<p>在大型语言模型（LLMs）的时代，鉴于它们在文本理解和生成方面的卓越能力，我们迎来了前所未有的机会，可以开发基于LLM的新方法，以实现可信的医疗知识综合、提取和摘要。本文聚焦于药物警戒（PhV）问题，其重要性和挑战在于从多样化的文本来源中识别不良反应事件（ADEs），如医学文献、临床笔记和药物标签。不幸的是，这项任务受到包括药物和结果术语变化以及ADE描述通常被埋藏在大量叙述性文本中的因素阻碍。我们提出了MALADE，这是第一个有效利用LLM和检索增强生成技术从药物标签数据中提取ADEs的协作多智能体系统。这项技术涉及将相关文本资源中提取的信息增强到LLM的查询中，并指导LLM生成与增强数据一致的响应。MALADE是一个通用的LLM无关架构，其独特能力包括：（1）利用各种外部来源，如医学文献、药物标签和FDA工具（例如，OpenFDA药物信息API），（2）以结构化格式提取药物-结果关联及其强度，以及（3）为已建立的关联提供解释。通过实例化GPT-4 Turbo或GPT-4o以及FDA药物标签数据，MALADE在针对OMOP真实ADE表的ROC曲线下面积为0.90的情况下展示了其有效性。我们的实现利用了Langroid多智能体LLM框架，可在https://github.com/jihyechoi77/malade找到。</p>
<h4 id="_208">一句话总结：</h4>
<p>本文提出了一种基于大型语言模型和检索增强生成技术的药物警戒协作多智能体系统，有效提高了从药物标签数据中提取不良反应事件的能力。</p>
<hr />
<h2 id="debateqa-evaluating-question-answering-on-debatable-knowledge"><a href="http://arxiv.org/abs/2408.01419v1">DebateQA: Evaluating Question Answering on Debatable Knowledge</a></h2>
<p>发布时间：2024-08-02</p>
<p>作者：Rongwu Xu, Xuan Qi, Zehan Qi, Wei Xu, Zhijiang Guo</p>
<h4 id="_209">中文摘要：</h4>
<p>随着大型语言模型（LLMs）的兴起，我们能够寻求在LLM聊天机器人上对固有可辩论问题的答案，这需要一种可靠的方式来评估它们的能力。然而，传统的问答基准假设固定答案不足以达到这个目的。为了解决这个问题，我们引入了DebateQA，这是一个包含2,941个可辩论问题的数据集，每个问题都伴随着多个由人类标注的局部答案，这些答案捕捉了各种观点。我们开发了两个指标：视角多样性（Perspective Diversity），它评估视角的全面性；以及争议意识（Dispute Awareness），它评估LLM是否承认问题的可辩论性质。实验表明，这两个指标与人类偏好一致，并且在不同基础模型上具有稳定性。使用DebateQA和这两个指标，我们评估了12种流行的LLMs和检索增强生成方法。我们的发现表明，尽管LLMs通常擅长识别可辩论问题，但它们提供涵盖各种观点的全面答案的能力差异很大。</p>
<h4 id="_210">一句话总结：</h4>
<p>DebateQA通过引入视角多样性和争议意识两个指标，评估LLMs在提供全面、多角度答案方面的能力。</p>
<hr />
<h2 id="rageval-scenario-specific-rag-evaluation-dataset-generation-framework"><a href="http://arxiv.org/abs/2408.01262v1">RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework</a></h2>
<p>发布时间：2024-08-02</p>
<p>作者：Kunlun Zhu, Yifan Luo, Dingling Xu, Ruobing Wang, Shi Yu, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, Zhiyuan Liu, Maosong Sun</p>
<h4 id="_211">中文摘要：</h4>
<p>检索增强生成（RAG）系统在缓解大型语言模型（LLMs）的幻觉方面已经显示出其优势。现有的RAG基准主要关注评估LLMs是否能够正确回答一般知识。然而，它们无法评估RAG系统在处理不同垂直领域数据时的有效性。本文介绍了RAGEval，这是一个用于自动生成评估数据集的框架，以评估不同LLMs在不同场景下的知识使用能力。具体来说，RAGEval从种子文档中总结出一个模式，应用配置生成多样化的文档，并根据文章和配置构建问答对。我们提出了三个新颖的指标，即完整性、幻觉和无关性，以仔细评估LLMs生成的响应。通过在垂直领域基准测试RAG模型，RAGEval能够更好地评估LLMs的知识使用能力，避免了在现有问答数据集中关于知识来源的混淆——即知识是来自参数化记忆还是检索。</p>
<h4 id="_212">一句话总结：</h4>
<p>RAGEval通过在垂直领域基准测试RAG模型，为评估LLMs在不同场景下的知识使用能力提供了一种新的框架。</p>
<hr />
<h2 id="biorag-a-rag-llm-framework-for-biological-question-reasoning"><a href="http://arxiv.org/abs/2408.01107v1">BioRAG: A RAG-LLM Framework for Biological Question Reasoning</a></h2>
<p>发布时间：2024-08-02</p>
<p>作者：Chengrui Wang, Qingqing Long, Xiao Meng, Xunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi Wang, Yuanchun Zhou</p>
<h4 id="_213">中文摘要：</h4>
<p>生命科学研究的问答系统，因其发现速度快、见解不断演变以及知识实体之间复杂的相互作用而具有独特的挑战，这要求在维护全面的知识仓库和准确的信息检索方面保持高效。为了解决这些问题，我们引入了BioRAG，这是一种基于大型语言模型（LLMs）框架的检索增强生成（RAG）方法。我们的方法从解析、索引和分割大量（2200万篇）科学论文作为基本知识开始，随后训练一个针对该领域的特定嵌入模型。此外，我们通过结合领域特定的知识层次结构来增强向量检索过程，这有助于模拟每个查询和上下文之间复杂的相互关系。对于需要最新信息的查询，BioRAG分解问题并采用结合搜索引擎的迭代检索过程进行逐步推理。严格的实验表明，我们的模型在多个生命科学问答任务上优于微调的LLM、带有搜索引擎的LLM以及其他科学RAG框架。</p>
<h4 id="_214">一句话总结：</h4>
<p>BioRAG通过结合大型语言模型和领域特定知识层次结构，为生命科学研究提供了一种高效的问答系统，能够实现准确的信息检索和逐步推理。</p>
<hr />
<h2 id="adaptive-contrastive-decoding-in-retrieval-augmented-generation-for-handling-noisy-contexts"><a href="http://arxiv.org/abs/2408.01084v1">Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts</a></h2>
<p>发布时间：2024-08-02</p>
<p>作者：Youna Kim, Hyuhng Joon Kim, Cheonbok Park, Choonghyun Park, Hyunsoo Cho, Junyeob Kim, Kang Min Yoo, Sang-goo Lee, Taeuk Kim</p>
<h4 id="_215">中文摘要：</h4>
<p>在利用大型语言模型（LLMs）进行知识密集型任务，如开放域问答时，外部上下文可以弥合外部知识与LLMs参数化知识之间的差距。近期的研究已经开发出对比解码方法来增强LLMs的参数化知识上的上下文知识。尽管这些方法在提供相关上下文时可以产生真实的回答，但它们在面对噪声上下文时容易受到攻击。我们扩展了先前研究的范围，以包括噪声上下文，并提出了自适应对比解码（ACD）来有效地利用上下文影响。ACD在开放域问答任务中与基线相比表现出改进，尤其是在检索增强生成中，通过不受噪声上下文的干扰，提高了鲁棒性。</p>
<h4 id="_216">一句话总结：</h4>
<p>本研究提出了一种自适应对比解码方法（ACD），有效提升了大型语言模型在噪声上下文中的开放域问答性能。</p>
<hr />
<h2 id="openlogparser-unsupervised-parsing-with-open-source-large-language-models"><a href="http://arxiv.org/abs/2408.01585v1">OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models</a></h2>
<p>发布时间：2024-08-02</p>
<p>作者：Zeyang Ma, Dong Jae Kim, Tse-Hsun Chen</p>
<h4 id="_217">中文摘要：</h4>
<p>日志解析是将非结构化日志数据转换为结构化格式的重要步骤，这有助于后续的基于日志的分析。传统的基于语法的日志解析器既高效又有效，但当处理不符合预定义规则的日志时，它们的准确性通常会下降。最近，基于大型语言模型（LLM）的日志解析器显示出更高的解析精度。然而，现有的基于LLM的解析器面临三个主要挑战：1）用于微调或上下文学习的耗时且劳动密集型的手动标记；2）由于日志数据量庞大和LLM的上下文尺寸有限，解析成本增加；3）使用像ChatGPT这样的商业模型处理敏感日志信息所带来的隐私风险。为了克服这些限制，本文介绍了OpenLogParser，这是一种无监督日志解析方法，它利用开源LLM（即Llama3-8B）来提高隐私性并降低运营成本，同时实现最先进的解析精度。OpenLogParser首先使用固定深度的分组树将具有相似静态文本但动态变量不同的日志分组。然后，它使用三个组件对这些组内的日志进行解析：i）基于相似度评分的检索增强生成：根据Jaccard相似度在每个组内选择多样化的日志，帮助LLM区分静态文本和动态变量；ii）自我反思：迭代查询LLM以细化日志模板，提高解析精度；iii）日志模板内存：存储解析模板以减少LLM查询，提高解析效率。我们在LogHub-2.0上的评估表明，与最先进的基于LLM的解析器相比，OpenLogParser实现了25%更高的解析精度，并且处理日志的速度快了2.7倍。简而言之，OpenLogParser在实现最先进的解析效率和精度的同时，解决了使用商业LLM的隐私和成本问题。</p>
<h4 id="_218">一句话总结：</h4>
<p>OpenLogParser通过利用开源LLM，实现了高精度且隐私友好的日志解析，同时降低了处理成本。</p>
<hr />
<h2 id="evaluating-the-impact-of-advanced-llm-techniques-on-ai-lecture-tutors-for-a-robotics-course"><a href="http://arxiv.org/abs/2408.04645v1">Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course</a></h2>
<p>发布时间：2024-08-02</p>
<p>作者：Sebastian Kahl, Felix Löffler, Martin Maciol, Fabian Ridder, Marius Schmitz, Jennifer Spanagel, Jens Wienkamp, Christopher Burgahn, Malte Schilling</p>
<h4 id="_219">中文摘要：</h4>
<p>本研究评估了大型语言模型（LLMs）作为基于人工智能的大学课程辅导系统的性能。特别是，本研究采用了多种高级技术，如提示工程、检索增强生成（RAG）和微调。我们使用常见的相似度指标，如BLEU-4、ROUGE和BERTScore，对不同的模型和所应用的技术进行了评估，并辅以小规模的人类评估，以评估其帮助性和可信度。研究发现，RAG与提示工程的结合显著提升了模型响应的质量，并产生了更准确的事实性回答。在教育领域，RAG因其能够通过增加额外的信息和材料来丰富模型输入而显得是一个理想的技巧。另一方面，微调可以产生相当小但仍然强大的专家模型，但也存在过拟合的风险。本研究进一步探讨了如何衡量LLMs的性能以及当前衡量方法如何代表正确性或相关性。我们发现相似度指标之间存在高度相关性，但这些指标大多数存在偏向较短回答的偏差。总体而言，我们的研究指出了在教育环境中整合LLMs的潜力和挑战，并建议需要平衡的训练方法和先进的评估框架。</p>
<h4 id="_220">一句话总结：</h4>
<p>本研究评估了大型语言模型在大学课程辅导中的应用，发现RAG与提示工程结合能显著提升模型性能，但同时也揭示了评估方法和过拟合等挑战。</p>
<hr />
<h2 id="rageval-scenario-specific-rag-evaluation-dataset-generation-framework_1"><a href="http://arxiv.org/abs/2408.01262v3">RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework</a></h2>
<p>发布时间：2024-08-02</p>
<p>作者：Kunlun Zhu, Yifan Luo, Dingling Xu, Ruobing Wang, Shi Yu, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, Zhiyuan Liu, Maosong Sun</p>
<h4 id="_221">中文摘要：</h4>
<p>检索增强生成（RAG）系统在缓解大型语言模型（LLMs）的幻觉方面已经显示出其优势。现有的RAG基准主要关注评估LLMs是否能够正确回答一般知识。然而，它们无法评估RAG系统在处理来自不同垂直领域的数据时的有效性。本文介绍了RAGEval，这是一个用于自动生成评估数据集的框架，以评估不同场景下不同LLMs的知识使用能力。具体来说，RAGEval从种子文档中总结出一个模式，应用配置生成多样化的文档，并根据文章和配置构建问答对。我们提出了三个新颖的指标，即完整性、幻觉和无关性，以仔细评估LLMs生成的响应。通过在垂直领域对RAG模型进行基准测试，RAGEval能够更好地评估LLMs的知识使用能力，避免了在现有问答数据集中关于知识来源的混淆——即回答问题时知识是来自参数化记忆还是检索。</p>
<h4 id="_222">一句话总结：</h4>
<p>本文提出的RAGEval框架通过自动生成评估数据集，能够更有效地评估大型语言模型在不同场景下的知识使用能力。</p>
<hr />
<h2 id="biorag-a-rag-llm-framework-for-biological-question-reasoning_1"><a href="http://arxiv.org/abs/2408.01107v2">BioRAG: A RAG-LLM Framework for Biological Question Reasoning</a></h2>
<p>发布时间：2024-08-02</p>
<p>作者：Chengrui Wang, Qingqing Long, Meng Xiao, Xunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi Wang, Yuanchun Zhou</p>
<h4 id="_223">中文摘要：</h4>
<p>生命科学研究的问答系统，因其发现速度快、见解不断演变以及知识实体之间复杂的相互作用而具有独特挑战，这要求保持一个全面的知识仓库和准确的信息检索。为了解决这些问题，我们引入了BioRAG，这是一种基于大型语言模型（LLMs）框架的检索增强生成（RAG）方法。我们的方法从解析、索引和分割大量科学论文（共计2200万篇）作为基本知识开始，随后训练一个针对该领域的特定嵌入模型。此外，我们通过引入一个特定领域的知识层次结构来增强向量检索过程，这有助于模拟每个查询和上下文之间复杂的相互关系。对于需要最新信息的查询，BioRAG分解问题并采用结合搜索引擎的迭代检索过程进行逐步推理。严格的实验表明，我们的模型在多个生命科学问答任务中优于微调的LLM、带有搜索引擎的LLM以及其他科学RAG框架。</p>
<h4 id="_224">一句话总结：</h4>
<p>BioRAG通过结合大型语言模型和特定领域知识层次结构，实现了高效的生命科学问答系统。</p>
<hr />
<h2 id="improving-retrieval-augmented-generation-in-medicine-with-iterative-follow-up-questions"><a href="http://arxiv.org/abs/2408.00727v1">Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions</a></h2>
<p>发布时间：2024-08-01</p>
<p>作者：Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang</p>
<h4 id="_225">中文摘要：</h4>
<p>大型语言模型（LLMs）在解决医学问题方面展现出巨大的潜力。它们可以拥有相当多的医学知识，但可能仍然会出现幻觉，并且在知识更新方面不够灵活。尽管检索增强生成（RAG）已被提出用于通过外部知识库增强LLMs的医学问答能力，但在需要多轮信息检索的复杂病例中，它可能仍然会失败。为了解决这一问题，我们提出了用于医学的迭代RAG（i-MedRAG），其中LLMs可以根据先前信息检索尝试迭代地提出后续查询。在i-MedRAG的每个迭代中，后续查询将由标准的RAG系统回答，并将进一步用于指导下一迭代中的查询生成。我们的实验表明，与标准RAG相比，i-MedRAG在各种LLMs上提高了在复杂问题上的性能，这些问题来自美国医学执照考试（USMLE）的临床病例以及大规模多任务语言理解（MMLU）数据集中的各种知识测试。值得注意的是，我们的零样本i-MedRAG在GPT-3.5上优于所有现有的提示工程和微调方法，在MedQA数据集上达到了69.68%的准确率。此外，我们描述了i-MedRAG的扩展特性，包括不同迭代次数的后续查询和每次迭代中查询的数量。我们的案例研究表明，i-MedRAG可以灵活地提出后续查询以形成推理链，对医学问题进行深入分析。据我们所知，这是首次将后续查询纳入医学RAG的研究。</p>
<h4 id="_226">一句话总结：</h4>
<p>i-MedRAG通过迭代查询和推理链，显著提升了LLMs在医学问答中的性能。</p>
<hr />
<h2 id="alleviating-hallucination-in-large-vision-language-models-with-active-retrieval-augmentation"><a href="http://arxiv.org/abs/2408.00555v1">Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation</a></h2>
<p>发布时间：2024-08-01</p>
<p>作者：Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, Jianfeng Dong</p>
<h4 id="_227">中文摘要：</h4>
<p>尽管大型视觉语言模型（LVLMs）在图像理解方面表现出色，但这些模型经常生成看似合理但实际上错误的回答，这种现象被称为幻觉。最近，在大语言模型（LLMs）中，通过从外部知识资源检索信息来增强LLMs已被证明是缓解幻觉的有希望的方法。然而，LVLM中的检索增强在应用上远远落后于LVLM的广泛应用。此外，当转移到增强LVLMs时，有时模型的幻觉程度甚至加剧。受研究差距和反直觉现象的启发，我们引入了一个新的框架，即主动检索增强的大型视觉语言模型（ARA），专门设计用于通过结合三个关键维度来解决幻觉问题：（i）基于图像固有的层次结构分解检索目标。（ii）确定最有效的检索方法并过滤出可靠的检索结果。（iii）在低确定性事件期间进行检索，同时避免在高确定性期间进行不必要的检索。为了评估我们提出的ARA模型在减少幻觉方面的能力，我们在四个基准测试中使用了三个广泛使用的LVLM模型（LLaVA-1.5、Qwen-VL和mPLUG-Owl2）。我们的实验观察结果表明，通过利用合适的检索机制并明智地安排检索时间，我们可以有效地缓解幻觉问题。我们希望这项研究能够更深入地了解如何将检索增强适应LVLMs，以通过更有效的检索和最少的检索发生来减少幻觉。</p>
<h4 id="_228">一句话总结：</h4>
<p>本研究提出了一种名为ARA的新型框架，通过结合图像的层次结构、有效的检索方法和时机选择，有效缓解了大型视觉语言模型中的幻觉问题。</p>
<hr />
<h2 id="vecaug-unveiling-camouflaged-frauds-with-cohort-augmentation-for-enhanced-detection"><a href="http://arxiv.org/abs/2408.00513v1">VecAug: Unveiling Camouflaged Frauds with Cohort Augmentation for Enhanced Detection</a></h2>
<p>发布时间：2024-08-01</p>
<p>作者：Fei Xiao, Shaofeng Cai, Gang Chen, H. V. Jagadish, Beng Chin Ooi, Meihui Zhang</p>
<h4 id="_229">中文摘要：</h4>
<p>欺诈检测是一个具有挑战性的任务，其特征是欺诈模式不断演变且标注数据稀缺。现有方法主要依赖于基于图或基于序列的方法。虽然基于图的方法通过共享实体连接用户以捕获结构信息，但它们仍然容易受到欺诈者的攻击，欺诈者可以破坏或操纵这些连接。相比之下，基于序列的方法分析用户的消费行为模式，对篡改具有鲁棒性，但忽略了相似用户之间的交互。受保留和医疗保健中群体分析（cohort analysis）的启发，本文提出了一种名为VecAug的新型群体增强学习框架，通过增强目标用户的个性化群体信息来应对这些挑战。为此，我们首先提出了一种向量烧入技术（vector burn-in technique）用于自动识别群体，为每个目标用户检索一个特定任务的群体。然后，为了充分利用群体信息，我们引入了一种注意力群体聚合技术（attentive cohort aggregation technique）来增强目标用户的表示。为了提高这种群体增强的鲁棒性，我们还提出了一种新颖的标签感知群体邻居分离机制（label-aware cohort neighbor separation mechanism），以隔离负向群体邻居并校准聚合的群体信息。通过将这种群体信息与目标用户表示相结合，VecAug增强了待增强模型的建模能力和泛化能力。我们的框架是灵活的，可以无缝集成到现有的欺诈检测模型中。我们在电子商务平台上部署了我们的框架，并在三个欺诈检测数据集上进行了评估，结果表明VecAug将基础模型的AUC提高了高达2.48%，将R@P$_{0.9}$提高了22.5%，显著优于现有方法。</p>
<h4 id="_230">一句话总结：</h4>
<p>本文提出的VecAug框架通过引入群体增强学习，显著提升了欺诈检测模型的性能。</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
    
  </body>
</html>