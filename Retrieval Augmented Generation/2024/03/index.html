
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../04/">
      
      
        <link rel="next" href="../02/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.31">
    
    
      
        <title>2024-03(70) - ArXiv Tracker</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="ArXiv Tracker" class="md-header__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ArXiv Tracker
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2024-03(70)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Emission%20Trading%20System/2024/07/" class="md-tabs__link">
          
  
  Emission Trading System

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../08/" class="md-tabs__link">
          
  
  Retrieval Augmented Generation

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="ArXiv Tracker" class="md-nav__button md-logo" aria-label="ArXiv Tracker" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ArXiv Tracker
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Emission Trading System
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Emission Trading System
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2024/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2019/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2018/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2016/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2015/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-02(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Emission%20Trading%20System/2011/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2011-10(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Retrieval Augmented Generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Retrieval Augmented Generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-08(19)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-07(126)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-06(152)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-05(121)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-04(86)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2024-03(70)
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-02(87)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024-01(50)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-12(33)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-11(38)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-10(43)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-09(20)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-08(13)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-07(11)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-06(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-05(33)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-04(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-03(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-02(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2023/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-12(12)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-11(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-10(15)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-09(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-07(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-06(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-05(6)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-04(10)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-02(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2022/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022-01(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-12(9)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-10(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-09(4)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-08(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-06(7)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-05(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-04(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-02(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2021/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021-01(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-11(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-10(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-08(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-07(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-06(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-04(5)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-03(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2020/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-12(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-09(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-06(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2019/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-11(3)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-02(2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2018/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-12(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-09(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2017/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2017-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-10(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-08(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-07(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-05(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-04(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2016/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2016-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2015/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2015-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-11(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2014/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2014-03(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2012/01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2012-01(1)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2009/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2009-03(1)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="retrieval-augmented-generation-202403">Retrieval Augmented Generation - 2024年03月</h1>
<h2 id="observations-on-building-rag-systems-for-technical-documents"><a href="http://arxiv.org/abs/2404.00657v1">Observations on Building RAG Systems for Technical Documents</a></h2>
<p>发布时间：2024-03-31</p>
<p>作者：Sumit Soman, Sujoy Roychowdhury</p>
<h4 id="_1">中文摘要：</h4>
<p>检索增强生成（RAG）技术在处理技术文档时面临挑战，因为嵌入向量往往无法充分捕捉领域信息。本文回顾了影响RAG的关键因素的相关研究，并通过实验突出了构建技术文档RAG系统的最佳实践和潜在挑战。</p>
<h4 id="_2">一句话总结：</h4>
<p>本文探讨了技术文档检索增强生成系统中的关键因素和构建挑战。</p>
<hr />
<h2 id="rq-rag-learning-to-refine-queries-for-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2404.00610v1">RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-03-31</p>
<p>作者：Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, Jie Fu</p>
<h4 id="_3">中文摘要：</h4>
<p>大型语言模型（LLMs）展现出惊人的能力，但容易生成不准确或幻觉性的回应。这种限制源于它们依赖于庞大的预训练数据集，使得它们在未见过的场景中容易出错。为了应对这些挑战，检索增强生成（RAG）通过将外部相关文档纳入回应生成过程来解决这个问题，从而利用非参数知识以及LLMs的上下文学习能力。然而，现有的RAG实现主要关注初始输入用于上下文检索，忽略了需要进一步澄清或分解以获得准确回应的模糊或复杂查询的细微差别。为此，本文提出了学习用于检索增强生成（RQ-RAG）的查询细化，旨在通过赋予模型显式重写、分解和消歧的能力来提升模型。我们的实验结果表明，当应用于7B Llama2模型时，我们的方法在三个单跳问答（QA）数据集上平均超越了之前的最先进（SOTA）水平1.9%，同时也展示了在处理复杂的多跳QA数据集上的性能提升。我们的代码可在https://github.com/chanchimin/RQ-RAG上获取。</p>
<h4 id="_4">一句话总结：</h4>
<p>本文提出了一种名为RQ-RAG的查询细化方法，旨在通过增强大型语言模型在检索增强生成中的上下文学习能力，从而提高其在处理复杂查询时的准确性和性能。</p>
<hr />
<h2 id="dialectical-alignment-resolving-the-tension-of-3h-and-security-threats-of-llms"><a href="http://arxiv.org/abs/2404.00486v1">Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs</a></h2>
<p>发布时间：2024-03-30</p>
<p>作者：Shu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad Asif Ali, Lijie Hu, Di Wang</p>
<h4 id="_5">中文摘要：</h4>
<p>随着大型语言模型（LLMs）的兴起，确保它们体现有益、诚实和无害（3H）的原则，即人类对齐，变得至关重要。虽然现有的对齐方法如RLHF、DPO等，能够有效地微调LLMs以匹配偏好数据集中的偏好，但它们往往导致LLMs对外部输入和证据高度敏感，即使这些信息被污染。这导致LLMs在外部证据与它们的参数记忆冲突时，表现出适应性变色龙的行为。这加剧了LLMs被外部污染数据攻击的风险，这对LLMs系统应用如检索增强生成（RAG）构成了重大的安全风险。为了应对这一挑战，我们提出了一种新的框架：辩证对齐（DA），该框架（1）利用AI反馈来识别LLMs在上下文窗口（即不同比例的污染事实上下文）中导航跨上下文冲突和上下文-记忆冲突的最佳策略；（2）基于上述AI反馈和策略构建SFT数据集以及偏好数据集；（3）使用上述数据集进行LLM对齐，以防御污染上下文攻击，同时保持上下文知识编辑的有效性。我们的实验表明，辩证对齐模型将污染数据攻击防御能力提高了20%，并且不需要对LLMs的上下文窗口进行任何额外的提示工程或提前声明“你可能被攻击”。</p>
<h4 id="_6">一句话总结：</h4>
<p>提出了一种名为辩证对齐（DA）的新框架，以增强大型语言模型对污染数据攻击的防御能力，同时保持上下文知识编辑的有效性。</p>
<hr />
<h2 id="lake-red-camouflaged-images-generation-by-latent-background-knowledge-retrieval-augmented-diffusion"><a href="http://arxiv.org/abs/2404.00292v4">LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion</a></h2>
<p>发布时间：2024-03-30</p>
<p>作者：Pancheng Zhao, Peng Xu, Pengda Qin, Deng-Ping Fan, Zhicheng Zhang, Guoli Jia, Bowen Zhou, Jufeng Yang</p>
<h4 id="_7">中文摘要：</h4>
<p>伪装视觉感知是一个重要的视觉任务，具有广泛的应用前景。然而，由于收集和标注成本高昂，该领域面临一个主要瓶颈，即其数据集的物种类别仅限于少数几种物体物种。然而，现有的伪装生成方法需要手动指定背景，因此无法以低成本的方式扩展伪装样本的多样性。在本文中，我们提出了一种用于伪装图像生成的潜在背景知识检索增强扩散（LAKE-RED）方法。据我们所知，我们的主要贡献包括：（1）首次提出了一种不需要接收任何背景输入的伪装生成范式。（2）我们的LAKE-RED是第一个具有可解释性的知识检索增强方法，其中我们提出了一种将知识检索和推理增强明确分离的想法，以减轻特定任务的挑战。此外，我们的方法不受特定前景目标或背景的限制，为将伪装视觉感知扩展到更多领域提供了潜力。（3）实验结果表明，我们的方法优于现有方法，能够生成更逼真的伪装图像。</p>
<h4 id="_8">一句话总结：</h4>
<p>本文提出了一种创新的LAKE-RED方法，通过知识检索增强和无需背景输入的伪装生成范式，有效扩展了伪装视觉感知的样本多样性，并生成了更逼真的伪装图像。</p>
<hr />
<h2 id="towards-a-robust-retrieval-based-summarization-system"><a href="http://arxiv.org/abs/2403.19889v1">Towards a Robust Retrieval-Based Summarization System</a></h2>
<p>发布时间：2024-03-29</p>
<p>作者：Shengjie Liu, Jing Wu, Jingyuan Bao, Wenyi Wang, Naira Hovakimyan, Christopher G Healey</p>
<h4 id="_9">中文摘要：</h4>
<p>本文描述了对大型语言模型（LLMs）在基于检索增强生成（RAG）的摘要任务中的鲁棒性的研究。虽然LLMs提供了摘要能力，但它们在复杂、现实场景中的表现仍被低估。我们的第一个贡献是LogicSumm，这是一个创新的评估框架，它通过结合现实场景来评估LLMs在RAG摘要过程中的鲁棒性。基于LogiSumm识别出的限制，我们随后开发了SummRAG，这是一个全面的系统，用于创建训练对话并微调模型，以增强LogicSumm场景中的鲁棒性。SummRAG是我们定义结构化方法以测试LLM能力的目标的例证，而不是一次性解决这些问题。实验结果证实了SummRAG的力量，展示了改进的逻辑一致性和摘要质量。数据、相应的模型权重和Python代码可在网上获取。</p>
<h4 id="_10">一句话总结：</h4>
<p>本文提出了一种名为SummRAG的系统，用于提高大型语言模型在基于检索增强的摘要任务中的鲁棒性，并通过实验验证了其有效性。</p>
<hr />
<h2 id="fairrag-fair-human-generation-via-fair-retrieval-augmentation"><a href="http://arxiv.org/abs/2403.19964v3">FairRAG: Fair Human Generation via Fair Retrieval Augmentation</a></h2>
<p>发布时间：2024-03-29</p>
<p>作者：Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, Siqi Deng</p>
<h4 id="_11">中文摘要：</h4>
<p>现有的文本到图像生成模型反映了甚至放大了其训练数据中固有的社会偏见。这对于人类图像生成尤其令人担忧，因为模型对某些人口群体存在偏见。现有的尝试纠正这一问题的方法受到预训练模型固有局限性的阻碍，并且未能显著提高人口多样性。在这项工作中，我们引入了公平检索增强生成（FairRAG），这是一个新颖的框架，它通过从外部图像数据库检索到的参考图像来对预训练生成模型进行条件化，以改善人类生成的公平性。FairRAG通过一个轻量级的线性模块实现条件化，该模块将参考图像投影到文本空间。为了增强公平性，FairRAG应用简单但有效的去偏策略，在生成过程中提供来自不同人口群体的图像。广泛的实验表明，FairRAG在人口多样性、图像-文本对齐和图像保真度方面优于现有方法，同时在推理过程中产生最小的计算开销。</p>
<h4 id="_12">一句话总结：</h4>
<p>FairRAG通过从外部数据库检索参考图像来对预训练生成模型进行条件化，从而提高人类图像生成的公平性。</p>
<hr />
<h2 id="mfort-qa-multi-hop-few-shot-open-rich-table-question-answering"><a href="http://arxiv.org/abs/2403.19116v1">MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering</a></h2>
<p>发布时间：2024-03-28</p>
<p>作者：Che Guan, Mengyu Huang, Peng Zhang</p>
<h4 id="_13">中文摘要：</h4>
<p>在当今快节奏的行业中，专业人士面临着每天总结大量文档并从中提取关键信息的挑战。这些关键指标通常隐藏在表格及其嵌套的超链接中。为了应对这一挑战，已经开发出表格问答（QA）方法来提取相关信息。然而，传统的表格QA训练任务，即提供表格和从黄金单元格坐标提供的答案（s）的问题，可能并不总是确保提取准确的答案（s）。近年来，大型语言模型（LLMs）的进步为使用提示从表格数据中提取信息开辟了新的可能性。在本文中，我们介绍了多跳少样本开放丰富表格QA（MFORT-QA）方法，它包括两个主要步骤。第一步涉及少样本学习（FSL），根据给定的问题检索相关的表格和超链接的相关上下文。然后，检索到的内容被用来构建少样本提示，作为LLM（如ChatGPT）的输入。为了应对回答复杂问题的挑战，第二步利用思维链（CoT）提示以多跳方式将复杂问题分解成一系列问题和推理思考。检索增强生成（RAG）通过检索与结果推理思考和问题相关的相关表格和超链接上下文来增强这一过程。然后，这些额外的上下文被用来补充第一步中使用的提示，从而从LLM中获得更准确的答案。OTT-QA的实证结果表明，我们的抽象QA方法显著提高了提取式表格QA方法的准确性。</p>
<h4 id="_14">一句话总结：</h4>
<p>本文提出的多跳少样本开放丰富表格QA方法，通过结合少样本学习和思维链提示，显著提高了从表格数据中提取信息的准确性。</p>
<hr />
<h2 id="factoid-factual-entailment-for-hallucination-detection"><a href="http://arxiv.org/abs/2403.19113v1">FACTOID: FACtual enTailment fOr hallucInation Detection</a></h2>
<p>发布时间：2024-03-28</p>
<p>作者：Vipula Rawte, S. M Towhidul Islam Tonmoy, Krishnav Rajbangshi, Shravani Nag, Aman Chadha, Amit P. Sheth, Amitava Das</p>
<h4 id="_15">中文摘要：</h4>
<p>大型语言模型（LLMs）的广泛采用带来了许多好处。然而，幻觉（hallucination）是一个重要的问题。为此，检索增强生成（RAG）作为一种非常有前途的范式出现了，它通过将LLM输出基于事实信息进行改进。RAG依赖于文本蕴涵（TE）或类似方法来检查LLM生成的文本是否得到检索到的文档的支持或反驳。本文认为传统的TE方法不足以识别LLM生成内容中的幻觉。例如，考虑到一个关于“美国对乌克兰战争的立场”的提示，AI生成的文本写道：“美国总统巴拉克·奥巴马表示，美国不会向乌克兰派遣军队”，然而，在战争期间的美国总统是乔·拜登，这与事实相矛盾。此外，当前的TE系统无法准确注释给定文本并识别出被反驳的确切部分。为了解决这个问题，我们引入了一种新的TE类型，称为“事实蕴涵（FE）”，旨在检测LLM生成内容中的事实不准确性，同时突出显示与现实相矛盾的具体文本段落。我们提出了FACTOID（检测幻觉的事实蕴涵），一个用于FE的基准数据集。我们提出了一种多任务学习（MTL）框架用于FE，结合了最先进（SoTA）的长文本嵌入，如e5-mistral-7b-instruct，以及GPT-3、SpanBERT和RoFormer。所提出的FE的MTL架构在FACTOID基准上相对于SoTA TE方法平均提高了40%的准确性。由于FE自动检测幻觉，我们评估了15个现代LLM，并使用我们提出的自动幻觉脆弱性指数（HVI_auto）对它们进行排名。该指数量化并提供了一个比较尺度来评估和排名LLM的幻觉。</p>
<h4 id="_16">一句话总结：</h4>
<p>本文提出了一种新的事实蕴涵（FE）方法和多任务学习框架，用于检测和量化大型语言模型（LLM）生成内容中的事实不准确性，并显著提高了准确性。</p>
<hr />
<h2 id="img2loc-revisiting-image-geolocalization-using-multi-modality-foundation-models-and-image-based-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2403.19584v1">Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation</a></h2>
<p>发布时间：2024-03-28</p>
<p>作者：Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, Gengchen Mai</p>
<h4 id="_17">中文摘要：</h4>
<p>从图像中精确定位地理位置是计算机视觉和信息检索中的一个具有挑战性的问题。传统方法通常采用分类方法，即将地球表面划分为网格单元并对图像进行分类，或者采用检索方法，通过将图像与图像-位置对数据库进行匹配来识别位置。然而，基于分类的方法受单元格大小的限制，无法产生精确的预测，而基于检索的系统通常在搜索质量和全球景观的覆盖范围方面存在不足。为了克服这些缺点，我们提出了Img2Loc，这是一个新颖的系统，它将图像地理定位重新定义为文本生成任务。这是通过使用像GPT4V或LLaVA这样的尖端大型多模态模型以及检索增强生成来实现的。Img2Loc首先使用基于CLIP的表示来生成基于图像的坐标查询数据库。然后，它独特地将查询结果与图像本身相结合，形成针对LMMs的定制化提示。在Im2GPS3k和YFCC4k等基准数据集上测试时，Img2Loc不仅超越了先前最先进模型的性能，而且无需任何模型训练。</p>
<h4 id="_18">一句话总结：</h4>
<p>Img2Loc通过将图像地理定位作为文本生成任务，利用先进的模型和检索增强生成技术，实现了对图像地理位置的精确定位。</p>
<hr />
<h2 id="are-large-language-models-good-at-utility-judgments"><a href="http://arxiv.org/abs/2403.19216v2">Are Large Language Models Good at Utility Judgments?</a></h2>
<p>发布时间：2024-03-28</p>
<p>作者：Hengran Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng</p>
<h4 id="_19">中文摘要：</h4>
<p>检索增强生成（RAG）被认为是缓解大型语言模型（LLMs）幻觉问题的有前景的方法，最近受到了研究者的广泛关注。由于检索模型在语义理解上的局限性，RAG的成功很大程度上依赖于LLMs识别有用段落的能力。近期的研究探索了LLMs评估检索中段落相关性的能力，但在支持问答中评估段落有用性的工作还相对有限。在本工作中，我们对LLMs在开放域问答中评估有用性的能力进行了全面研究。具体来说，我们引入了一种基准测试程序和具有不同特征的候选段落集合，以促进对五种代表性LLMs的一系列实验。我们的实验揭示：（i）经过良好指导的LLMs能够区分相关性和有用性，并且LLMs对新生成的反事实段落非常敏感。（ii）我们审视了影响指令设计中有用性判断的关键因素。（iii）为了验证有用性判断在实际检索增强应用中的有效性，我们利用有用性判断的证据和直接密集检索结果深入研究了LLMs的问答能力。（iv）我们提出了一种k采样、列表式方法来减少LLMs对输入段落序列的依赖，从而促进后续答案生成。我们相信，我们形式化和研究问题的方法以及我们的发现有助于对检索增强LLMs进行关键评估。我们的代码和基准可以在\url{https://github.com/ict-bigdatalab/utility_judgments}找到。</p>
<h4 id="_20">一句话总结：</h4>
<p>本研究通过实验和基准测试，深入探讨了大型语言模型在开放域问答中评估段落有用性的能力，并提出了一种减少LLMs对输入段落序列依赖的新方法。</p>
<hr />
<h2 id="scaling-laws-for-dense-retrieval"><a href="http://arxiv.org/abs/2403.18684v2">Scaling Laws For Dense Retrieval</a></h2>
<p>发布时间：2024-03-27</p>
<p>作者：Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu</p>
<h4 id="_21">中文摘要：</h4>
<p>本研究探讨了密集检索模型的性能是否遵循与其他神经网络模型相似的缩放定律。研究发现，在特定的设置下，密集检索模型的性能与模型大小和注释数量呈精确的幂律缩放关系。此外，我们还通过对比对数似然作为评估指标，对使用不同参数数量和不同标注数据量训练的密集检索模型进行了广泛实验。我们还研究了使用常见的数据增强方法进行缩放，以评估标注质量的影响，并应用缩放定律在预算约束下找到最佳资源分配策略。我们认为这些见解将显著促进对密集检索模型缩放效应的理解，并为未来的研究提供有意义的指导。</p>
<h4 id="_22">一句话总结：</h4>
<p>本研究揭示了密集检索模型的性能遵循幂律缩放定律，为资源分配和未来研究提供了指导。</p>
<hr />
<h2 id="blade-enhancing-black-box-large-language-models-with-small-domain-specific-models"><a href="http://arxiv.org/abs/2403.18365v1">BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models</a></h2>
<p>发布时间：2024-03-27</p>
<p>作者：Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, Qi Tian</p>
<h4 id="_23">中文摘要：</h4>
<p>大型语言模型（LLMs）如ChatGPT和GPT-4功能多样，能够处理各种任务。然而，基于开放域数据开发的通用LLMs可能缺乏垂直领域（如法律、医疗等）任务所需的特定领域知识。为了解决这个问题，先前的方法要么使用特定领域数据进行持续预训练，要么采用检索增强来支持通用LLMs。不幸的是，这些策略要么成本高昂，要么在实际应用中不可靠。为此，我们提出了一种名为BLADE的新框架，该框架通过小型特定领域模型增强黑盒大型语言模型。BLADE由一个黑盒LLM和一个小型特定领域LM组成。小型LM保留特定领域知识并提供专业见解，而通用LLM则贡献了强大的语言理解和推理能力。具体来说，我们的方法包括三个步骤：1）使用特定领域数据预训练小型LM，2）使用知识指令数据微调此模型，3）对通用LLM和小型LM进行联合贝叶斯优化。在公共法律和医疗基准上进行的广泛实验表明，BLADE显著优于现有方法。这表明BLADE作为适应垂直领域通用LLM的有效且成本效益高的解决方案具有潜力。</p>
<h4 id="_24">一句话总结：</h4>
<p>BLADE通过结合黑盒大型语言模型和小型特定领域模型，为通用LLMs在垂直领域的应用提供了一种高效且成本效益高的解决方案。</p>
<hr />
<h2 id="evaluation-of-semantic-search-and-its-role-in-retrieved-augmented-generation-rag-for-arabic-language"><a href="http://arxiv.org/abs/2403.18350v2">Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language</a></h2>
<p>发布时间：2024-03-27</p>
<p>作者：Ali Mahboub, Muhy Eddin Za'ter, Bashar Al-Rfooh, Yazan Estaitia, Adnan Jaljuli, Asma Hakouz</p>
<h4 id="_25">中文摘要：</h4>
<p>最新的机器学习和深度学习进展带来了语义相似性的概念，这一概念在多个应用中证明极为有益，并很大程度上取代了关键词搜索。然而，评估语义相似性和在多种文档中针对特定查询进行搜索仍然是一个复杂任务。这种复杂性源于任务的多元性质，以及缺乏标准基准，而这些问题在阿拉伯语中更为突出。本文旨在为阿拉伯语的语义搜索建立一个简单而有效的基准。此外，为了精确评估这些指标和数据集的有效性，我们在检索增强生成（RAG）框架内进行语义搜索的评估。</p>
<h4 id="_26">一句话总结：</h4>
<p>本文提出了一种针对阿拉伯语语义搜索的简单而有效的基准，并在检索增强生成框架内评估了其有效性。</p>
<hr />
<h2 id="cpr-retrieval-augmented-generation-for-copyright-protection"><a href="http://arxiv.org/abs/2403.18920v1">CPR: Retrieval Augmented Generation for Copyright Protection</a></h2>
<p>发布时间：2024-03-27</p>
<p>作者：Aditya Golatkar, Alessandro Achille, Luca Zancato, Yu-Xiang Wang, Ashwin Swaminathan, Stefano Soatto</p>
<h4 id="_27">中文摘要：</h4>
<p>检索增强生成（RAG）正成为一种灵活且稳健的技术，能够在不进行训练的情况下适应私有用户数据，处理信用归属问题，并允许大规模高效地进行机器反学习。然而，用于图像生成的RAG技术可能会导致检索到的样本的一部分被复制到模型的输出中。为了减少检索集中包含的私有信息泄露的风险，我们引入了带有检索的复制保护生成（CPR），这是一种在混合私有设置下为扩散模型提供强大版权保护保证的新方法。CPR允许将扩散模型的输出条件化为一组检索到的图像，同时保证那些示例的唯一可识别信息不会在生成的输出中暴露。特别是，它通过在推理时合并它们的扩散分数，从公共（安全）分布和私有（用户）分布的混合中采样来实现。我们证明了CPR满足近似无访问自由度（NAF），这限制了攻击者可能从生成的图像中提取的信息量。我们提供了两种版权保护算法，CPR-KL和CPR-Choose。与之前提出的基于拒绝采样的NAF方法不同，我们的方法通过单次反向扩散运行实现高效的版权保护采样。我们表明，我们的方法可以应用于任何预训练的条件扩散模型，如Stable Diffusion或unCLIP。特别是，我们通过实验表明，在unCLIP之上应用CPR可以提高生成结果的质量和文本到图像的对齐（在TIFA基准测试中从81.4提高到83.17），同时实现信用归属、版权保护和确定性、恒定时间的反学习。</p>
<h4 id="_28">一句话总结：</h4>
<p>CPR是一种结合了版权保护和高效机器反学习的RAG技术，能够有效防止私有信息泄露，并提升图像生成质量。</p>
<hr />
<h2 id="rap-retrieval-augmented-planner-for-adaptive-procedure-planning-in-instructional-videos"><a href="http://arxiv.org/abs/2403.18600v1">RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos</a></h2>
<p>发布时间：2024-03-27</p>
<p>作者：Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang</p>
<h4 id="_29">中文摘要：</h4>
<p>在指导性视频中的程序规划涉及根据对初始状态和目标状态的视觉观察生成一系列动作步骤。尽管在这一任务上取得了快速进展，但仍存在几个需要解决的挑战：(1)自适应程序：先前的工作持有一种不切实际的假设，即动作步骤的数量是已知且固定的，导致在序列长度变化的现实场景中，模型不具有通用性。(2)时间关系：理解步骤时间关系知识对于生成合理且可执行的计划至关重要。(3)标注成本：对指导性视频进行步骤级标签（即时间戳）或序列级标签（即动作类别）的标注是耗时且劳动密集型的，限制了其在大规模数据集上的通用性。在本工作中，我们提出了一种新的实用设置，称为指导性视频中的自适应程序规划，其中程序长度不是固定的或预先确定的。为了解决这些挑战，我们引入了检索增强规划器（RAP）模型。具体来说，对于自适应程序，RAP使用自回归模型架构自适应地确定动作的结论。对于时间关系，RAP建立了一个外部记忆模块，以显式检索训练视频中与最相关的状态-动作对，并修改生成的程序。为了应对高标注成本，RAP利用弱监督学习方法，通过为动作步骤生成伪标签，将训练数据集扩展到其他与任务相关的、未标注的视频。在CrossTask和COIN基准上的实验表明，RAP优于传统的固定长度模型，将其确立为自适应程序规划的一个强大基线解决方案。</p>
<h4 id="_30">一句话总结：</h4>
<p>本文提出了一种名为RAP的检索增强规划器模型，用于指导性视频中的自适应程序规划，有效解决了自适应程序、时间关系和标注成本等挑战。</p>
<hr />
<h2 id="leveraging-large-language-models-for-relevance-judgments-in-legal-case-retrieval"><a href="http://arxiv.org/abs/2403.18405v1">Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval</a></h2>
<p>发布时间：2024-03-27</p>
<p>作者：Shengjie Ma, Chong Chen, Qi Chu, Jiaxin Mao</p>
<h4 id="_31">中文摘要：</h4>
<p>收集与法律案例检索相关的判决是一项具有挑战性和耗时的工作。准确判断两个法律案例之间的相关性需要阅读大量文本的相当努力，以及高水平的专业领域知识来提取法律事实和做出司法判断。随着高级大型语言模型的问世，一些近期研究表明，使用LLMs（大型语言模型）进行相关性判断具有前景。然而，在法律案例检索中，使用通用大型语言模型进行可靠的相关性判断的方法尚未得到充分探索。为了填补这一研究空白，我们设计了一种针对法律案例相关性判断的新型少样本工作流程。所提出的工作流程将标注过程分解为一系列阶段，模仿人工标注者的过程，并允许灵活地整合专家推理以提高相关性判断的准确性。通过比较LLMs和人类专家的相关性判断，我们通过实证研究证明了我们可以通过所提出的工作流程获得可靠的相关性判断。此外，我们还展示了通过合成大型语言模型生成数据的能力来增强现有法律案例检索模型的能力。</p>
<h4 id="_32">一句话总结：</h4>
<p>本研究提出了一种基于少样本工作流程的法律案例相关性判断方法，通过整合专家推理和LLMs生成数据，提高了法律案例检索的准确性。</p>
<hr />
<h2 id="boosting-conversational-question-answering-with-fine-grained-retrieval-augmentation-and-self-check"><a href="http://arxiv.org/abs/2403.18243v1">Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check</a></h2>
<p>发布时间：2024-03-27</p>
<p>作者：Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He</p>
<h4 id="_33">中文摘要：</h4>
<p>检索增强生成（RAG）旨在通过将外部广泛且动态的知识与大型语言模型（LLMs）相结合，生成更可靠和准确的响应。大多数先前的研究集中于使用RAG进行单轮问答，而对于如何将RAG适应复杂对话环境，其中问题与先前上下文相互依赖，研究并不充分。在本文中，我们提出了一种对话级RAG方法，该方法结合了细粒度检索增强和对话问答（CQA）的自我检查。具体来说，我们的方法由三个组件组成，即对话问题精炼器、细粒度检索器和基于自我检查的响应生成器，它们在对话环境中协同工作以实现问题理解和相关信息获取。大量的实验表明，我们的方法在性能上优于最先进的基线。此外，我们还发布了一个包含新特征的中文CQA数据集，包括重新表述的问题、提取的关键词、检索的段落及其有用性，这有助于进一步研究RAG增强的CQA。</p>
<h4 id="_34">一句话总结：</h4>
<p>本文提出了一种对话级检索增强生成方法，通过细粒度检索和自我检查，显著提升了对话问答的准确性和可靠性。</p>
<hr />
<h2 id="generation-of-asset-administration-shell-with-large-language-model-agents-toward-semantic-interoperability-in-digital-twins-in-the-context-of-industry-40"><a href="http://arxiv.org/abs/2403.17209v4">Generation of Asset Administration Shell with Large Language Model Agents: Toward Semantic Interoperability in Digital Twins in the Context of Industry 4.0</a></h2>
<p>发布时间：2024-03-25</p>
<p>作者：Yuchen Xia, Zhewen Xiao, Nasser Jazdi, Michael Weyrich</p>
<h4 id="_35">中文摘要：</h4>
<p>本研究提出了一种在工业4.0背景下实现数字孪生语义互操作性的新方法，并协助创建资产管理壳（AAS）作为数字孪生模型。我们的研究基础理念是，基于语义的通信和生成有意义的文本数据是直接相关的，我们假设如果交换的信息可以以文本形式序列化，那么这些过程是等效的。基于此，我们在研究中构建了一个“语义节点”数据结构来捕捉文本数据的语义本质。然后，设计并实现了一个由大型语言模型驱动的系统来处理“语义节点”，并从描述技术资产的规格说明书中收集的原始文本数据生成标准化的数字孪生模型。我们的评估显示，生成效率为62-79%，表明源文本中相当比例的信息可以无错误地翻译到目标数字孪生实例模型中，这得益于大型语言模型的生成能力。这一结果在工业4.0背景下具有直接应用，设计的系统作为数据模型生成工具，用于减少创建AAS模型的手动工作量。在我们的评估中，对不同大型语言模型的比较分析和检索增强生成（RAG）机制的深入消融研究提供了关于LLM系统解释技术概念和翻译数据的有效性的见解。我们的发现强调了LLM自动化AAS实例创建的能力，并为工业应用中数字孪生的语义互操作性更广泛领域做出了贡献。原型实现和评估结果可在我们的GitHub仓库中找到：https://github.com/YuchenXia/AASbyLLM。</p>
<h4 id="_36">一句话总结：</h4>
<p>本研究通过构建语义节点数据结构和利用大型语言模型，实现了从技术资产规格说明书到标准化数字孪生模型的自动化转换，有效推动了工业4.0背景下数字孪生语义互操作性。</p>
<hr />
<h2 id="lexdrafter-terminology-drafting-for-legislative-documents-using-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2403.16295v1">LexDrafter: Terminology Drafting for Legislative Documents using Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-03-24</p>
<p>作者：Ashish Chouhan, Michael Gertz</p>
<h4 id="_37">中文摘要：</h4>
<p>随着欧盟立法文件的增多，新术语及其定义的数量也在增加。根据欧洲议会、理事会和委员会的联合实用指南，法律文件中使用的术语应保持一致，并且相同的概念应在不偏离其普通、法律或技术语言意义的情况下表达。因此，在起草新的立法文件时，拥有一个提供现有定义见解并帮助根据文件上下文定义新术语的框架，将支持不同法规之间的协调一致的法律定义，从而避免歧义。在本文中，我们提出了LexDrafter框架，该框架通过检索增强生成（RAG）和不同立法文件中现有的术语定义来协助起草立法文件的“定义”文章。为此，通过从现有文件中提取定义来构建定义元素。使用定义元素和RAG，可以根据需要为正在起草的立法文件建议“定义”文章。我们使用来自能源领域的欧盟文件集合来演示和评估LexDrafter框架的功能。LexDrafter框架的代码可在https://github.com/achouhan93/LexDrafter上找到。</p>
<h4 id="_38">一句话总结：</h4>
<p>LexDrafter框架通过检索增强生成技术，辅助立法文件中定义条款的起草，以实现法律术语的一致性和清晰性。</p>
<hr />
<h2 id="llms-instruct-llmsan-extraction-and-editing-method"><a href="http://arxiv.org/abs/2403.15736v1">LLMs Instruct LLMs:An Extraction and Editing Method</a></h2>
<p>发布时间：2024-03-23</p>
<p>作者：Xin Zhang, Tianjie Ju, Huijia Liang, Ying Fu, Qin Zhang</p>
<h4 id="_39">中文摘要：</h4>
<p>对在无需从头开始重新训练的情况下更新大型语言模型（LLMs）的兴趣很大，但这带来了一些挑战。这在需要用有限样本进行复杂推理的情况下尤其如此，这种情况我们称之为LLMs的稀缺约束复杂推理适应（PCRA-LLM）。传统的低秩适应（LoRA）和检索增强生成（RAG）方法对于这个关键问题是不够的，这在我们对特定医疗情境的探索中尤为明显，该情境体现了PCRA-LLM的独特需求。为了解决这个问题，我们提出了一种顺序融合方法，将复杂上下文中的知识纳入LLMs。这种方法采用两阶段框架：首先，它利用通用LLMs构建知识图谱（KGs），以从复杂文本中提取知识；随后，它通过知识编辑更新领域LLMs。根据我们的方法，领域LLMs在问答任务中达到了71.69%的准确率。随后，我们将评估扩展到了我们开发的经济学和管理领域的新数据集，我们的方法实现了75%的准确率。这些结果突显了我们的方法在PCRA-LLM跨多个领域的有效性和适应性。</p>
<h4 id="_40">一句话总结：</h4>
<p>本研究提出了一种顺序融合方法，通过知识编辑将复杂上下文中的知识融入LLMs，有效提高了LLMs在稀缺约束复杂推理适应（PCRA-LLM）场景下的性能。</p>
<hr />
<h2 id="towards-a-rag-based-summarization-agent-for-the-electron-ion-collider"><a href="http://arxiv.org/abs/2403.15729v3">Towards a RAG-based Summarization Agent for the Electron-Ion Collider</a></h2>
<p>发布时间：2024-03-23</p>
<p>作者：Karthik Suresh, Neeltje Kackar, Luke Schleck, Cristiano Fanelli</p>
<h4 id="_41">中文摘要：</h4>
<p>该论文摘要指出，处理大规模实验中包含的文档、论文、数据和其他资源所需的信息复杂性和数量巨大，这要求大量的时间和精力去导航，使得访问和利用这些多样化的信息形式变得艰巨，尤其是对于新合作者和早期职业科学家来说。为了解决这个问题，正在开发一种基于检索增强生成（RAG）的EIC（RAGS4EIC）摘要人工智能。这种AI代理不仅能够浓缩信息，还能有效地引用相关响应，为合作者提供了显著的优势。项目采用两步方法：首先，查询包含所有相关实验信息的综合向量数据库；其次，利用大型语言模型（LLM）根据用户查询和检索到的数据生成包含引用的简洁摘要。论文描述了使用RAG评估（RAGAs）评分机制来评估响应有效性的评估方法。此外，还描述了基于提示模板的指令调整概念，它为摘要提供了灵活性和准确性。重要的是，实现依赖于LangChain，它是我们整个工作流程的基础。这种集成确保了效率和可扩展性，促进了在电子离子对撞机（EIC）社区中各种用户组的顺利部署和可访问性。这个创新的AI驱动框架不仅简化了大数据集的理解，还鼓励了协作参与，从而赋予了研究人员力量。作为演示，开发了一个网络应用程序，详细解释了RAG代理开发的每个阶段。</p>
<h4 id="_42">一句话总结：</h4>
<p>开发了一种基于RAG的AI代理，旨在简化大规模实验数据的理解，并促进EIC社区的协作研究。</p>
<hr />
<h2 id="improving-retrieval-for-rag-based-question-answering-models-on-financial-documents"><a href="http://arxiv.org/abs/2404.07221v2">Improving Retrieval for RAG based Question Answering Models on Financial Documents</a></h2>
<p>发布时间：2024-03-23</p>
<p>作者：Spurthi Setty, Harsh Thakkar, Alyssa Lee, Eden Chung, Natan Vidra</p>
<h4 id="_43">中文摘要：</h4>
<p>大型语言模型（LLMs）在生成准确响应方面的有效性高度依赖于输入质量，尤其是在采用检索增强生成（RAG）技术时。RAG通过检索与查询最相关的文本片段来增强LLMs。尽管近年来LLMs在响应质量方面取得了显著进步，但用户仍可能遇到不准确或不相关的答案；这些问题通常源于RAG对文本片段检索的次优选择，而不是LLMs固有的能力。为了增强LLMs的效能，关键在于优化RAG过程。本文探讨了现有RAG管道的约束，并介绍了增强文本检索的方法。它深入探讨了诸如复杂的分块技术、查询扩展、元数据标注的融入、再排名算法的应用以及嵌入算法的微调等策略。实施这些方法可以显著提高检索质量，从而提升LLMs在处理和响应查询方面的整体性能和可靠性。</p>
<h4 id="_44">一句话总结：</h4>
<p>本文通过优化检索增强生成（RAG）过程，提升大型语言模型（LLMs）生成准确响应的能力。</p>
<hr />
<h2 id="fine-tuning-llm-for-enterprise-practical-guidelines-and-recommendations"><a href="http://arxiv.org/abs/2404.10779v1">Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations</a></h2>
<p>发布时间：2024-03-23</p>
<p>作者：Mathav Raj J, Kushala VM, Harikrishna Warrier, Yogesh Gupta</p>
<h4 id="_45">中文摘要：</h4>
<p>企业对于对大型语言模型（LLMs）进行微调以使其训练在专有领域知识上的需求十分迫切。挑战在于以最优化资源和成本，在尽可能短的时间内将领域特定知识注入LLMs中。许多企业依赖检索增强生成（RAG）技术，它不需要对LLMs进行微调，但它们受限于向量数据库的质量及其检索能力，而不是LLMs本身的内在能力。在我们当前的研究中，我们专注于对开源LLM LLaMA进行微调，使用来自企业存储库的专有文档和代码，并使用微调后的模型来评估响应的质量。作为这项工作的一部分，我们旨在通过合理猜测所需的GPU大小和可用的数据格式选项来指导初学者如何开始对文档和代码进行LLM微调。我们还提出了文档和代码的预处理方案，以准备不同格式的数据集。对于文档数据集，我们提出形成段落块、形成问答对以及形成关键词和段落块对的方法。对于代码数据集，我们提出形成摘要和函数对的方法。此外，我们对针对特定领域查询的模型结果进行了定性评估。最后，我们还提出了微调LLMs的实用指南和建议。</p>
<h4 id="_46">一句话总结：</h4>
<p>本研究提出了一种基于专有文档和代码对开源LLM进行微调的方法，旨在提高企业领域特定查询的响应质量，并提供微调LLM的实用指南。</p>
<hr />
<h2 id="blended-rag-improving-rag-retriever-augmented-generation-accuracy-with-semantic-search-and-hybrid-query-based-retrievers"><a href="http://arxiv.org/abs/2404.07220v2">Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers</a></h2>
<p>发布时间：2024-03-22</p>
<p>作者：Kunal Sawarkar, Abhilasha Mangal, Shivam Raj Solanki</p>
<h4 id="_47">中文摘要：</h4>
<p>检索增强生成（RAG）是一种将大型语言模型（LLM）与私有知识库中的文档相结合的常见方法，用于构建生成式问答（Question-Answering）系统。然而，随着文档语料库的规模扩大，RAG的准确性变得越来越具有挑战性，检索器在整体RAG准确性中扮演着过大的角色，通过从语料库中提取最相关的文档来为LLM提供上下文。在本文中，我们提出了“混合RAG”方法，该方法利用语义搜索技术，如密集向量索引和稀疏编码器索引，并结合混合查询策略。我们的研究实现了更好的检索结果，并为NQ和TREC-COVID等信息检索（IR）数据集设定了新的基准。我们进一步将这种“混合检索器”扩展到RAG系统中，以在SQUAD等生成式问答数据集上展示出远超微调性能的结果。</p>
<h4 id="_48">一句话总结：</h4>
<p>本文提出的“混合RAG”方法通过结合语义搜索技术和混合查询策略，显著提升了检索增强生成系统的问答准确性。</p>
<hr />
<h2 id="event-temporal-relation-extraction-based-on-retrieval-augmented-on-llms"><a href="http://arxiv.org/abs/2403.15273v1">Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs</a></h2>
<p>发布时间：2024-03-22</p>
<p>作者：Xiaobin Zhang, Liangjun Zang, Qianwen Liu, Shuchong Wei, Songlin Hu</p>
<h4 id="_49">中文摘要：</h4>
<p>事件时间关系（TempRel）是事件关系抽取任务的一个主要主题。然而，TempRel的固有歧义增加了任务难度。随着提示工程的兴起，设计有效的提示模板和词汇化器来提取相关知识变得尤为重要。传统的手动设计模板在提取精确的时间知识方面存在困难。本文介绍了一种新颖的检索增强TempRel抽取方法，利用从大型语言模型（LLMs）检索到的知识来增强提示模板和词汇化器。我们的方法利用了各种LLMs的多样化能力，为模板和词汇化器设计生成了一系列想法。我们提出的方法充分利用了LLMs在生成任务中的潜力，并为我们的设计贡献了更多知识。在三个广泛认可的语料库上的实证评估表明，我们的方法在提高事件时间关系抽取任务性能方面是有效的。</p>
<h4 id="_50">一句话总结：</h4>
<p>本文提出了一种基于大型语言模型的检索增强事件时间关系抽取方法，有效提高了事件时间关系抽取任务的性能。</p>
<hr />
<h2 id="imagination-augmented-generation-learning-to-imagine-richer-context-for-question-answering-over-large-language-models"><a href="http://arxiv.org/abs/2403.15268v3">Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models</a></h2>
<p>发布时间：2024-03-22</p>
<p>作者：Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao</p>
<h4 id="_51">中文摘要：</h4>
<p>检索增强生成（Retrieval-Augmented-Generation）和生成增强生成（Generation-Augmented-Generation）被提出以增强大型语言模型（LLMs）在问答任务中所需的知识。然而，前者依赖于外部资源，而两者都需要将显式文档纳入上下文中，这增加了执行成本和易受噪声数据的影响。最近的研究表明，LLMs已经模拟了丰富的知识，尽管这些知识并未被有效触发或唤醒。受此启发，我们提出了一种新颖的知识增强框架，即想象增强生成（Imagination-Augmented-Generation，IAG），该框架模拟了人类在仅通过想象回答问题时补偿知识缺陷的能力，从而在没有依赖外部资源的情况下唤醒LLMs中的相关知识。在IAG的指导下，我们提出了一个用于问答的想象更丰富上下文方法（IMcQA）。IMcQA由两个模块组成：显式想象模块通过学习长上下文压缩生成一个简短的虚拟文档，以及隐式想象模块通过从具有长上下文的教师模型中提取知识来创建灵活的适配器。在三个数据集上的实验结果表明，IMcQA在开放域和闭卷设置中，以及在分布外泛化方面都表现出显著的优势。我们的代码将在https://github.com/Xnhyacinth/IAG上提供。</p>
<h4 id="_52">一句话总结：</h4>
<p>我们提出了一种名为想象增强生成（IAG）的新框架，通过模拟人类的想象能力来补偿大型语言模型在问答任务中的知识缺陷，从而提高模型的知识利用效率和泛化能力。</p>
<hr />
<h2 id="evidence-driven-retrieval-augmented-response-generation-for-online-misinformation"><a href="http://arxiv.org/abs/2403.14952v1">Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation</a></h2>
<p>发布时间：2024-03-22</p>
<p>作者：Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, Dong Wang</p>
<h4 id="_53">中文摘要：</h4>
<p>网络虚假信息的泛滥对公众利益构成了重大威胁。尽管众多网络用户积极参与对抗虚假信息的斗争，但许多此类回应往往缺乏礼貌和事实支持。作为解决方案，提出了文本生成方法来自动生成反驳虚假信息的回应。然而，现有方法通常在未利用外部知识的情况下端到端训练，导致文本质量低下和过度重复的回应。在本文中，我们提出了针对网络虚假信息的检索增强回应生成（RARG），它从科学来源收集支持证据，并基于这些证据生成反驳虚假信息的回应。具体来说，我们的RARG包括两个阶段：（1）证据收集，我们设计了一个检索管道，使用包含超过100万篇学术论文的数据库检索和重新排序证据文档；（2）回应生成，其中我们将大型语言模型（LLMs）与强化学习从人类反馈（RLHF）相结合，以生成基于证据的回应。我们提出了一种奖励函数，以最大化检索证据的利用同时保持生成文本的质量，从而产生礼貌且事实清楚的反驳虚假信息的回应。为了证明我们方法的有效性，我们研究了COVID-19案例，并在跨领域数据集上进行了广泛的实验，其中RARG通过生成高质量的反驳虚假信息回应，始终优于基线方法。</p>
<h4 id="_54">一句话总结：</h4>
<p>本文提出的RARG方法通过检索增强和基于证据的回应生成，有效提高了反驳网络虚假信息回应的质量和准确性。</p>
<hr />
<h2 id="fit-rag-black-box-rag-with-factual-information-and-token-reduction"><a href="http://arxiv.org/abs/2403.14374v1">FIT-RAG: Black-Box RAG with Factual Information and Token Reduction</a></h2>
<p>发布时间：2024-03-21</p>
<p>作者：Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang</p>
<h4 id="_55">中文摘要：</h4>
<p>由于参数数量极其庞大，在许多应用中，微调大型语言模型（LLMs）以更新长尾或过时知识是不切实际的。为了避免微调，我们可以将LLM视为黑盒（即冻结LLM的参数）并使用检索增强生成（RAG）系统对其进行增强，即黑盒RAG。最近，黑盒RAG在知识密集型任务中取得了成功，并引起了广泛关注。现有的黑盒RAG方法通常微调检索器以适应LLMs的偏好，并将所有检索到的文档作为输入拼接，这存在两个问题：（1）忽视事实信息。LLM偏好的文档可能不包含给定问题的事实信息，这可能会误导检索器并损害黑盒RAG的有效性；（2）Token浪费。简单地将所有检索到的文档拼接在一起，为LLMs带来了大量不必要的Token，这降低了黑盒RAG的效率。为了解决这些问题，本文提出了一种新颖的黑盒RAG框架，该框架利用检索中的事实信息并减少增强的Token数量，称为FIT-RAG。FIT-RAG通过构建一个双标签文档评分器来利用事实信息。此外，它通过引入自我知识识别器和子文档级别的Token减少器来减少Token。FIT-RAG在三个开放域问答数据集（TriviaQA、NQ和PopQA）上实现了卓越的有效性和效率，并通过大量实验得到了验证。FIT-RAG可以将Llama2-13B-Chat在TriviaQA上的回答准确率提高14.3%，在NQ上提高19.9%，在PopQA上提高27.5%。此外，它在三个数据集上的平均Token节省量约为一半。</p>
<h4 id="_56">一句话总结：</h4>
<p>本文提出了一种名为FIT-RAG的黑盒RAG框架，通过利用事实信息和减少Token数量，显著提高了LLMs在问答任务中的性能和效率。</p>
<hr />
<h2 id="context-quality-matters-in-training-fusion-in-decoder-for-extractive-open-domain-question-answering"><a href="http://arxiv.org/abs/2403.14197v1">Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering</a></h2>
<p>发布时间：2024-03-21</p>
<p>作者：Kosuke Akimoto, Kunihiro Takeoka, Masafumi Oyamada</p>
<h4 id="_57">中文摘要：</h4>
<p>本文研究了在模型训练过程中，上下文的数量和质量如何影响最先进的检索增强生成模型Fusion-in-Decoder（FiD）在抽取式开放域问答任务中的性能。实验结果表明，FiD模型在训练过程中对上下文质量过度拟合，并在不同上下文质量下表现出次优性能。通过实验结果，我们还揭示了使用不同上下文质量训练的FiD模型具有不同的交叉注意力分布模式。具体来说，随着训练过程中上下文质量的提高，FiD模型倾向于更均匀地关注上下文中的每一篇文章。最后，基于这些观察结果，我们提出了一种通过引入交叉注意力分布偏差来减轻对特定上下文质量过度拟合的方法，该方法被证明能够有效提高FiD模型在不同上下文质量下的性能。</p>
<h4 id="_58">一句话总结：</h4>
<p>本文揭示了上下文质量对检索增强生成模型FiD性能的影响，并提出了一种减轻过度拟合的方法，以改善模型在不同上下文质量下的性能。</p>
<hr />
<h2 id="dp-rdm-adapting-diffusion-models-to-private-domains-without-fine-tuning"><a href="http://arxiv.org/abs/2403.14421v3">DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning</a></h2>
<p>发布时间：2024-03-21</p>
<p>作者：Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo</p>
<h4 id="_59">中文摘要：</h4>
<p>文本到图像的扩散模型已被证明存在样本级别的记忆问题，可能会复制出与训练图像几乎完美的复制品，这可能是不希望的。为了解决这个问题，我们开发了第一个差分隐私（DP）检索增强生成算法，能够在提供可证明隐私保证的同时生成高质量的图像样本。具体来说，我们假设可以访问一个在少量公共数据上训练的文本到图像扩散模型，并设计了一个DP检索机制来通过从私有检索数据集中检索的样本来增强文本提示。我们的（差分隐私检索增强扩散模型）（DP-RDM）不需要在检索数据集上进行微调以适应另一个领域，并且可以使用最先进的生成模型生成高质量的图像样本，同时满足严格的DP保证。例如，当在MS-COCO上进行评估时，我们的DP-RDM可以在$\epsilon=10$的隐私预算下生成样本，与仅公开检索相比，在最多10,000次查询中提供了3.5个FID点的改进。</p>
<h4 id="_60">一句话总结：</h4>
<p>该研究开发了一种结合差分隐私和检索增强的文本到图像扩散模型，以生成高质量图像样本的同时确保隐私保护。</p>
<hr />
<h2 id="alphafin-benchmarking-financial-analysis-with-retrieval-augmented-stock-chain-framework"><a href="http://arxiv.org/abs/2403.12582v1">AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework</a></h2>
<p>发布时间：2024-03-19</p>
<p>作者：Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, Jun Huang, Wei Lin</p>
<h4 id="_61">中文摘要：</h4>
<p>金融分析任务主要涵盖两个关键领域：股票趋势预测和相应的金融问答。目前，机器学习和深度学习算法（ML&amp;DL）已被广泛应用于股票趋势预测，取得了显著进展。然而，这些方法无法提供预测的原因，缺乏可解释性和推理过程。此外，它们无法整合文本信息，如财务新闻或报告。同时，大型语言模型（LLMs）具有卓越的文本理解和生成能力。但由于金融训练数据集的稀缺和与实时知识的有限整合，LLMs仍然存在幻觉，无法跟上最新信息。为了应对这些挑战，我们首先发布了AlphaFin数据集，结合了传统研究数据集、实时财务数据和手写思维链（CoT）数据。这对训练LLMs进行金融分析具有积极影响。然后，我们使用AlphaFin数据集对一种称为Stock-Chain的先进方法进行基准测试，该方法有效地解决了金融分析任务，并集成了检索增强生成（RAG）技术。进行了大量实验，以证明我们框架在金融分析中的有效性。</p>
<h4 id="_62">一句话总结：</h4>
<p>本研究通过发布AlphaFin数据集并应用Stock-Chain方法，有效地提升了金融分析任务的预测准确性和可解释性。</p>
<hr />
<h2 id="loops-on-retrieval-augmented-generation-lorag"><a href="http://arxiv.org/abs/2403.15450v1">Loops On Retrieval Augmented Generation (LoRAG)</a></h2>
<p>发布时间：2024-03-18</p>
<p>作者：Ayush Thakur, Rashmi Vashisth</p>
<h4 id="_63">中文摘要：</h4>
<p>本文提出了一种名为LoRAG（Loops On Retrieval Augmented Generation）的新框架，旨在通过引入迭代循环机制来提升检索增强文本生成的质量。该架构集成了生成模型、检索机制和动态循环模块，允许通过与从输入上下文中检索到的相关信息进行交互，对生成的文本进行迭代优化。在基准数据集上的实验评估表明，LoRAG在BLEU分数、ROUGE分数和困惑度方面均超越了现有的最先进模型，展示了其在实现生成文本的连贯性和相关性方面的有效性。定性评估进一步说明了LoRAG产生上下文丰富且连贯输出的能力。这项研究为迭代循环在缓解文本生成挑战中的潜力提供了有价值的见解，将LoRAG定位为该领域的一个有希望的进步。</p>
<h4 id="_64">一句话总结：</h4>
<p>LoRAG通过引入迭代循环机制，显著提升了检索增强文本生成的质量和连贯性。</p>
<hr />
<h2 id="embedded-named-entity-recognition-using-probing-classifiers"><a href="http://arxiv.org/abs/2403.11747v1">Embedded Named Entity Recognition using Probing Classifiers</a></h2>
<p>发布时间：2024-03-18</p>
<p>作者：Nicholas Popovič, Michael Färber</p>
<h4 id="_65">中文摘要：</h4>
<p>从生成的文本中提取语义信息是自动化事实核查或增强检索生成等应用的有用工具。目前，这需要在进行推理时使用单独的模型，这增加了计算成本，或者对语言模型进行破坏性的微调。相反，我们提出直接使用探针分类器将信息提取能力嵌入到预训练的语言模型中，从而实现高效的文本生成和信息提取同步。为此，我们引入了一种称为EMBER的方法，并表明它能够在不进行微调的情况下，在解码器仅语言模型中实现命名实体识别，同时在推理时产生最小的额外计算成本。具体来说，我们使用GPT-2进行的实验表明，EMBER在流式文本生成过程中保持了高令牌生成率，与使用单独的NER模型作为基线相比，速度仅下降了大约1%，而基线的速度下降了43.64%。代码和数据可在https://github.com/nicpopovic/EMBER上获取。</p>
<h4 id="_66">一句话总结：</h4>
<p>我们提出了一种名为EMBER的方法，通过将信息提取能力嵌入到预训练语言模型中，实现了高效的文本生成和信息提取同步，同时保持了高令牌生成率。</p>
<hr />
<h2 id="dynamic-contexts-for-generating-suggestion-questions-in-rag-based-conversational-systems"><a href="http://arxiv.org/abs/2403.11413v1">Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems</a></h2>
<p>发布时间：2024-03-18</p>
<p>作者：Anuja Tayal, Aman Tyagi</p>
<h4 id="_67">中文摘要：</h4>
<p>在与基于检索增强生成（RAG）的对话代理交互时，用户必须精心构建他们的查询以确保被正确理解。然而，用户理解系统的能力可能具有挑战性，导致需要进一步澄清的模糊性问题。本研究旨在通过开发一个建议问题生成器来弥合这一差距。为了生成建议问题，我们的方法涉及利用动态上下文，这包括动态的少样本示例和动态检索的上下文。通过实验，我们表明与其它提示方法相比，动态上下文方法可以生成更好的建议问题。</p>
<h4 id="_68">一句话总结：</h4>
<p>本研究开发了一种基于动态上下文的建议问题生成器，以提高用户与基于RAG的对话代理交互时的查询理解准确性。</p>
<hr />
<h2 id="hdldebugger-streamlining-hdl-debugging-with-large-language-models"><a href="http://arxiv.org/abs/2403.11671v1">HDLdebugger: Streamlining HDL debugging with Large Language Models</a></h2>
<p>发布时间：2024-03-18</p>
<p>作者：Xufeng Yao, Haoyang Li, Tsz Ho Chan, Wenyi Xiao, Mingxuan Yuan, Yu Huang, Lei Chen, Bei Yu</p>
<h4 id="_69">中文摘要：</h4>
<p>在芯片设计领域，硬件描述语言（HDLs）扮演着至关重要的角色。然而，由于HDLs复杂的语法以及在线资源的有限可用性，即使是经验丰富的工程师，调试HDL代码仍然是一项困难且耗时的工作。因此，迫切需要开发自动化的HDL代码调试模型，以减轻硬件工程师的负担。尽管大型语言模型（LLMs）在生成、完成和调试软件代码方面具有强大的能力，但它们在HDL调试这一专业领域的应用仍然有限，并且迄今为止尚未取得令人满意的结果。在本文中，我们提出了一种基于LLM的HDL调试框架，称为HDLdebugger，该框架包括通过逆向工程方法生成HDL调试数据、用于检索增强生成的搜索引擎以及检索增强的LLM微调方法。通过整合这些组件，HDLdebugger可以自动化和简化芯片设计的HDL调试。我们在华为提供的HDL代码数据集上进行的全面实验表明，HDLdebugger优于13个前沿的LLM基线，显示出在HDL代码调试方面的卓越有效性。</p>
<h4 id="_70">一句话总结：</h4>
<p>本文提出了一种基于LLM的HDL调试框架，显著提高了HDL代码调试的效率和准确性。</p>
<hr />
<h2 id="jora-jax-tensor-parallel-lora-library-for-retrieval-augmented-fine-tuning"><a href="http://arxiv.org/abs/2403.11366v2">JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning</a></h2>
<p>发布时间：2024-03-17</p>
<p>作者：Anique Tahir, Lu Cheng, Huan Liu</p>
<h4 id="_71">中文摘要：</h4>
<p>大规模语言模型（LLMs）在检索任务中的应用，尤其是在检索增强生成（RAG）中，面临着显著的内存限制，尤其是在微调大量提示序列时。当前的开放源代码库支持跨多个GPU的全模型推理和微调，但不足以满足检索上下文所需的参数高效分布。为了解决这一差距，我们引入了一个新的框架，用于兼容PEFT（Parameter Efficient Fine-tuning）的Llama-2模型微调，利用分布式训练。我们的框架独特地利用了JAX的即时（JIT）编译和张量分片技术，以实现高效的资源管理，从而在降低内存需求的同时加速微调。这一进步显著提高了在复杂RAG应用中微调LLMs的可扩展性和可行性，即使在有限的GPU资源系统上也是如此。我们的实验表明，与使用四个GPU的Hugging Face/DeepSpeed实现相比，运行时间提高了12倍以上，同时每个GPU消耗的VRAM不到一半。</p>
<h4 id="_72">一句话总结：</h4>
<p>该研究提出了一种基于分布式训练的框架，通过JAX的即时编译和张量分片技术，有效提升了LLMs在RAG任务中的微调效率和可扩展性。</p>
<hr />
<h2 id="enhancing-llm-factual-accuracy-with-rag-to-counter-hallucinations-a-case-study-on-domain-specific-queries-in-private-knowledge-bases"><a href="http://arxiv.org/abs/2403.10446v1">Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases</a></h2>
<p>发布时间：2024-03-15</p>
<p>作者：Jiarui Li, Ye Yuan, Zehua Zhang</p>
<h4 id="_73">中文摘要：</h4>
<p>我们提出了一种端到端系统设计，旨在利用检索增强生成（RAG）来提高大型语言模型（LLMs）在针对特定领域和时间敏感的查询中，与私有知识库相关的真实准确性。我们的系统将RAG管道与上游数据集处理和下游性能评估相结合。为了解决LLM的幻觉问题，我们使用来自卡内基梅隆大学（CMU）广泛资源的一个精心制作的语料库对模型进行微调，并使用教师模型进行标注。我们的实验表明，该系统在生成针对特定领域和时间敏感查询的更准确答案方面非常有效。结果还揭示了使用小规模和倾斜数据集微调LLMs的局限性。这项研究突出了RAG系统在通过外部数据集增强LLMs以改善知识密集型任务性能方面的潜力。我们的代码和模型可在GitHub上获取。</p>
<h4 id="_74">一句话总结：</h4>
<p>本研究提出了一种利用检索增强生成（RAG）技术提高大型语言模型（LLMs）针对特定领域和时间敏感查询真实准确性的端到端系统设计。</p>
<hr />
<h2 id="dragin-dynamic-retrieval-augmented-generation-based-on-the-information-needs-of-large-language-models"><a href="http://arxiv.org/abs/2403.10081v2">DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models</a></h2>
<p>发布时间：2024-03-15</p>
<p>作者：Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu</p>
<h4 id="_75">中文摘要：</h4>
<p>动态检索增强生成（Dynamic Retrieval Augmented Generation，简称DRAG）范式在大型语言模型（Large Language Models，简称LLMs）的文本生成过程中，主动决定何时以及检索什么内容。该范式包含两个关键要素：一是识别激活检索模块的最佳时机（即决定何时检索）；二是触发检索后构建合适的查询（即确定检索什么）。然而，现有的动态RAG方法在这两方面都存在不足。首先，决定何时检索的策略通常依赖于静态规则。其次，决定检索什么内容的策略通常仅限于LLM的最近一句或最后几个标记，而LLM的实际信息需求可能跨越整个上下文。为了克服这些限制，我们提出了一种新的框架，即基于LLM实时信息需求的动态检索增强生成（DRAGIN）。我们的框架专门设计用于在文本生成过程中根据LLM的实时信息需求来决定何时以及检索什么。我们在4个知识密集型生成数据集上对DRAGIN与现有方法进行了全面评估。实验结果表明，DRAGIN在所有任务上都实现了优越的性能，证明了我们方法的有效性。我们已在GitHub上开源了所有代码、数据和模型：https://github.com/oneal2000/DRAGIN/tree/main</p>
<h4 id="_76">一句话总结：</h4>
<p>提出了一种基于LLM实时信息需求的动态检索增强生成框架DRAGIN，有效提升了LLMs在文本生成过程中的检索和生成性能。</p>
<hr />
<h2 id="repoformer-selective-retrieval-for-repository-level-code-completion"><a href="http://arxiv.org/abs/2403.10059v2">Repoformer: Selective Retrieval for Repository-Level Code Completion</a></h2>
<p>发布时间：2024-03-15</p>
<p>作者：Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, Xiaofei Ma</p>
<h4 id="_77">中文摘要：</h4>
<p>近年来，检索增强生成（RAG）在代码补全领域的进展开启了代码库级别代码补全的新时代。然而，现有方法中检索的固定使用暴露了效率和鲁棒性方面的问题，大量检索到的上下文对代码语言模型（code LMs）无益甚至有害。在本文中，我们提出了一种选择性RAG框架，以避免不必要的检索。为此，我们设计了一种自监督学习方法，使代码LM能够准确自我评估检索是否能提高其输出质量，并稳健地利用潜在的噪声检索上下文。使用此LM作为选择性RAG策略和生成模型，我们的框架在包括RepoEval、CrossCodeEval和CrossCodeLongEval（一个新的长格式代码补全基准）在内的多种基准测试中实现了最先进的代码库级别代码补全性能。同时，我们的分析表明，选择性检索在在线服务设置中可以带来高达70%的推理速度提升，而不会损害性能。我们进一步证明，我们的框架能够适应不同的生成模型、检索器和编程语言。这些进步使我们的框架成为向更准确、更高效的代码库级别代码补全迈出的重要一步。</p>
<h4 id="_78">一句话总结：</h4>
<p>本文提出的选择性RAG框架通过自监督学习，实现了高效的代码库级别代码补全，显著提升了在线服务性能。</p>
<hr />
<h2 id="s3llm-large-scale-scientific-software-understanding-with-llms-using-source-metadata-and-document"><a href="http://arxiv.org/abs/2403.10588v1">S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document</a></h2>
<p>发布时间：2024-03-15</p>
<p>作者：Kareem Shaik, Dali Wang, Weijian Zheng, Qinglei Cao, Heng Fan, Peter Schwartz, Yunhe Feng</p>
<h4 id="_79">中文摘要：</h4>
<p>由于大规模科学软件具有多样化的代码库、庞大的代码长度和目标计算架构，对其理解提出了重大挑战。生成式人工智能（特别是大型语言模型LLMs）的出现为理解这类复杂的科学代码提供了新的途径。本文提出了一种基于LLM的框架S3LLM，旨在通过用户友好的界面以交互式、对话式的方式检查源代码、代码元数据和总结信息，并与文本技术报告相结合。S3LLM利用开源的LLaMA-2模型，通过将自然语言查询自动转换为领域特定语言（DSL）查询来增强代码分析。具体来说，它将这些查询转换为特征查询语言（FQL），从而实现整个代码库的高效扫描和解析。此外，S3LLM能够处理多种元数据类型，包括DOT、SQL和自定义格式。此外，S3LLM还集成了检索增强生成（RAG）和LangChain技术，可以直接查询大量文档。S3LLM展示了使用本地部署的开源LLMs快速理解大规模科学计算软件的潜力，消除了对广泛编码专业知识的需求，从而使得过程更加高效和有效。S3LLM可在https://github.com/ResponsibleAILab/s3llm上获取。</p>
<h4 id="_80">一句话总结：</h4>
<p>S3LLM是一种基于LLM的框架，通过将自然语言查询转换为领域特定语言查询，以交互式方式快速理解大规模科学软件。</p>
<hr />
<h2 id="socialgenpod-privacy-friendly-generative-ai-social-web-applications-with-decentralised-personal-data-stores"><a href="http://arxiv.org/abs/2403.10408v1">SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores</a></h2>
<p>发布时间：2024-03-15</p>
<p>作者：Vidminas Vizgirda, Rui Zhao, Naman Goel</p>
<h4 id="_81">中文摘要：</h4>
<p>本文提出了一种名为SocialGenPod的去中心化和隐私友好的部署生成式AI Web应用的方法。与将用户数据绑定到应用和服务提供商的集中式Web和数据架构不同，本文展示了如何使用Solid（一种去中心化Web规范）将用户数据与生成式AI应用解耦。我们通过一个原型来演示SocialGenPod，该原型允许用户与不同的大型语言模型进行对话，并可选择利用检索增强生成技术，基于用户有权直接或间接访问的任何Solid Pod中存储的私有文档生成答案。SocialGenPod利用Solid访问控制机制，使用户能够完全控制谁有权访问其Pod中存储的数据。SocialGenPod将所有用户数据（聊天历史、应用配置、个人文档等）安全地存储在用户的个人Pod中，与特定的模型或应用提供商分离。除了更好的隐私控制外，这种方法还实现了在不同服务和应用之间的可移植性。最后，我们讨论了由最先进模型的大规模计算需求带来的挑战，并指出未来研究应解决这些问题。我们的原型是开源的，可在以下链接获取：https://github.com/Vidminas/socialgenpod/。</p>
<h4 id="_82">一句话总结：</h4>
<p>SocialGenPod通过去中心化技术和隐私保护机制，实现了生成式AI应用的隐私友好部署和数据可移植性。</p>
<hr />
<h2 id="improving-medical-multi-modal-contrastive-learning-with-expert-annotations"><a href="http://arxiv.org/abs/2403.10153v3">Improving Medical Multi-modal Contrastive Learning with Expert Annotations</a></h2>
<p>发布时间：2024-03-15</p>
<p>作者：Yogesh Kumar, Pekka Marttinen</p>
<h4 id="_83">中文摘要：</h4>
<p>我们引入了eCLIP，这是CLIP模型的一个增强版本，它通过整合放射科医生的眼动热图形式的专家标注来克服对比多模态医学影像分析中的关键挑战，特别是数据稀缺性和“模态差距”——图像和文本嵌入之间的显著差异，这降低了表示质量并阻碍了跨模态互操作性。eCLIP集成了热图处理器，并利用mixup增强技术来有效地利用稀缺的专家标注，从而提高模型的学习效率。eCLIP被设计成可以普遍适用于任何CLIP变体，而无需对核心架构进行任何修改。通过在多个任务上的详细评估，包括零样本推理、线性探测、跨模态检索以及使用冻结的大型语言模型对放射学报告进行检索增强生成（RAG），eCLIP展示了嵌入质量的持续改进。这些结果揭示了增强的对齐和一致性，证实了eCLIP在医学影像领域利用高质量标注进行丰富多模态分析的能力。</p>
<h4 id="_84">一句话总结：</h4>
<p>eCLIP通过整合专家标注和混合增强技术，显著提升了CLIP模型在医学影像分析中的嵌入质量和对齐一致性。</p>
<hr />
<h2 id="retrieval-augmented-text-to-sql-generation-for-epidemiological-question-answering-using-electronic-health-records"><a href="http://arxiv.org/abs/2403.09226v2">Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records</a></h2>
<p>发布时间：2024-03-14</p>
<p>作者：Angelo Ziletti, Leonardo D'Ambrosi</p>
<h4 id="_85">中文摘要：</h4>
<p>电子健康记录（EHR）和索赔数据是反映患者健康状况和医疗保健利用的丰富现实世界数据来源。由于医学术语的复杂性和需要复杂的SQL查询，查询这些数据库以回答流行病学问题是具有挑战性的。在这里，我们介绍了一种端到端的方法，该方法结合了文本到SQL生成和检索增强生成（RAG），以使用EHR和索赔数据回答流行病学问题。我们表明，我们的方法将医学编码步骤整合到文本到SQL过程中，与简单的提示相比，显著提高了性能。我们的发现表明，尽管当前的语言模型尚未足够准确以进行无监督使用，但在现实行业环境中，RAG为提高其能力提供了一个有希望的方向。</p>
<h4 id="_86">一句话总结：</h4>
<p>本研究提出了一种结合文本到SQL生成和检索增强生成的方法，以利用EHR和索赔数据回答流行病学问题，并显著提高了性能。</p>
<hr />
<h2 id="ragged-towards-informed-design-of-retrieval-augmented-generation-systems"><a href="http://arxiv.org/abs/2403.09040v1">RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems</a></h2>
<p>发布时间：2024-03-14</p>
<p>作者：Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, Graham Neubig</p>
<h4 id="_87">中文摘要：</h4>
<p>检索增强生成（RAG）通过为诸如基于文档的问答（DBQA）等任务提供额外上下文，极大地提升了语言模型（LMs）的性能。尽管RAG具有巨大潜力，但其效能高度依赖于其配置，引发了关于最佳RAG配置的问题。为了回答这一问题，我们引入了RAGGED框架来分析和优化RAG系统。在一系列代表性的DBQA任务上，我们研究了两种经典的稀疏和密集检索器，以及四种在编码器-解码器和仅解码器架构中表现最佳的LMs。通过RAGGED，我们发现不同的模型适用于不同的RAG配置。尽管编码器-解码器模型随着文档数量的增加而单调提升，但我们发现仅解码器模型只能有效利用&lt;5个文档，尽管它们通常具有更长的上下文窗口。RAGGED进一步揭示了LMs的上下文利用习惯，我们发现编码器-解码器模型更依赖于上下文，因此对检索质量更为敏感，而仅解码器模型则倾向于依赖训练期间记忆的知识。</p>
<h4 id="_88">一句话总结：</h4>
<p>RAGGED框架揭示了不同类型的语言模型在检索增强生成中的配置差异，揭示了编码器-解码器模型对检索质量敏感，而仅解码器模型更依赖训练记忆的知识。</p>
<hr />
<h2 id="re-search-for-the-truth-multi-round-retrieval-augmented-large-language-models-are-strong-fake-news-detectors"><a href="http://arxiv.org/abs/2403.09747v1">Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors</a></h2>
<p>发布时间：2024-03-14</p>
<p>作者：Guanghua Li, Wensheng Lu, Wei Zhang, Defu Lian, Kezhong Lu, Rui Mao, Kai Shu, Hao Liao</p>
<h4 id="_89">中文摘要：</h4>
<p>虚假新闻的泛滥对政治、经济和社会产生了深远的影响。尽管已经采用了虚假新闻检测方法来减轻这一问题，但这些方法主要依赖于两个基本要素：证据的质量和相关性，以及判决预测机制的有效性。传统方法通常从静态存储库如维基百科中获取信息，但受限于过时或不完整的数据，尤其是在处理新兴或罕见主张时。大型语言模型（LLMs）因其卓越的推理和生成能力而闻名，为虚假新闻检测带来了新的前沿。然而，与传统的类似，基于LLM的解决方案也面临着陈旧和长尾知识的局限性。此外，检索增强的LLMs经常面临诸如低质量证据检索和上下文长度限制等问题。为了解决这些挑战，我们引入了一个新颖的检索增强LLMs框架——这是第一个能够自动和策略性地从网络来源中提取关键证据以进行主张验证的框架。采用多轮检索策略，我们的框架确保获取了足够的相关证据，从而提高了性能。在三个真实世界数据集上的全面实验验证了该框架相较于现有方法的优越性。重要的是，我们的模型不仅提供准确的判决，还提供了可读性强的解释，以改善结果的可解释性。</p>
<h4 id="_90">一句话总结：</h4>
<p>该研究提出了一种新颖的检索增强LLMs框架，用于自动从网络中提取关键证据，以更有效地检测和验证虚假新闻。</p>
<hr />
<h2 id="exploring-the-capabilities-and-limitations-of-large-language-models-in-the-electric-energy-sector"><a href="http://arxiv.org/abs/2403.09125v5">Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector</a></h2>
<p>发布时间：2024-03-14</p>
<p>作者：Subir Majumder, Lin Dong, Fatemeh Doudi, Yuting Cai, Chao Tian, Dileep Kalathi, Kevin Ding, Anupam A. Thatte, Na Li, Le Xie</p>
<h4 id="_91">中文摘要：</h4>
<p>大型语言模型（LLMs）作为聊天机器人因其自然语言处理能力和广泛任务的应用而受到广泛关注。尽管在所有可能的行业中采用基于这种基础模型的人工智能工具的热情很高，但这类LLMs在改善电力行业运营方面的能力和局限性需要被探索，本文确定了这方面的有益方向。关键的未来研究方向包括用于微调LLMs的数据收集系统、将电力系统特定工具嵌入到LLMs中，以及基于检索增强生成（RAG）的知识库，以提高LLMs响应的质量和在关键安全应用中的LLMs性能。</p>
<h4 id="_92">一句话总结：</h4>
<p>本文探讨了大型语言模型在电力行业应用中的潜力和研究方向，包括数据收集、工具嵌入和知识库构建。</p>
<hr />
<h2 id="detecting-hallucination-and-coverage-errors-in-retrieval-augmented-generation-for-controversial-topics"><a href="http://arxiv.org/abs/2403.08904v1">Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics</a></h2>
<p>发布时间：2024-03-13</p>
<p>作者：Tyler A. Chang, Katrin Tomanek, Jessica Hoffmann, Nithum Thain, Erin van Liemt, Kathleen Meier-Hellstern, Lucas Dixon</p>
<h4 id="_93">中文摘要：</h4>
<p>本文探讨了基于LLM（大型语言模型）的聊天机器人处理争议性话题的策略，该策略基于维基百科的中立观点（NPOV）原则：承认不存在唯一正确答案，并呈现多个观点。我们将此视为检索增强生成，其中观点从知识库中检索，而LLM的任务是从给定的观点中生成流畅且忠实于原文的回答。作为起点，我们使用确定性检索系统，然后关注在文本生成过程中出现的常见LLM失败模式，即幻觉和覆盖错误。我们基于（1）词重叠、（2）显著性和（3）基于LLM的分类器提出了三种检测此类错误的方法。我们的结果表明，即使是仅基于合成错误训练的LLM分类器，也能达到高错误检测性能，在模糊错误案例中，幻觉检测的ROC AUC得分为95.3%，覆盖错误检测的ROC AUC得分为90.5%。我们展示了在没有训练数据的情况下，我们的其他方法在幻觉检测（84.0%）和覆盖错误检测（85.2%）上仍然能取得良好的结果。</p>
<h4 id="_94">一句话总结：</h4>
<p>本文提出了一种基于维基百科中立观点原则的策略，用于改进LLM聊天机器人处理争议性话题的能力，并通过多种方法有效检测文本生成中的幻觉和覆盖错误。</p>
<hr />
<h2 id="medinsight-a-multi-source-context-augmentation-framework-for-generating-patient-centric-medical-responses-using-large-language-models"><a href="http://arxiv.org/abs/2403.08607v1">MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models</a></h2>
<p>发布时间：2024-03-13</p>
<p>作者：Subash Neupane, Shaswata Mitra, Sudip Mittal, Noorbakhsh Amiri Golilarz, Shahram Rahimi, Amin Amirlatifi</p>
<h4 id="_95">中文摘要：</h4>
<p>大型语言模型（LLMs）在生成类似人类的响应方面表现出令人印象深刻的能力。然而，它们缺乏特定领域的知识，限制了其在医疗保健环境中的应用，在这种环境中，上下文和全面的响应至关重要。为了解决这一挑战并能够生成与上下文相关且全面的以患者为中心的响应，我们提出了MedInsight：一种新颖的检索增强框架，该框架通过从多个来源添加相关背景信息来增强LLM输入（提示）。MedInsight从患者的病历或咨询记录中提取相关信息。然后，根据患者的健康史和状况，它整合来自权威医学教科书和精选网络资源的信息。通过构建一个增强的上下文，将患者的记录与相关医学知识相结合，MedInsight生成丰富、针对特定患者的响应，适用于医疗保健应用，如诊断、治疗建议或患者教育。在MTSamples数据集上的实验验证了MedInsight在生成上下文适当的医疗响应方面的有效性。使用Ragas指标和TruLens进行答案相似性和答案正确性的定量评估证明了该模型的有效性。此外，涉及主题专家（SMEs）的人类评估研究证实了MedInsight的实用性，对生成的响应的相关性和正确性有适度的评分者间一致性。</p>
<h4 id="_96">一句话总结：</h4>
<p>MedInsight通过整合患者病历和医学知识，增强了大型语言模型在医疗保健领域生成上下文相关和全面响应的能力。</p>
<hr />
<h2 id="from-human-experts-to-machines-an-llm-supported-approach-to-ontology-and-knowledge-graph-construction"><a href="http://arxiv.org/abs/2403.08345v1">From human experts to machines: An LLM supported approach to ontology and knowledge graph construction</a></h2>
<p>发布时间：2024-03-13</p>
<p>作者：Vamsi Krishna Kommineni, Birgitta König-Ries, Sheeba Samuel</p>
<h4 id="_97">中文摘要：</h4>
<p>传统的构建本体（Ontologies）和知识图谱（KGs）的过程高度依赖人类领域专家来定义实体和关系类型、建立层次结构、保持与领域的相关性、填充ABox（或用实例填充），并确保数据质量（包括准确性、完整性等）。另一方面，大型语言模型（LLMs）因其理解和生成类似人类自然语言的能力而近年来受到关注，为自动化这一过程提供了有前景的方法。本研究探讨了利用开源LLMs（大型语言模型）促进的（半）自动构建KGs的方法。我们的流程包括制定能力问题（CQs）、基于这些CQs开发本体（TBox）、使用开发的本体构建KGs，以及以最小或无人类专家参与的方式评估结果KG。我们通过利用学术出版物创建一个关于深度学习方法论的KG，展示了我们半自动化流程的可行性。为了评估通过检索增强生成（RAG）生成的答案以及使用LLMs自动提取的KG概念，我们设计了一个裁判LLM，该LLM根据事实真相对生成的内容进行评分。我们的发现表明，使用LLMs有可能减少构建KGs中涉及的人类工作量，尽管推荐使用人工参与的方式来评估自动生成的KGs。</p>
<h4 id="_98">一句话总结：</h4>
<p>本研究提出了一种利用大型语言模型（LLMs）半自动构建知识图谱的方法，通过减少人工参与，提高了知识图谱构建的效率和质量。</p>
<hr />
<h2 id="investigating-the-performance-of-retrieval-augmented-generation-and-fine-tuning-for-the-development-of-ai-driven-knowledge-based-systems"><a href="http://arxiv.org/abs/2403.09727v1">Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems</a></h2>
<p>发布时间：2024-03-12</p>
<p>作者：Robert Lakatos, Peter Pollner, Andras Hajdu, Tamas Joo</p>
<h4 id="_99">中文摘要：</h4>
<p>本文探讨了生成式大型语言模型（G-LLM）在开发类似ChatGPT、Bing或Gemini的知识系统方面的应用。微调（FN）和检索增强生成（RAG）是用于实现领域适应的技术。本研究中，我们使用ROUGE、BLEU、METEOR评分和余弦相似度，比较和分析了GPT-J-6B、OPT-6.7B、LlaMA、LlaMA-2语言模型在RAG和FN技术下的性能。基于不同数据集上的测量结果，我们证明了基于RAG的构建比使用FN生成的模型更高效。我们指出，将RAG与FN连接并非易事，因为将FN模型与RAG连接可能导致性能下降。此外，我们概述了一种简单的基于RAG的架构，该架构在ROUGE评分上平均比FN模型高出16%，在BLEU评分上高出15%，在余弦相似度上高出53%。这表明RAG在幻觉方面相对于FN具有显著优势，而FN模型平均8%更好的METEOR评分表明其相比RAG具有更高的创造力。</p>
<h4 id="_100">一句话总结：</h4>
<p>本研究表明，基于RAG的架构在生成式大型语言模型中比微调模型更有效，尽管微调模型在创造力方面略胜一筹。</p>
<hr />
<h2 id="generating-clarification-questions-for-disambiguating-contracts"><a href="http://arxiv.org/abs/2403.08053v1">Generating Clarification Questions for Disambiguating Contracts</a></h2>
<p>发布时间：2024-03-12</p>
<p>作者：Anmol Singhal, Chirag Jain, Preethu Rose Anish, Arkajyoti Chakraborty, Smita Ghaisas</p>
<h4 id="_101">中文摘要：</h4>
<p>企业经常签订商业合同，这些合同可以作为项目特定需求的重要来源。合同条款是强制性的，从合同中提取的需求可以详细说明非法律利益相关者（包括需求分析师、工程师和交付人员）需要执行的下游实施活动。然而，由于法律术语的广泛使用和合同语言的固有复杂性，对于这些利益相关者来说，理解合同是一项认知要求高且容易出错的任务。此外，合同通常包含措辞含糊的条款以确保全面覆盖。相比之下，非法律利益相关者需要对合同条款有详细且明确的理解，以便制定可执行的需求。在这项工作中，我们引入了一个新的法律自然语言处理（NLP）任务，该任务涉及为合同生成澄清问题。这些问题旨在识别文档层面的合同歧义，从而帮助非法律利益相关者获得提取需求所需的相关细节。这项任务面临着三个核心问题：（1）数据可用性，（2）合同的长度和无结构性质，（3）法律文本的复杂性。为了解决这些问题，我们提出了ConRAP，这是一个用于生成澄清问题的检索增强提示框架，以消除合同文本的歧义。在公开可用的CUAD数据集来源的合同上进行的实验表明，ConRAP与ChatGPT结合使用可以检测到歧义，F2分数为0.87。70%生成的澄清问题被认为是有用的，由人类评估者进行评估。</p>
<h4 id="_102">一句话总结：</h4>
<p>本研究提出了一种名为ConRAP的检索增强提示框架，用于生成澄清问题，帮助非法律利益相关者理解合同中的歧义，从而提高需求提取的准确性。</p>
<hr />
<h2 id="development-of-a-reliable-and-accessible-caregiving-language-model-calm"><a href="http://arxiv.org/abs/2403.06857v1">Development of a Reliable and Accessible Caregiving Language Model (CaLM)</a></h2>
<p>发布时间：2024-03-11</p>
<p>作者：Bambang Parmanto, Bayu Aryoyudanta, Wilbert Soekinto, I Made Agus Setiawan, Yuhan Wang, Haomin Hu, Andi Saptono, Yong K. Choi</p>
<h4 id="_103">中文摘要：</h4>
<p>与专业护理人员不同，家庭护理人员往往在没有正式准备或培训的情况下承担这一角色。因此，迫切需要提高家庭护理人员提供高质量护理的能力。大型语言模型可以作为一种基础技术，作为教育工具或护理的辅助手段来支持护理人员。本研究旨在通过使用事实记忆（FM）和护理知识库来开发一个可靠的护理语言模型（CaLM），通过使用小型FM来开发一个易于访问的CaLM，该小型FM需要较少的计算资源，并评估该模型与大型FM相比的性能。我们使用检索增强生成（RAG）框架，并结合FM微调，通过将模型建立在护理知识库的基础上来提高FM答案的质量，开发了CaLM。我们将两个小型FM（LLaMA-2和参数为7B的Falcon）作为CaLM的FM候选者，并将更大的FM GPT-3.5作为基准。我们通过从互联网收集各种类型的文档来开发护理知识库。在本研究中，我们专注于阿尔茨海默病相关痴呆症患者的护理人员。我们使用在评估语言模型时常用的基准指标来评估模型的性能，并评估其提供准确参考答案的可靠性。RAG框架提高了本研究中使用的所有FM在所有指标上的性能。正如预期的那样，大型FM在所有指标上都优于小型FM。最有趣的结果是，使用RAG的小型微调FM在所有指标上都显著优于GPT 3.5。微调后的LLaMA-2小型FM在返回参考答案方面优于GPT 3.5（即使使用RAG）。这项研究表明，可以通过使用针对护理领域特定知识库的小型FM来开发可靠且易于访问的CaLM。</p>
<h4 id="_104">一句话总结：</h4>
<p>本研究开发了一种基于小型事实记忆模型和护理知识库的护理语言模型，提高了护理人员的护理质量。</p>
<hr />
<h2 id="ra-isf-learning-to-answer-and-understand-from-retrieval-augmentation-via-iterative-self-feedback"><a href="http://arxiv.org/abs/2403.06840v2">RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback</a></h2>
<p>发布时间：2024-03-11</p>
<p>作者：Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, Tianyu Du</p>
<h4 id="_105">中文摘要：</h4>
<p>大型语言模型（LLMs）在众多任务中表现出色，但仍然高度依赖于其参数中存储的知识。此外，更新这些知识需要高昂的训练成本。检索增强生成（RAG）方法通过整合外部知识来解决这一问题。模型可以通过检索与查询相关的知识来回答它之前无法回答的问题。这种方法在某些特定任务和场景中提高了性能。然而，如果检索到无关文本，可能会损害模型性能。在本文中，我们提出了检索增强迭代自我反馈（RA-ISF）框架，该框架通过迭代分解任务并在三个子模块中处理它们来增强模型的问题解决能力。实验表明，我们的方法优于现有基准，在GPT3.5、Llama2等模型上表现良好，显著增强了事实推理能力并减少了幻觉。</p>
<h4 id="_106">一句话总结：</h4>
<p>本文提出的RA-ISF框架通过迭代分解任务并整合外部知识，显著提升了大型语言模型在事实推理和减少幻觉方面的性能。</p>
<hr />
<h2 id="piperag-fast-retrieval-augmented-generation-via-algorithm-system-co-design"><a href="http://arxiv.org/abs/2403.05676v1">PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design</a></h2>
<p>发布时间：2024-03-08</p>
<p>作者：Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, Tim Kraska</p>
<h4 id="_107">中文摘要：</h4>
<p>检索增强生成（RAG）通过整合外部标记数据库可以提升大型语言模型（LLMs）的生成质量。然而，从大型数据库中检索信息可能会占据整体生成时间的大部分，尤其是在需要定期进行检索以使检索内容与生成状态的最新状态保持一致时。在本文中，我们提出了PipeRAG，这是一种新颖的算法-系统协同设计方法，旨在降低生成延迟并提升生成质量。PipeRAG集成了以下三个方面：（1）管道并行性，以实现检索和生成过程的并发执行；（2）灵活的检索间隔，以最大化管道并行性的效率；（3）性能模型，根据生成状态和底层硬件自动平衡检索质量和延迟。我们的评估表明，通过结合上述三种方法，PipeRAG在端到端生成延迟方面实现了高达2.6倍的加速，同时提升了生成质量。这些有希望的结果展示了与底层系统协同设计算法的有效性，为PipeRAG在未来的RAG系统中的应用铺平了道路。</p>
<h4 id="_108">一句话总结：</h4>
<p>PipeRAG通过协同设计算法和系统，显著降低了检索增强生成过程中的延迟并提升了生成质量。</p>
<hr />
<h2 id="rat-retrieval-augmented-thoughts-elicit-context-aware-reasoning-in-long-horizon-generation"><a href="http://arxiv.org/abs/2403.05313v1">RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation</a></h2>
<p>发布时间：2024-03-08</p>
<p>作者：Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, Yitao Liang</p>
<h4 id="_109">中文摘要：</h4>
<p>本文探讨了如何通过信息检索迭代地修正一系列思维，显著提高大型语言模型在长时生成任务中的推理和生成能力，同时大幅减轻幻觉现象。特别是，提出的方法——<em>检索增强思维</em>（RAT）——在生成初始零样本思维链后，逐个步骤地使用与任务查询、当前和过去思维步骤相关的检索信息进行修正。将RAT应用于GPT-3.5、GPT-4和CodeLLaMA-7b，显著提升了它们在各种长时生成任务上的表现；平均而言，代码生成、数学推理、创意写作和具身任务规划的评分分别相对提高了13.63%、16.96%、19.2%和42.78%。演示页面可在https://craftjarvis.github.io/RAT找到。</p>
<h4 id="_110">一句话总结：</h4>
<p>本文提出了一种名为“检索增强思维”（RAT）的方法，通过信息检索迭代修正思维链，显著提升了大型语言模型在长时生成任务中的表现，并减轻了幻觉现象。</p>
<hr />
<h2 id="telecom-language-models-must-they-be-large"><a href="http://arxiv.org/abs/2403.04666v2">Telecom Language Models: Must They Be Large?</a></h2>
<p>发布时间：2024-03-07</p>
<p>作者：Nicola Piovesan, Antonio De Domenico, Fadhel Ayed</p>
<h4 id="_111">中文摘要：</h4>
<p>随着电信行业对大型语言模型（LLMs）兴趣的增加，这些模型在提高运营效率方面的潜力日益凸显。然而，这些复杂模型的部署常常受到其庞大体积和计算需求的影响，这在资源受限的环境中引发了对其可行性的担忧。为了应对这一挑战，近期的研究进展中出现了小型语言模型，这些模型在许多任务（如编码和常识推理）中表现出与大型模型相当的性能，这令人惊讶。Phi-2，一个紧凑而强大的模型，代表了这一波高效小型语言模型的新浪潮。本文对Phi-2在电信领域内在理解能力进行了全面评估。鉴于规模相关的限制，我们通过检索增强生成方法增强了Phi-2的能力，精心整合了一个专门为电信标准规范定制的广泛知识库。增强后的Phi-2模型在准确性方面取得了显著提升，以接近资源密集型GPT-3.5的精确度回答了电信标准的问题。本文进一步探讨了Phi-2在电信领域内解决问题场景中的改进能力，突出了其潜力和局限性。</p>
<h4 id="_112">一句话总结：</h4>
<p>本文通过增强小型语言模型Phi-2的能力，展示了其在电信领域问题解决中的潜力，同时揭示了其局限性和改进空间。</p>
<hr />
<h2 id="halueval-wild-evaluating-hallucinations-of-language-models-in-the-wild"><a href="http://arxiv.org/abs/2403.04307v2">HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild</a></h2>
<p>发布时间：2024-03-07</p>
<p>作者：Zhiying Zhu, Yiming Yang, Zhiqing Sun</p>
<h4 id="_113">中文摘要：</h4>
<p>幻觉对大型语言模型（LLMs）在关键领域的可靠性构成了重大挑战。近期设计的用于评估LLMs在传统NLP任务（如知识密集型问答和摘要）中幻觉的基准，不足以捕捉动态、现实场景中用户与LLMs交互的复杂性。为了解决这一差距，我们引入了HaluEval-Wild，这是第一个专门设计来评估野外LLM幻觉的基准。我们精心收集了来自现有现实世界用户-LLM交互数据集（包括ShareGPT）的具有挑战性的（由Alpaca进行对抗性过滤）用户查询，以评估各种LLMs的幻觉率。在分析收集到的查询后，我们将它们分为五种不同的类型，这有助于对LLMs展现的幻觉类型进行细致的分析，并使用强大的GPT-4模型和检索增强生成（RAG）来综合参考答案。我们的基准提供了一种新颖的方法，以增强我们对反映现实世界交互场景的LLM可靠性的理解和改进。我们的基准可在https://github.com/Dianezzy/HaluEval-Wild获取。</p>
<h4 id="_114">一句话总结：</h4>
<p>HaluEval-Wild基准旨在评估LLMs在现实世界场景中的幻觉，通过收集真实用户查询并使用GPT-4和RAG技术进行细致分析，以提升LLMs的可靠性。</p>
<hr />
<h2 id="ultrawiki-ultra-fine-grained-entity-set-expansion-with-negative-seed-entities"><a href="http://arxiv.org/abs/2403.04247v2">UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities</a></h2>
<p>发布时间：2024-03-07</p>
<p>作者：Yangning Li, Qingsong Lv, Tianyu Yu, Yinghui Li, Shulin Huang, Tingwei Lu, Xuming Hu, Wenhao JIang, Hai-Tao Zheng, Hui Wang</p>
<h4 id="_115">中文摘要：</h4>
<p>实体集扩展（Entity Set Expansion，ESE）旨在识别与给定种子实体集属于同一语义类的新实体。传统方法主要依赖于正种子实体来表示目标语义类，这对于超细粒度语义类的表示构成了挑战。超细粒度语义类是基于具有更具体属性约束的细粒度语义类定义的。仅用正种子实体来描述它会导致两个问题：（i）超细粒度语义类之间的歧义。（ii）无法定义“不想要的”语义。由于这些固有的不足，先前的方法在处理超细粒度实体集扩展（Ultra-ESE）时遇到了困难。为了解决这个问题，我们首先在输入中引入了负种子实体，这些实体属于与正种子实体相同的细粒度语义类，但在某些属性上有所不同。负种子实体通过正负属性的对比消除了语义歧义，同时提供了一种直接表达“不想要的”语义的方法。为了评估模型在Ultra-ESE中的性能，我们构建了UltraWiki，这是第一个针对Ultra-ESE的大规模数据集。UltraWiki包含236个超细粒度语义类，其中每个查询都由3-5个正负种子实体表示。我们提出了一个基于检索的框架RetExpan和一个基于生成的框架GenExpan，从两种不同的范式全面评估大型语言模型在Ultra-ESE中的有效性。此外，我们设计了三种策略来增强模型对超细粒度实体语义的理解：对比学习、检索增强和思维链推理。大量的实验证实了我们提出策略的有效性，同时也揭示了在Ultra-ESE中仍有很大的改进空间。</p>
<h4 id="_116">一句话总结：</h4>
<p>本文提出了一种结合负种子实体和多种策略的Ultra-ESE方法，有效提升了超细粒度语义类实体集扩展的性能。</p>
<hr />
<h2 id="federated-recommendation-via-hybrid-retrieval-augmented-generation"><a href="http://arxiv.org/abs/2403.04256v1">Federated Recommendation via Hybrid Retrieval Augmented Generation</a></h2>
<p>发布时间：2024-03-07</p>
<p>作者：Huimin Zeng, Zhenrui Yue, Qian Jiang, Dong Wang</p>
<h4 id="_117">中文摘要：</h4>
<p>联邦推荐（Federated Recommendation，FR）作为一种新型范式，能够实现隐私保护下的推荐。然而，传统的联邦推荐系统通常使用离散身份（IDs）来表示用户和物品，由于联邦推荐中的数据稀疏性和异质性，这通常会导致性能下降。另一方面，大型语言模型（Large Language Models，LLMs）作为推荐器在各种推荐场景中已被证明是有效的。然而，基于LLM的推荐器面临着诸如低推理效率和潜在的幻觉等挑战，这影响了它们在实际场景中的性能。为此，我们提出了GPT-FedRec，这是一种利用ChatGPT和一种新颖的混合检索增强生成（Retrieval Augmented Generation，RAG）机制的联邦推荐框架。GPT-FedRec是一个两阶段解决方案。第一阶段是一个混合检索过程，挖掘基于ID的用户模式和基于文本的物品特征。接下来，检索到的结果被转换为文本提示并输入到GPT中进行重新排序。我们提出的混合检索机制和基于LLM的重新排序旨在从数据中提取通用特征并利用LLM中的预训练知识，克服联邦推荐中的数据稀疏性和异质性。此外，RAG方法还可以防止LLM的幻觉，提高为现实世界用户推荐的性能。在多个基准数据集上的实验结果表明，GPT-FedRec在性能上优于最先进的基线方法。</p>
<h4 id="_118">一句话总结：</h4>
<p>GPT-FedRec通过结合ChatGPT和RAG机制，实现了在联邦推荐场景中利用LLM进行高效且隐私保护的推荐。</p>
<hr />
<h2 id="faaf-facts-as-a-function-for-the-evaluation-of-generated-text"><a href="http://arxiv.org/abs/2403.03888v2">FaaF: Facts as a Function for the evaluation of generated text</a></h2>
<p>发布时间：2024-03-06</p>
<p>作者：Vasileios Katranidis, Gabor Barany</p>
<h4 id="_119">中文摘要：</h4>
<p>随着大型语言模型（LMs）生成的文本中信息准确性和效率验证的需求达到历史最高水平，但这一需求仍未得到解决。近期的研究工作主要集中在通过提示LM评估者从这些文本中提取和验证原子事实。然而，我们证明了当面对不完整或不准确参考信息时，这种提示方法并不可靠。我们引入了“事实作为函数”（FaaF），这是一种利用LM的函数调用能力的新方法，用于事实验证任务。FaaF显著增强了LM识别文本中不支持事实的能力，同时与基于提示的方法相比，提高了效率并显著降低了成本。此外，我们还提出了一种用于评估检索增强生成（RAG）系统中事实召回的框架，我们使用该框架在具有挑战性的条件下比较了基于提示和FaaF方法使用各种LM的效果。</p>
<h4 id="_120">一句话总结：</h4>
<p>FaaF通过利用大型语言模型的函数调用能力，提高了事实验证的准确性和效率，同时降低了成本。</p>
<hr />
<h2 id="neural-exec-learning-and-learning-from-execution-triggers-for-prompt-injection-attacks"><a href="http://arxiv.org/abs/2403.03792v2">Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks</a></h2>
<p>发布时间：2024-03-06</p>
<p>作者：Dario Pasquini, Martin Strohmeier, Carmela Troncoso</p>
<h4 id="_121">中文摘要：</h4>
<p>我们介绍了一种新的提示注入攻击方法，称为Neural Exec（神经网络执行）。与依赖手工制作的字符串（例如，“忽略之前的指令并...”）的已知攻击不同，我们表明可以将执行触发器的创建视为一个可微分的搜索问题，并使用基于学习的方法来自主生成它们。我们的结果表明，一个有动机的攻击者可以伪造出不仅比当前手工制作的触发器效果显著更好，而且在形状、属性和功能上表现出固有灵活性的触发器。在这一方向上，我们展示了攻击者可以设计和生成能够在多阶段预处理管道中持续存在的Neural Execs，例如在基于检索增强生成（RAG）的应用中。更重要的是，我们的发现表明，攻击者可以产生形式和形状与任何已知攻击明显不同的触发器，从而绕过现有的基于黑名单的检测和清理方法。</p>
<h4 id="_122">一句话总结：</h4>
<p>提出了一种名为Neural Exec的新型神经网络执行攻击方法，能够自主生成更有效且灵活的触发器，绕过现有检测方法。</p>
<hr />
<h2 id="meacap-memory-augmented-zero-shot-image-captioning"><a href="http://arxiv.org/abs/2403.03715v1">MeaCap: Memory-Augmented Zero-shot Image Captioning</a></h2>
<p>发布时间：2024-03-06</p>
<p>作者：Zequn Zeng, Yan Xie, Hao Zhang, Chiyu Chen, Zhengjue Wang, Bo Chen</p>
<h4 id="_123">中文摘要：</h4>
<p>零样本图像描述（IC）在没有良好配对的图像-文本数据的情况下可以分为两类：无训练和仅文本训练。通常，这两种类型的方法通过整合预训练的视觉-语言模型（如CLIP）进行图像-文本相似性评估和预训练的语言模型（LM）进行描述生成来实现零样本IC。它们之间的主要区别在于是否使用文本语料库来训练LM。尽管在某些指标上取得了吸引人的性能，但现有方法通常存在一些共同的缺点。无训练方法往往会产生幻觉，而仅文本训练则常常失去泛化能力。为了进一步发展，在本文中，我们提出了一种新颖的内存增强零样本图像描述框架（MeaCap）。具体来说，我们配备了一个文本记忆，引入了一个检索然后过滤模块来获取与图像高度相关的关键概念。通过在我们的关键词到句子LM中部署所提出的内存增强视觉相关融合分数，MeaCap可以生成以概念为中心的描述，与图像保持高度一致性，同时幻觉更少，世界知识更多。MeaCap的框架在一系列零样本IC设置中实现了最先进的性能。我们的代码可在https://github.com/joeyz0z/MeaCap上获取。</p>
<h4 id="_124">一句话总结：</h4>
<p>本文提出了一种基于内存增强的零样本图像描述框架（MeaCap），通过结合文本记忆和视觉相关融合分数，实现了更准确、更少幻觉的图像描述。</p>
<hr />
<h2 id="magid-an-automated-pipeline-for-generating-synthetic-multi-modal-datasets"><a href="http://arxiv.org/abs/2403.03194v1">MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets</a></h2>
<p>发布时间：2024-03-05</p>
<p>作者：Hossein Aboutalebi, Hwanjun Song, Yusheng Xie, Arshit Gupta, Justin Sun, Hang Su, Igor Shalyminov, Nikolaos Pappas, Siffi Singh, Saab Mansour</p>
<h4 id="_125">中文摘要：</h4>
<p>本文提出了一种名为MAGID（多模态增强图像对话）的框架，旨在通过丰富多样且高质量的图像来增强仅包含文本的对话。该框架首先利用扩散模型生成与识别文本相匹配的图像，然后通过一个创新的反馈循环，结合图像描述生成模块（文本型大型语言模型）和图像质量模块（处理美学、图像-文本匹配和安全问题），共同生成高质量的跨模态对话。在三个对话数据集上，通过自动和人工评估，将MAGID与其他SOTA基线进行了比较。结果显示，MAGID在人类评估中表现出色，尤其是在图像数据库较小的情况下，其性能优于检索基线。</p>
<h4 id="_126">一句话总结：</h4>
<p>MAGID通过结合文本型大型语言模型和图像生成技术，实现了高质量的多模态对话增强。</p>
<hr />
<h2 id="reliable-adaptable-and-attributable-language-models-with-retrieval"><a href="http://arxiv.org/abs/2403.03187v1">Reliable, Adaptable, and Attributable Language Models with Retrieval</a></h2>
<p>发布时间：2024-03-05</p>
<p>作者：Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih</p>
<h4 id="_127">中文摘要：</h4>
<p>参数化语言模型（LMs），经过大量网络数据的训练，展现出卓越的灵活性和能力。然而，它们仍然面临一些实际挑战，如幻觉、难以适应新的数据分布以及缺乏可验证性。在这篇立场论文中，我们主张检索增强型LMs作为下一代LMs来取代参数化LMs。通过在推理过程中整合大规模数据存储，检索增强型LMs可以更加可靠、适应性强和可追溯。尽管检索增强型LMs具有巨大潜力，但由于一些障碍，它们尚未得到广泛应用：具体来说，当前的检索增强型LMs难以在知识密集型任务（如问答）之外利用有用的文本，检索和LM组件之间的交互有限，并且缺乏扩展的基础设施。为了解决这些问题，我们提出了一份开发通用检索增强型LMs的路线图。这包括重新考虑数据存储和检索器，探索改进检索器-LM交互的管道，以及大量投资于高效训练和推理的基础设施。</p>
<h4 id="_128">一句话总结：</h4>
<p>本文提出将检索增强型语言模型作为下一代LMs，以解决现有参数化LMs的局限性，并通过改进数据存储、检索器交互和基础设施来推动其广泛应用。</p>
<hr />
<h2 id="towards-comprehensive-vietnamese-retrieval-augmented-generation-and-large-language-models"><a href="http://arxiv.org/abs/2403.01616v2">Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models</a></h2>
<p>发布时间：2024-03-03</p>
<p>作者：Nguyen Quang Duc, Le Hai Son, Nguyen Duc Nhan, Nguyen Dich Nhat Minh, Le Thanh Huong, Dinh Viet Sang</p>
<h4 id="_129">中文摘要：</h4>
<p>本文介绍了我们通过开发和推广面向越南语检索增强生成（RAG）和大型语言模型（LLMs）的开放数据集和预训练模型，在提升越南语理解和生成能力方面所做出的贡献。</p>
<h4 id="_130">一句话总结：</h4>
<p>本文致力于通过开放数据集和预训练模型推动越南语理解和生成技术的发展。</p>
<hr />
<h2 id="fine-tuning-vs-retrieval-augmented-generation-for-less-popular-knowledge"><a href="http://arxiv.org/abs/2403.01432v2">Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge</a></h2>
<p>发布时间：2024-03-03</p>
<p>作者：Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi</p>
<h4 id="_131">中文摘要：</h4>
<p>大型语言模型（LLMs）能够记忆大量的事实知识，在各个任务和领域展现出强大的性能。然而，观察发现，当处理不太常见或低频的概念和实体时，例如在特定领域应用中，其性能会下降。提高LLMs在低频主题上性能的两种主要方法是：检索增强生成（RAG）和基于合成数据的微调（FT）。本文探讨了评估RAG和FT在定制LLMs处理低频实体在问答任务上的影响。我们的研究发现，FT在提高不同流行度实体的性能方面有显著提升，尤其是在最流行和最不流行的群体中，而RAG则超越了其他方法。此外，RAG和FT方法的成功也得益于检索和数据增强技术的进步。我们将在https://github.com/informagi/RAGvsFT上发布我们的数据和代码。</p>
<h4 id="_132">一句话总结：</h4>
<p>本文研究了检索增强生成和微调对大型语言模型在处理低频实体问答任务上的性能提升作用，发现微调在提高实体性能方面有显著效果，而检索增强生成方法则超越了其他方法。</p>
<hr />
<h2 id="syllabusqa-a-course-logistics-question-answering-dataset"><a href="http://arxiv.org/abs/2403.14666v2">SyllabusQA: A Course Logistics Question Answering Dataset</a></h2>
<p>发布时间：2024-03-03</p>
<p>作者：Nigel Fernandez, Alexander Scarlatos, Andrew Lan</p>
<h4 id="_133">中文摘要：</h4>
<p>自动化教学助手和聊天机器人具有显著潜力减轻人类教师的负担，尤其是在与后勤相关的问答方面，这对于学生来说很重要，但对于教师来说却是重复性的工作。然而，由于隐私问题的存在，公开可用的数据集非常有限。我们介绍了SyllabusQA，这是一个开源数据集，包含63份真实的课程教学大纲，覆盖了36个专业，包含5,078个开放式的课程后勤相关问答对，这些问答对在问题和答案格式上都具有多样性。由于许多后勤相关的问题包含诸如考试日期等关键信息，因此评估答案的真实性非常重要。我们在这一任务上对几个强大的基线进行了基准测试，从大型语言模型提示到检索增强生成。我们引入了Fact-QA，这是一个基于LLM（GPT-4）的评价指标，用于评估预测答案的真实性。我们发现，尽管在文本相似性的传统指标上表现接近人类，但在事实精确性方面，自动化方法与人类之间仍然存在显著的差距。</p>
<h4 id="_134">一句话总结：</h4>
<p>本研究通过引入SyllabusQA数据集和Fact-QA评价指标，揭示了自动化教学助手在后勤问答任务上与人类在事实精确性方面存在的显著差距。</p>
<hr />
<h2 id="ragged-edges-the-double-edged-sword-of-retrieval-augmented-chatbots"><a href="http://arxiv.org/abs/2403.01193v3">RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots</a></h2>
<p>发布时间：2024-03-02</p>
<p>作者：Philip Feldman, James R. Foulds, Shimei Pan</p>
<h4 id="_135">中文摘要：</h4>
<p>大型语言模型（LLMs）如ChatGPT展示了人工智能的显著进步。然而，它们产生幻觉——生成看似合理但却是虚假信息的倾向——提出了重大挑战。这一问题至关重要，正如在最近的法庭案件中，ChatGPT的使用导致了引用不存在的法律裁决。本文探讨了如何通过整合外部知识与提示来利用检索增强生成（RAG）来对抗幻觉。我们使用旨在诱导幻觉的提示对RAG与标准LLMs进行了实证评估。我们的结果表明，RAG在某些情况下提高了准确性，但当提示直接与模型的预训练理解相矛盾时，仍然可能被误导。这些发现突显了幻觉的复杂性质以及确保LLM在实际应用中可靠性的更稳健解决方案的需求。我们提供了RAG部署的实用建议，并讨论了对更可信LLM开发的启示。</p>
<h4 id="_136">一句话总结：</h4>
<p>本文通过实证研究，探讨了检索增强生成（RAG）在对抗大型语言模型（LLMs）幻觉方面的有效性，并提出了提高LLM可靠性的实用建议。</p>
<hr />
<h2 id="open-assistant-toolkit-version-2"><a href="http://arxiv.org/abs/2403.00586v1">Open Assistant Toolkit -- version 2</a></h2>
<p>发布时间：2024-03-01</p>
<p>作者：Sophie Fischer, Federico Rossetto, Carlos Gemmell, Andrew Ramsay, Iain Mackie, Philip Zubel, Niklas Tecklenburg, Jeffrey Dalton</p>
<h4 id="_137">中文摘要：</h4>
<p>我们提出了开放助手工具包（OAT）的第二版本（OAT-v2），这是一个开源的任务导向型对话系统，用于构建生成型神经网络模型。OAT-v2是一个可扩展且灵活的助手平台，支持多个领域和用户交互模式。它将处理用户话语的过程分解为模块化系统组件，包括动作代码生成、多模态内容检索和知识增强响应生成的子模块。OAT-v2是在多年的Alexa TaskBot挑战赛中开发出来的，是一个经过验证的系统，它使得在实验和现实世界的部署中能够进行可扩展且稳健的实验。OAT-v2为研究和商业应用提供了开放模型和软件，以促进多模态虚拟助手在多种应用和丰富交互类型中的未来发展。</p>
<h4 id="_138">一句话总结：</h4>
<p>OAT-v2是一个可扩展且灵活的开源助手平台，旨在支持多模态虚拟助手在多种应用中的发展。</p>
<hr />
<h2 id="dfin-sql-integrating-focused-schema-with-din-sql-for-superior-accuracy-in-large-scale-databases"><a href="http://arxiv.org/abs/2403.00872v1">DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy in Large-Scale Databases</a></h2>
<p>发布时间：2024-03-01</p>
<p>作者：Shai Volvovsky, Marco Marcassa, Mustafa Panbiharwala</p>
<h4 id="_139">中文摘要：</h4>
<p>自然语言查询转换为SQL查询的任务复杂，需要精确的技术组合来实现准确的翻译。DIN-SQL（在上下文中分解的SQL）方法在这一领域取得了重要进展。本文介绍了DFIN（分解聚焦上下文），这是DIN-SQL的创新扩展，通过解决模式链接错误（这是不准确的主要原因）来增强文本到SQL的转换。DFIN独特地交替使用提示技术和检索增强生成（RAG），以适应数据库模式的大小和复杂性。预处理阶段嵌入数据库定义并利用类似BIRD数据集中的注释文件，便于运行时检索相关的模式信息。这种策略显著减少了模式链接提示的令牌计数，使得可以使用标准的GPT-4模型而不是其更大的上下文变体，从而更有效地处理大规模数据库。在BIRD数据集上的评估，这是一个具有挑战性的真实世界基准，表明DFIN不仅效率高，而且准确性也得到了提高，达到了51.69的分数。这一改进超过了DIN-SQL方法（目前的第三名），这是目前排名第一的采用上下文学习而不是微调的模型，之前得分为50.72。DFIN的进步突显了结合高级语言模型的上下文学习方法的发展能力，为复杂文本到SQL转换任务的未来研究提供了有希望的途径。</p>
<h4 id="_140">一句话总结：</h4>
<p>DFIN通过解决模式链接错误，提高了文本到SQL转换的准确性和效率，为大规模数据库提供了更有效的解决方案。</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
    
  </body>
</html>